"""
ç»Ÿä¸€æŠ“å–å·¥å…·ç±»

æä¾›æ ‡å‡†åŒ–çš„æ•°æ®æå–å’Œæ¸…ç†åŠŸèƒ½ï¼Œç”¨äºæ‰€æœ‰Scraperçš„æ•°æ®å¤„ç†ã€‚
"""

import logging
import re
from typing import Optional, Dict, Any, List, Callable

from bs4 import BeautifulSoup


def is_valid_product_image(image_url: str, image_config: Dict[str, Any]) -> bool:
    """
    éªŒè¯å›¾ç‰‡URLæ˜¯å¦ä¸ºæœ‰æ•ˆçš„å•†å“å›¾ç‰‡ï¼ˆé€šç”¨æ–¹æ³•ï¼‰

    Args:
        image_url: å›¾ç‰‡URL
        image_config: å›¾ç‰‡é…ç½®ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
            - valid_patterns: æœ‰æ•ˆå›¾ç‰‡æ¨¡å¼åˆ—è¡¨
            - valid_extensions: æœ‰æ•ˆå›¾ç‰‡æ‰©å±•ååˆ—è¡¨
            - valid_domains: æœ‰æ•ˆåŸŸååˆ—è¡¨
            - placeholder_keywords: å ä½ç¬¦å…³é”®è¯åˆ—è¡¨

    Returns:
        bool: Trueè¡¨ç¤ºæ˜¯æœ‰æ•ˆå•†å“å›¾ç‰‡ï¼ŒFalseè¡¨ç¤ºæ— æ•ˆ
    """
    if not image_url:
        return False

    url_lower = image_url.lower()

    # è·å–é…ç½®å‚æ•°
    valid_patterns = image_config.get('valid_patterns', [])
    valid_extensions = image_config.get('valid_extensions', ['.jpg', '.jpeg', '.png', '.webp'])
    valid_domains = image_config.get('valid_domains', [])
    placeholder_keywords = image_config.get('placeholder_keywords', ['doodle', 'placeholder', 'default', 'error'])

    # å¿…é¡»åŒ…å«è‡³å°‘ä¸€ä¸ªæœ‰æ•ˆæ¨¡å¼ï¼ˆå¦‚æœé…ç½®äº†çš„è¯ï¼‰
    if valid_patterns:
        has_valid_pattern = any(pattern in url_lower for pattern in valid_patterns)
        if not has_valid_pattern:
            return False

    # å¿…é¡»æ˜¯å›¾ç‰‡æ–‡ä»¶
    is_image_file = any(ext in url_lower for ext in valid_extensions)
    if not is_image_file:
        return False

    # å¿…é¡»æ¥è‡ªæœ‰æ•ˆåŸŸåï¼ˆå¦‚æœé…ç½®äº†çš„è¯ï¼‰
    if valid_domains:
        is_valid_domain = any(domain in url_lower for domain in valid_domains)
        if not is_valid_domain:
            return False

    # ä¸èƒ½åŒ…å«æ˜æ˜¾çš„å ä½ç¬¦ç‰¹å¾
    has_placeholder_features = any(keyword in url_lower for keyword in placeholder_keywords)
    if has_placeholder_features:
        return False

    return True


def is_placeholder_image(image_url: str, placeholder_patterns: List[str]) -> bool:
    """
    æ£€æŸ¥å›¾ç‰‡URLæ˜¯å¦ä¸ºå ä½ç¬¦å›¾ç‰‡ï¼ˆé€šç”¨æ–¹æ³•ï¼‰

    Args:
        image_url: å›¾ç‰‡URL
        placeholder_patterns: å ä½ç¬¦å›¾ç‰‡æ¨¡å¼åˆ—è¡¨

    Returns:
        bool: Trueè¡¨ç¤ºæ˜¯å ä½ç¬¦å›¾ç‰‡ï¼ŒFalseè¡¨ç¤ºä¸æ˜¯
    """
    if not image_url:
        return True

    # æ£€æŸ¥URLä¸­æ˜¯å¦åŒ…å«å ä½ç¬¦æ¨¡å¼
    for pattern in placeholder_patterns:
        if pattern in image_url:
            return True

    # æ£€æŸ¥æ˜¯å¦åŒ…å«å…¶ä»–å·²çŸ¥çš„å ä½ç¬¦ç‰¹å¾
    placeholder_keywords = ['doodle', 'placeholder', 'default', 'no-image', 'loading']
    url_lower = image_url.lower()

    for keyword in placeholder_keywords:
        if keyword in url_lower:
            return True

    return False


def clean_text(text: str) -> str:
    """
    æ¸…ç†æ–‡æœ¬å†…å®¹

    Args:
        text: åŸå§‹æ–‡æœ¬

    Returns:
        str: æ¸…ç†åçš„æ–‡æœ¬
    """
    if not text:
        return ""

    # å»é™¤é¦–å°¾ç©ºç™½å­—ç¬¦
    text = text.strip()

    # å»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
    text = re.sub(r'\s+', ' ', text)

    return text


def validate_price(price: float) -> bool:
    """
    éªŒè¯ä»·æ ¼æ˜¯å¦æœ‰æ•ˆ

    Args:
        price: ä»·æ ¼å€¼

    Returns:
        bool: ä»·æ ¼æ˜¯å¦æœ‰æ•ˆ
    """
    try:
        # ä»·æ ¼åº”è¯¥å¤§äº0ä¸”å°äºä¸€ä¸ªåˆç†çš„ä¸Šé™
        return 0 < price < 10000000  # 1000ä¸‡ä»¥ä¸‹è®¤ä¸ºæ˜¯åˆç†ä»·æ ¼
    except Exception:
        return False


def create_content_validator(min_text_length: int = 20) -> Callable[[List], bool]:
    """
    åˆ›å»ºé€šç”¨çš„å†…å®¹éªŒè¯å™¨

    Args:
        min_text_length: æœ€å°æ–‡æœ¬é•¿åº¦é˜ˆå€¼ï¼Œé»˜è®¤20å­—ç¬¦

    Returns:
        callable: éªŒè¯å‡½æ•°ï¼Œæ¥å—elementså‚æ•°ï¼Œè¿”å›bool
    """

    def validator(elements):
        """
        é€šç”¨å†…å®¹éªŒè¯å™¨ï¼šæ£€æŸ¥elementsä¸­æ˜¯å¦æœ‰è¶³å¤Ÿé•¿åº¦çš„æ–‡æœ¬å†…å®¹

        Args:
            elements: BeautifulSoupå…ƒç´ åˆ—è¡¨æˆ–å•ä¸ªå…ƒç´ 

        Returns:
            bool: æ˜¯å¦é€šè¿‡éªŒè¯
        """
        if not elements:
            return False

        # ç¡®ä¿elementsæ˜¯å¯è¿­ä»£çš„
        if not hasattr(elements, '__iter__') or isinstance(elements, str):
            elements = [elements] if elements else []

        return any(
            el and el.get_text(strip=True) and len(el.get_text(strip=True)) > min_text_length
            for el in elements if el
        )

    return validator


def validate_content_length(elements, min_length: int = 20) -> bool:
    """
    éªŒè¯å…ƒç´ å†…å®¹é•¿åº¦çš„å¿«æ·å‡½æ•°

    Args:
        elements: BeautifulSoupå…ƒç´ åˆ—è¡¨æˆ–å•ä¸ªå…ƒç´ 
        min_length: æœ€å°æ–‡æœ¬é•¿åº¦é˜ˆå€¼ï¼Œé»˜è®¤20å­—ç¬¦

    Returns:
        bool: æ˜¯å¦æœ‰å…ƒç´ çš„æ–‡æœ¬å†…å®¹è¶…è¿‡æœ€å°é•¿åº¦
    """
    return create_content_validator(min_length)(elements)


class ScrapingUtils:
    """
    ç»Ÿä¸€æŠ“å–å·¥å…·ç±»
    
    æä¾›æ ‡å‡†åŒ–çš„æ•°æ®æå–å’Œæ¸…ç†åŠŸèƒ½
    """

    def __init__(self, logger: Optional[logging.Logger] = None):
        """
        åˆå§‹åŒ–æŠ“å–å·¥å…·
        
        Args:
            logger: æ—¥å¿—è®°å½•å™¨
        """
        from common.config.ozon_selectors_config import get_ozon_selectors_config
        self.selectors_config = get_ozon_selectors_config()
        self.logger = logger or logging.getLogger(__name__)

    # =============================================================================
    # æ•°æ®æå–æ–¹æ³•
    # =============================================================================

    def extract_price(self, text: str) -> Optional[float]:
        """
        æå–ä»·æ ¼ä¿¡æ¯
        
        Args:
            text: åŒ…å«ä»·æ ¼çš„æ–‡æœ¬
            
        Returns:
            Optional[float]: æå–çš„ä»·æ ¼ï¼Œå¤±è´¥è¿”å›None
        """
        if not text:
            return None

        try:
            # æ¸…ç†æ–‡æœ¬
            cleaned_text = clean_text(text)

            # åŒ¹é…ä»·æ ¼æ¨¡å¼ (æ”¯æŒå¤šç§è´§å¸ç¬¦å·)
            price_pattern = r'[\d\s,.]+(?:\s*(?:â‚½|â‚¬|\$|USD|EUR|RUB|Ñ€ÑƒĞ±))|(?:\d+(?:[.,]\d{2})?)'
            matches = re.findall(price_pattern, cleaned_text)

            if matches:
                # å–ç¬¬ä¸€ä¸ªåŒ¹é…çš„ä»·æ ¼
                price_str = matches[0]
                # ç§»é™¤è´§å¸ç¬¦å·å’Œç©ºæ ¼
                price_str = re.sub(r'[^\d.,]', '', price_str)
                # æ ‡å‡†åŒ–å°æ•°ç‚¹
                price_str = price_str.replace(',', '.')

                # è½¬æ¢ä¸ºæµ®ç‚¹æ•°
                return float(price_str)

            return None
        except Exception as e:
            self.logger.warning(f"ä»·æ ¼æå–å¤±è´¥: {e}")
            return None

    def extract_number(self, text: str) -> Optional[int]:
        """
        æå–æ•°å­—ä¿¡æ¯
        
        Args:
            text: åŒ…å«æ•°å­—çš„æ–‡æœ¬
            
        Returns:
            Optional[int]: æå–çš„æ•°å­—ï¼Œå¤±è´¥è¿”å›None
        """
        if not text:
            return None

        try:
            # æ¸…ç†æ–‡æœ¬
            cleaned_text = clean_text(text)

            # åŒ¹é…æ•°å­—æ¨¡å¼
            number_pattern = r'\d+'
            matches = re.findall(number_pattern, cleaned_text)

            if matches:
                # å–ç¬¬ä¸€ä¸ªåŒ¹é…çš„æ•°å­—
                return int(matches[0])

            return None
        except Exception as e:
            self.logger.warning(f"æ•°å­—æå–å¤±è´¥: {e}")
            return None

    def extract_data_with_js(self, browser_service, script: str,
                             description: str = "æ•°æ®", *args) -> Any:
        """
        ä½¿ç”¨JavaScriptæå–æ•°æ®
        
        Args:
            browser_service: æµè§ˆå™¨æœåŠ¡
            script: JavaScriptè„šæœ¬
            description: æ•°æ®æè¿°
            
        Returns:
            Any: æå–çš„æ•°æ®
        """
        try:
            if not browser_service:
                self.logger.error("Browser service not initialized")
                return None

            # ğŸ”§ æ”¯æŒå‚æ•°ä¼ é€’ï¼šå¦‚æœæœ‰å‚æ•°ï¼Œåˆ›å»ºå‡½æ•°è°ƒç”¨æ ¼å¼
            if args:
                # å°†å‚æ•°è½¬ä¸ºJSONå­—ç¬¦ä¸²æ•°ç»„
                args_json = [f"'{arg}'" if isinstance(arg, str) else str(arg) for arg in args]
                script_with_args = f"(function() {{ {script} }})({', '.join(args_json)})"
                result = browser_service.evaluate_sync(script_with_args)
            else:
                result = browser_service.evaluate_sync(script)
            self.logger.debug(f"âœ… JavaScriptæå–{description}æˆåŠŸ")
            return result
        except Exception as e:
            self.logger.warning(f"JavaScriptæå–{description}å¤±è´¥: {e}")
            return None

    # =============================================================================
    # é€šç”¨å¯¼èˆªå’ŒSoupè·å–æ–¹æ³•
    # =============================================================================

    @staticmethod
    def get_or_navigate_soup(soup: BeautifulSoup, url: str, browser_service=None) -> BeautifulSoup:
        """
        å¯¼èˆªåˆ°æŒ‡å®šURLå¹¶è·å–é¡µé¢soupå¯¹è±¡

        Args:
            soup: å·²æœ‰çš„BeautifulSoupå¯¹è±¡ï¼Œå¦‚æœå­˜åœ¨åˆ™ç›´æ¥è¿”å›
            url: è¦å¯¼èˆªåˆ°çš„URL
            browser_service: æµè§ˆå™¨æœåŠ¡å®ä¾‹ï¼Œç”¨äºé¡µé¢å¯¼èˆª

        Returns:
            BeautifulSoup: é¡µé¢soupå¯¹è±¡

        Raises:
            ValueError: å½“éœ€è¦å¯¼èˆªä½†browser_serviceä¸ºNoneæ—¶
            Exception: é¡µé¢å¯¼èˆªå¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
        """
        if soup:
            return soup
        else:
            if browser_service is None:
                raise ValueError("éœ€è¦browser_serviceå®ä¾‹æ¥è¿›è¡Œé¡µé¢å¯¼èˆª")

            # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„æµè§ˆå™¨å¯¼èˆªæ–¹æ³•å
            success = False
            if hasattr(browser_service, 'navigate_to_sync'):
                success = browser_service.navigate_to_sync(url)
            elif hasattr(browser_service, 'open_page_sync'):
                success = browser_service.open_page_sync(url)
            else:
                raise ValueError(f"æµè§ˆå™¨æœåŠ¡ {type(browser_service).__name__} ä¸æ”¯æŒåŒæ­¥å¯¼èˆªæ–¹æ³•")

            if not success:
                raise Exception(f"é¡µé¢å¯¼èˆªå¤±è´¥: {url}")

            try:
                page_content = browser_service.evaluate_sync("() => document.documentElement.outerHTML")
                return BeautifulSoup(page_content, 'html.parser')
            except Exception as e:
                raise Exception(f"é¡µé¢å†…å®¹è§£æå¤±è´¥: {e}")

    # åŸºäºget_or_navigate_soupæœåŠ¡æä¾›ä¸€ä¸ªå¢å¼ºæ–¹æ³•ï¼Œget_or_navi_on_condition

    # =============================================================================
    # Ozon è·Ÿå–ç›¸å…³çš„é€šç”¨æ–¹æ³•
    # =============================================================================
    def get_competitor_area(self, soup):
        """
        è·å–è·Ÿå–åŒºåŸŸ

        Args:
            soup: BeautifulSoupå¯¹è±¡

        Returns:
            Any: è·Ÿå–åŒºåŸŸ
        """
        try:
            # ä½¿ç”¨é…ç½®åŒ–çš„ç«äº‰è€…å®¹å™¨é€‰æ‹©å™¨
            competitor_container = None
            for selector in self.selectors_config.competitor_area_selectors:
                try:
                    competitor_container = soup.select_one(selector)
                    if competitor_container:
                        self.logger.debug(f"âœ… æ‰¾åˆ°ç«äº‰è€…å®¹å™¨: {selector}")
                        return competitor_container
                except Exception as e:
                    self.logger.debug(f"å®¹å™¨é€‰æ‹©å™¨å¤±è´¥: {e}")
                    continue

            self.logger.warning("âš ï¸ æœªæ‰¾åˆ°ç«äº‰è€…ä¿¡æ¯å®¹å™¨")
            return None
        except Exception as e:
            self.logger.error(f"è·å–ç«äº‰è€…åŒºåŸŸå¤±è´¥: {e}")
            return None

    def extract_store_id_from_url(self, href: str) -> Optional[str]:
        """
        ä»URLä¸­æå–åº—é“ºIDï¼ˆé€šç”¨æ–¹æ³•ï¼‰

        æ”¯æŒå¤šç§Ozonåº—é“ºURLæ ¼å¼çš„IDæå–ï¼ŒåŒ…æ‹¬ï¼š
        - /seller/name-123619/
        - /seller/123619/
        - seller/123619 æˆ– seller_123619
        - sellerId=123619
        - /shop/123619
        - /store/123619

        Args:
            href (str): åº—é“ºURLå­—ç¬¦ä¸²

        Returns:
            Optional[str]: æå–çš„åº—é“ºIDï¼Œæå–å¤±è´¥è¿”å›None

            None
        """
        if not href or not isinstance(href, str):
            return None

        try:
            # æ”¯æŒçš„URLæ¨¡å¼ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
            patterns = [
                r'/seller/[^/]+-(\d+)/?$',  # /seller/name-123619/
                r'/seller/(\d+)/?$',  # /seller/123619/
                r'seller[/_](\d+)',  # seller/123619 æˆ– seller_123619
                r'sellerId=(\d+)',  # sellerId=123619
                r'/shop/(\d+)',  # /shop/123619
                r'/store/(\d+)'  # /store/123619
            ]

            for pattern in patterns:
                match = re.search(pattern, href)
                if match:
                    store_id = match.group(1)
                    self.logger.debug(f"âœ… æå–åº—é“ºID: {store_id} (æ¨¡å¼: {pattern})")
                    return store_id

            self.logger.debug(f"âš ï¸ æ— æ³•ä»URLæå–åº—é“ºID: {href[:100]}...")
            return None

        except Exception as e:
            self.logger.warning(f"æå–åº—é“ºIDå¤±è´¥: {e}")
            return None

    # =============================================================================
    # Ozon å›¾ç‰‡å¤„ç†é€šç”¨æ–¹æ³•
    # =============================================================================

    def convert_to_high_res_image(self, image_url: str, conversion_config: Optional[Dict[str, str]] = None) -> str:
        """
        å°†å›¾ç‰‡URLè½¬æ¢ä¸ºé«˜æ¸…ç‰ˆæœ¬ï¼ˆé€šç”¨æ–¹æ³•ï¼‰

        Args:
            image_url: åŸå§‹å›¾ç‰‡URL
            conversion_config: è½¬æ¢é…ç½®ï¼Œæ ¼å¼ä¸º {"pattern": "replacement"}
                ä¾‹å¦‚: {"wc\\d+": "wc1000"} è¡¨ç¤ºå°†wc50ã€wc100ç­‰æ›¿æ¢ä¸ºwc1000

        Returns:
            str: é«˜æ¸…å›¾ç‰‡URL
        """
        if not image_url or not conversion_config:
            return image_url

        try:
            # æ‰§è¡ŒURLè½¬æ¢
            result_url = image_url
            for pattern, replacement in conversion_config.items():
                result_url = re.sub(pattern, replacement, result_url)

            self.logger.debug(f"å›¾ç‰‡URLè½¬æ¢: {image_url} -> {result_url}")
            return result_url
        except Exception as e:
            self.logger.warning(f"è½¬æ¢é«˜æ¸…å›¾ç‰‡URLå¤±è´¥: {e}")
            return image_url

    def extract_product_image(self, soup, image_selectors: List[str], image_config: Dict[str, Any]) -> Optional[str]:
        """
        é€šç”¨å•†å“å›¾ç‰‡æå–æ–¹æ³•

        Args:
            soup: BeautifulSoupå¯¹è±¡
            image_selectors: å›¾ç‰‡é€‰æ‹©å™¨åˆ—è¡¨
            image_config: å›¾ç‰‡é…ç½®ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
                - placeholder_patterns: å ä½ç¬¦æ¨¡å¼åˆ—è¡¨
                - valid_patterns: æœ‰æ•ˆå›¾ç‰‡æ¨¡å¼åˆ—è¡¨
                - valid_extensions: æœ‰æ•ˆå›¾ç‰‡æ‰©å±•ååˆ—è¡¨
                - valid_domains: æœ‰æ•ˆåŸŸååˆ—è¡¨
                - conversion_config: é«˜æ¸…è½¬æ¢é…ç½®

        Returns:
            str: å•†å“å›¾ç‰‡URLï¼Œå¦‚æœæå–å¤±è´¥è¿”å›None
        """
        try:
            # è·å–é…ç½®
            placeholder_patterns = image_config.get('placeholder_patterns', [])
            conversion_config = image_config.get('conversion_config', {})

            for selector in image_selectors:
                img_elements = soup.select(selector)
                self.logger.debug(f"ğŸ” é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(img_elements)} ä¸ªå›¾ç‰‡å…ƒç´ ")

                for img_element in img_elements:
                    src = img_element.get('src')
                    if not src:
                        continue

                    # è½¬æ¢ä¸ºé«˜æ¸…ç‰ˆæœ¬
                    high_res_url = self.convert_to_high_res_image(src, conversion_config)

                    # éªŒè¯å›¾ç‰‡URLæ˜¯å¦ä¸ºå ä½ç¬¦
                    if is_placeholder_image(high_res_url, placeholder_patterns):
                        self.logger.warning(f"âš ï¸ è·³è¿‡å ä½ç¬¦å›¾ç‰‡: {high_res_url}")
                        continue

                    # éªŒè¯å›¾ç‰‡URLæ˜¯å¦ä¸ºæœ‰æ•ˆçš„å•†å“å›¾ç‰‡
                    if is_valid_product_image(high_res_url, image_config):
                        self.logger.info(f"âœ… æˆåŠŸæå–å•†å“å›¾ç‰‡: {high_res_url}")
                        return high_res_url
                    else:
                        self.logger.debug(f"ğŸ” è·³è¿‡æ— æ•ˆå›¾ç‰‡: {high_res_url}")

            self.logger.warning("âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆçš„å•†å“å›¾ç‰‡")
            return None

        except Exception as e:
            self.logger.error(f"æå–å•†å“å›¾ç‰‡å¤±è´¥: {e}")
            return None

    def extract_price_from_soup(self, soup, price_type: str = "default", max_elements: int = 50) -> Optional[float]:

        if not soup:
            self.logger.warning("BeautifulSoupå¯¹è±¡ä¸ºç©º")
            return None

        if not hasattr(self, 'selectors_config') or not self.selectors_config:
            self.logger.error("é€‰æ‹©å™¨é…ç½®æœªåˆå§‹åŒ–")
            return None

        try:
            # è·å–ä»·æ ¼é€‰æ‹©å™¨åˆ—è¡¨ï¼ˆå·²æŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
            price_selectors = self.selectors_config.get_price_selectors_for_type(price_type)

            if not price_selectors:
                self.logger.warning(f"æœªæ‰¾åˆ° {price_type} ç±»å‹çš„ä»·æ ¼é€‰æ‹©å™¨")
                return None

            # å®‰å…¨è·å–é€‰æ‹©å™¨æ•°é‡ï¼Œé¿å…Mockå¯¹è±¡len()é”™è¯¯
            try:
                selector_count = len(price_selectors) if hasattr(price_selectors, '__len__') else 'N/A'
            except (TypeError, AttributeError):
                selector_count = 'N/A'

            self.logger.debug(f"ğŸ” å°è¯•æå– {price_type} ä»·æ ¼ï¼Œä½¿ç”¨ {selector_count} ä¸ªé€‰æ‹©å™¨")

            # ä¼˜åŒ–ï¼šé™åˆ¶å¤„ç†çš„é€‰æ‹©å™¨æ•°é‡ï¼Œé¿å…è¿‡åº¦å¤„ç†
            processed_elements = 0

            # ç¡®ä¿price_selectorsæ˜¯å¯è¿­ä»£çš„ï¼Œå¤„ç†Mockå¯¹è±¡æƒ…å†µ
            try:
                # æµ‹è¯•æ˜¯å¦å¯è¿­ä»£
                iter(price_selectors)
                selectors_to_use = price_selectors
            except (TypeError, AttributeError):
                # å¦‚æœä¸å¯è¿­ä»£ï¼ˆå¦‚Mockå¯¹è±¡ï¼‰ï¼Œä½¿ç”¨é»˜è®¤é€‰æ‹©å™¨
                self.logger.debug(f"ä»·æ ¼é€‰æ‹©å™¨ä¸å¯è¿­ä»£ï¼Œä½¿ç”¨é»˜è®¤é€‰æ‹©å™¨")
                # æ ¹æ®ä»·æ ¼ç±»å‹æä¾›ä¸åŒçš„é»˜è®¤é€‰æ‹©å™¨
                if price_type == "green":
                    selectors_to_use = ['.price-green', '.green-price', '.discount-price', '.sale-price']
                elif price_type == "black":
                    selectors_to_use = ['.price-black', '.black-price', '.regular-price', '.original-price']
                else:
                    selectors_to_use = ['.price', '.current-price', '.sale-price', '.product-price']

            for i, selector in enumerate(selectors_to_use):
                # æ£€æŸ¥æ˜¯å¦è¶…å‡ºå¤„ç†é™åˆ¶
                if processed_elements >= max_elements:
                    self.logger.debug(f"å·²å¤„ç† {processed_elements} ä¸ªå…ƒç´ ï¼Œè¾¾åˆ°é™åˆ¶")
                    break

                try:
                    # ä½¿ç”¨CSSé€‰æ‹©å™¨æŸ¥æ‰¾å…ƒç´ 
                    elements = soup.select(selector)

                    # é™åˆ¶æ¯ä¸ªé€‰æ‹©å™¨å¤„ç†çš„å…ƒç´ æ•°é‡
                    elements_to_process = elements[:min(10, max_elements - processed_elements)]

                    for element in elements_to_process:
                        processed_elements += 1

                        # æå–å…ƒç´ æ–‡æœ¬
                        price_text = element.get_text(strip=True) if element else None
                        if not price_text:
                            continue

                        try:
                            # è°ƒç”¨ä»·æ ¼æå–æ–¹æ³•
                            price = self.extract_price(price_text)

                            # éªŒè¯ä»·æ ¼æœ‰æ•ˆæ€§
                            if price and validate_price(price):
                                # å®‰å…¨å¤„ç†é€‰æ‹©å™¨å­—ç¬¦ä¸²æ˜¾ç¤º
                                try:
                                    selector_display = (
                                        f"{selector[:50]}{'...' if len(selector) > 50 else ''}"
                                        if hasattr(selector, '__len__') and hasattr(selector, '__getitem__')
                                        else str(selector)[:50]
                                    )
                                except (TypeError, AttributeError):
                                    selector_display = str(selector)[:50]

                                self.logger.debug(
                                    f"âœ… æå–åˆ°{price_type}ä»·æ ¼: {price} "
                                    f"(é€‰æ‹©å™¨: {selector_display})"
                                )
                                return price

                        except (ValueError, TypeError, AttributeError) as e:
                            # ä»·æ ¼æå–æˆ–éªŒè¯å¤±è´¥ï¼Œç»§ç»­ä¸‹ä¸€ä¸ªå…ƒç´ 
                            self.logger.debug(f"ä»·æ ¼è§£æå¤±è´¥: {price_text[:30]} - {e}")
                            continue
                        except Exception as e:
                            # å…¶ä»–æœªé¢„æœŸçš„å¼‚å¸¸
                            self.logger.warning(f"å¤„ç†å…ƒç´ æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
                            continue

                except (AttributeError, TypeError) as e:
                    # BeautifulSoupç›¸å…³å¼‚å¸¸
                    self.logger.debug(f"é€‰æ‹©å™¨ '{selector[:30]}...' æ‰§è¡Œå¤±è´¥: {e}")
                    continue
                except Exception as e:
                    # å…¶ä»–é€‰æ‹©å™¨æ‰§è¡Œå¼‚å¸¸
                    self.logger.debug(f"é€‰æ‹©å™¨ '{selector[:30]}...' å¤„ç†å¼‚å¸¸: {e}")
                    continue

            self.logger.debug(f"âš ï¸ æœªèƒ½æå–åˆ°{price_type}ä»·æ ¼ (å¤„ç†äº†{processed_elements}ä¸ªå…ƒç´ )")
            return None

        except AttributeError as e:
            # é…ç½®æˆ–æ–¹æ³•ä¸å­˜åœ¨
            self.logger.error(f"é…ç½®æˆ–æ–¹æ³•è®¿é—®é”™è¯¯: {e}")
            return None
        except Exception as e:
            # å…¶ä»–æœªé¢„æœŸçš„å¼‚å¸¸
            self.logger.warning(f"ä»soupæå–ä»·æ ¼å¤±è´¥: {e}")
            return None


# å…¨å±€å®ä¾‹ç®¡ç†
_scraping_utils_instance = None

# =============================================================================
# ä»·æ ¼å¤„ç†ä¼˜åŒ– - é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼å’Œç¼“å­˜é…ç½®
# =============================================================================

from functools import lru_cache
from typing import Optional, Pattern, Any

# é¢„ç¼–è¯‘å¸¸ç”¨çš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ï¼Œæå‡æ€§èƒ½
_BASIC_NUMBER_PATTERN: Pattern[str] = re.compile(r'(\d+(?:\.\d+)?)')
_DECIMAL_NUMBER_PATTERN: Pattern[str] = re.compile(r'(\d+(?:[.,]\d+)?)')
_NON_NUMERIC_PATTERN: Pattern[str] = re.compile(r'[^\d.,]')

# é…ç½®ç¼“å­˜ï¼Œé¿å…é‡å¤å¯¼å…¥
_config_cache: Optional[Any] = None


def _get_cached_config():
    """
    è·å–ç¼“å­˜çš„é…ç½®å¯¹è±¡ï¼Œæå‡æ€§èƒ½å¹¶å‡å°‘é‡å¤å¯¼å…¥

    Returns:
        é…ç½®å¯¹è±¡æˆ–Noneï¼ˆå¦‚æœæ— æ³•å¯¼å…¥ï¼‰
    """
    global _config_cache
    if _config_cache is None:
        try:
            from common.config.ozon_selectors_config import get_ozon_selectors_config
            _config_cache = get_ozon_selectors_config()
        except ImportError:
            # ğŸ”§ ä¿®å¤ï¼šå¯¼å…¥å¤±è´¥æ—¶æ ‡è®°ä¸º Falseï¼Œé¿å…é‡å¤å°è¯•
            _config_cache = False  # æ ‡è®°ä¸ºå¯¼å…¥å¤±è´¥
    return _config_cache if _config_cache is not False else None


@lru_cache(maxsize=128)
def _compile_prefix_pattern(prefix_words_tuple: tuple) -> Optional[Pattern[str]]:
    """
    ç¼–è¯‘å¹¶ç¼“å­˜ä»·æ ¼å‰ç¼€è¯æ­£åˆ™è¡¨è¾¾å¼

    Args:
        prefix_words_tuple: å‰ç¼€è¯å…ƒç»„ï¼ˆç”¨äºç¼“å­˜ï¼‰

    Returns:
        ç¼–è¯‘åçš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼æˆ–None
    """
    if not prefix_words_tuple:
        return None

    try:
        prefix_pattern = '|'.join(re.escape(prefix) for prefix in prefix_words_tuple)
        return re.compile(f'^({prefix_pattern})\\s+', re.IGNORECASE)
    except re.error:
        return None


@lru_cache(maxsize=128)
def _compile_currency_pattern(currency_symbols_tuple: tuple, space_chars_tuple: tuple) -> Pattern[str]:
    """
    ç¼–è¯‘å¹¶ç¼“å­˜è´§å¸ç¬¦å·å’Œç‰¹æ®Šç©ºæ ¼å­—ç¬¦æ¸…ç†æ­£åˆ™è¡¨è¾¾å¼

    Args:
        currency_symbols_tuple: è´§å¸ç¬¦å·å…ƒç»„
        space_chars_tuple: ç‰¹æ®Šç©ºæ ¼å­—ç¬¦å…ƒç»„

    Returns:
        ç¼–è¯‘åçš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
    """
    try:
        space_chars = ''.join(space_chars_tuple)
        if currency_symbols_tuple:
            currency_pattern = '|'.join(re.escape(symbol) for symbol in currency_symbols_tuple)
            return re.compile(f'[{re.escape(space_chars)}\\s]|({currency_pattern})', re.IGNORECASE)
        else:
            return re.compile(f'[{re.escape(space_chars)}\\s]')
    except re.error:
        return _NON_NUMERIC_PATTERN


def _parse_number_basic(price_str: str) -> Optional[float]:
    """
    åŸºç¡€æ•°å­—è§£ææ–¹æ³•ï¼Œç”¨äºé…ç½®ä¸å¯ç”¨æ—¶çš„fallback

    Args:
        price_str: ä»·æ ¼å­—ç¬¦ä¸²

    Returns:
        è§£æåçš„æ•°å­—æˆ–None
    """
    try:
        # ç§»é™¤æ‰€æœ‰éæ•°å­—å­—ç¬¦ï¼Œä¿ç•™å°æ•°ç‚¹å’Œé€—å·
        cleaned = _NON_NUMERIC_PATTERN.sub('', price_str)
        cleaned = cleaned.replace(',', '.')

        # æå–ç¬¬ä¸€ä¸ªæ•°å­—
        match = _BASIC_NUMBER_PATTERN.search(cleaned)
        if match:
            return float(match.group(1))
        return None
    except (ValueError, TypeError, AttributeError):
        return None


def clean_price_string(price_str: str, selectors_config=None) -> Optional[float]:
    """
    æ¸…ç†ä»·æ ¼å­—ç¬¦ä¸²å¹¶æå–æ•°å€¼ï¼ˆä¼˜åŒ–ç‰ˆï¼‰

    ä¼˜åŒ–å†…å®¹ï¼š
    1. é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼ï¼Œé¿å…é‡å¤ç¼–è¯‘å¼€é”€
    2. ç¼“å­˜é…ç½®å¯¹è±¡ï¼Œå‡å°‘é‡å¤å¯¼å…¥
    3. ç®€åŒ–æ§åˆ¶æµï¼Œæé«˜ä»£ç å¯è¯»æ€§
    4. å¢å¼ºå¼‚å¸¸å¤„ç†ï¼Œæä¾›æ›´å…·ä½“çš„é”™è¯¯ä¿¡æ¯
    5. æ·»åŠ æ€§èƒ½ä¼˜åŒ–çš„fallbackæœºåˆ¶

    Args:
        price_str (str): ä»·æ ¼å­—ç¬¦ä¸²
        selectors_config (Optional): é€‰æ‹©å™¨é…ç½®å¯¹è±¡ï¼ŒåŒ…å«è´§å¸ç¬¦å·ç­‰é…ç½®
                                   å¦‚æœä¸ºNoneï¼Œå°†å°è¯•è‡ªåŠ¨è·å–é»˜è®¤é…ç½®

    Returns:
        Optional[float]: æå–çš„ä»·æ ¼æ•°å€¼ï¼Œè§£æå¤±è´¥è¿”å›None

    Raises:
        ä¸ä¼šæŠ›å‡ºå¼‚å¸¸ï¼Œæ‰€æœ‰é”™è¯¯éƒ½ä¼šè¢«æ•è·å¹¶è¿”å›None
    """
    # è¾“å…¥éªŒè¯ - å¿«é€Ÿå¤±è´¥
    if not price_str or not isinstance(price_str, str):
        return None

    # è·å–é…ç½®å¯¹è±¡ï¼ˆä½¿ç”¨ç¼“å­˜æå‡æ€§èƒ½ï¼‰
    if selectors_config is None:
        selectors_config = _get_cached_config()

    # å¦‚æœé…ç½®ä¸å¯ç”¨ï¼Œä½¿ç”¨åŸºç¡€è§£ææ–¹æ³•
    if selectors_config is None:
        return _parse_number_basic(price_str)

    try:
        text = price_str.strip()

        # æ­¥éª¤1ï¼šç§»é™¤ä»·æ ¼å‰ç¼€è¯ï¼ˆä½¿ç”¨ç¼“å­˜çš„é¢„ç¼–è¯‘æ­£åˆ™ï¼‰
        if hasattr(selectors_config, 'price_prefix_words') and selectors_config.price_prefix_words:
            prefix_words_tuple = tuple(selectors_config.price_prefix_words)
            prefix_pattern = _compile_prefix_pattern(prefix_words_tuple)
            if prefix_pattern:
                text = prefix_pattern.sub('', text)

        # æ­¥éª¤2ï¼šç§»é™¤è´§å¸ç¬¦å·å’Œç‰¹æ®Šç©ºæ ¼å­—ç¬¦ï¼ˆä½¿ç”¨ç¼“å­˜çš„é¢„ç¼–è¯‘æ­£åˆ™ï¼‰
        if (hasattr(selectors_config, 'currency_symbols') and
                hasattr(selectors_config, 'special_space_chars')):
            currency_tuple = tuple(selectors_config.currency_symbols or [])
            space_tuple = tuple(selectors_config.special_space_chars or [])

            currency_pattern = _compile_currency_pattern(currency_tuple, space_tuple)
            text = currency_pattern.sub('', text)

        # æ­¥éª¤3ï¼šæ ‡å‡†åŒ–åƒä½åˆ†éš”ç¬¦å’Œç©ºæ ¼
        # å¤„ç†ä¿„è¯­ä¸­çš„çª„ç©ºæ ¼åƒä½åˆ†éš”ç¬¦
        text = text.replace(',', '.').replace(' ', '').replace('\u202f', '')

        # æ­¥éª¤4ï¼šæå–æ•°å­—ï¼ˆä½¿ç”¨é¢„ç¼–è¯‘æ­£åˆ™ï¼‰
        match = _DECIMAL_NUMBER_PATTERN.search(text)
        if match:
            number_str = match.group(1).replace(',', '.')
            return float(number_str)

        return None

    except (ValueError, TypeError, AttributeError, re.error) as e:
        # è®°å½•å…·ä½“é”™è¯¯ä½†ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œä¿æŒå‘åå…¼å®¹æ€§
        # æ³¨ï¼šè¿™é‡Œå¯ä»¥æ·»åŠ æ—¥å¿—è®°å½•ï¼Œä½†ä¸ºäº†é¿å…ä¾èµ–ï¼Œæš‚æ—¶çœç•¥
        return None
    except Exception:
        # æ•è·æ‰€æœ‰å…¶ä»–å¼‚å¸¸ï¼Œç¡®ä¿å‡½æ•°çš„å¥å£®æ€§
        return None
