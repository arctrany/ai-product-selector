"""
æ¯›å­ERPæ’ä»¶æŠ“å–å™¨

è´Ÿè´£ä»æ¯›å­ERPæ’ä»¶æ¸²æŸ“åŒºåŸŸæŠ“å–å•†å“çš„ç»“æ„åŒ–æ•°æ®ã€‚
æ”¯æŒå…±äº«browser_serviceå®ä¾‹ï¼Œä¾¿äºç‹¬ç«‹æµ‹è¯•ã€‚
"""

import asyncio
import logging
import time
import re
from typing import Dict, Any, Optional, List, Callable
from datetime import datetime

from .base_scraper import BaseScraper
from .global_browser_singleton import get_global_browser_service
from common.models.scraping_result import ScrapingResult as ScrapingResultImport
from common.models.scraping_result import ScrapingResult
from common.utils.wait_utils import WaitUtils
from common.utils.scraping_utils import ScrapingUtils
from common.config.erp_selectors_config import ERPSelectorsConfig, get_erp_selectors_config
from ..interfaces.scraper_interface import IERPScraper, ScrapingMode, StandardScrapingOptions
from ..exceptions.scraping_exceptions import ScrapingException, NavigationException, DataExtractionException

class ErpPluginScraper(BaseScraper, IERPScraper):
    """
    æ¯›å­ERPæ’ä»¶æŠ“å–å™¨ - ä½¿ç”¨å…¨å±€æµè§ˆå™¨å•ä¾‹

    å®ç°IERPScraperæ¥å£ï¼Œæä¾›æ ‡å‡†åŒ–çš„ERPæ•°æ®æŠ“å–åŠŸèƒ½
    """

    def __init__(self, selectors_config: Optional[ERPSelectorsConfig] = None, browser_service = None):
        """
        åˆå§‹åŒ–ERPæ’ä»¶æŠ“å–å™¨

        Args:
            selectors_config: ERPé€‰æ‹©å™¨é…ç½®å¯¹è±¡
            browser_service: å¯é€‰çš„å…±äº«æµè§ˆå™¨æœåŠ¡å®ä¾‹ï¼ˆå‘åå…¼å®¹ï¼Œæ¨èä½¿ç”¨å…¨å±€å•ä¾‹ï¼‰
        """
        super().__init__()
        self.selectors_config = selectors_config or get_erp_selectors_config()
        # ä¸ºäº†å…¼å®¹æµ‹è¯•ï¼Œæ·»åŠ configå±æ€§ï¼ˆæŒ‡å‘selectors_configï¼‰
        self.config = self.selectors_config
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")

        # ä½¿ç”¨å…¨å±€æµè§ˆå™¨å•ä¾‹
        if browser_service:
            self.browser_service = browser_service
            self._owns_browser_service = False  # ä¸æ‹¥æœ‰æµè§ˆå™¨æœåŠ¡ï¼Œä¸è´Ÿè´£å…³é—­
        else:
            self.browser_service = get_global_browser_service()
            self._owns_browser_service = False  # ä½¿ç”¨å…¨å±€å•ä¾‹ï¼Œä¸è´Ÿè´£å…³é—­
        
        # ğŸ”§ é‡æ„ï¼šåˆå§‹åŒ–ç»Ÿä¸€å·¥å…·ç±»
        self.wait_utils = WaitUtils(self.browser_service, self.logger)
        self.scraping_utils = ScrapingUtils(self.logger)
        
        # ERPåŒºåŸŸæ•°æ®å­—æ®µæ˜ å°„
        self.field_mappings = {
            'ç±»ç›®': 'category',
            'rFBSä½£é‡‘': 'rfbs_commission',
            'SKU': 'sku',
            'å“ç‰Œ': 'brand_name',
            'æœˆé”€é‡': 'monthly_sales_volume',
            'æœˆé”€å”®é¢': 'monthly_sales_amount',
            'æœˆå‘¨è½¬åŠ¨æ€': 'monthly_turnover_trend',
            'æ—¥é”€é‡': 'daily_sales_volume',
            'æ—¥é”€å”®é¢': 'daily_sales_amount',
            'å¹¿å‘Šè´¹å æ¯”': 'ad_cost_ratio',
            'å‚ä¸ä¿ƒé”€å¤©æ•°': 'promotion_days',
            'å‚ä¸ä¿ƒé”€çš„æŠ˜æ‰£': 'promotion_discount',
            'ä¿ƒé”€æ´»åŠ¨çš„è½¬åŒ–ç‡': 'promotion_conversion_rate',
            'ä»˜è´¹æ¨å¹¿å¤©æ•°': 'paid_promotion_days',
            'å•†å“å¡æµè§ˆé‡': 'product_card_views',
            'å•†å“å¡åŠ è´­ç‡': 'product_card_add_rate',
            'æœç´¢ç›®å½•æµè§ˆé‡': 'search_catalog_views',
            'æœç´¢ç›®å½•åŠ è´­ç‡': 'search_catalog_add_rate',
            'å±•ç¤ºè½¬åŒ–ç‡': 'display_conversion_rate',
            'å•†å“ç‚¹å‡»ç‡': 'product_click_rate',
            'å‘è´§æ¨¡å¼': 'shipping_mode',
            'é€€è´§å–æ¶ˆç‡': 'return_cancel_rate',
            'é•¿ å®½ é«˜': 'dimensions',
            'é‡ é‡': 'weight',
            'ä¸Šæ¶æ—¶é—´': 'listing_date',
            'è·Ÿå–åˆ—è¡¨': 'competitor_list',
            'è·Ÿå–æœ€ä½ä»·': 'competitor_min_price',
            'è·Ÿå–æœ€é«˜ä»·': 'competitor_max_price'
        }

    def scrape_erp_data(self,
                       product_url: str,
                       include_attributes: bool = True,
                       options: Optional[Dict[str, Any]] = None) -> ScrapingResult:
        """
        æŠ“å–ERPæ•°æ®ï¼ˆæ ‡å‡†æ¥å£å®ç°ï¼‰

        Args:
            product_url: å•†å“URL
            include_attributes: æ˜¯å¦åŒ…å«å•†å“å±æ€§
            options: æŠ“å–é€‰é¡¹

        Returns:
            ScrapingResult: ERPæ•°æ®æŠ“å–ç»“æœ

        Raises:
            NavigationException: é¡µé¢å¯¼èˆªå¤±è´¥
            DataExtractionException: æ•°æ®æå–å¤±è´¥
        """
        try:
            # è§£æé€‰é¡¹
            scraping_options = StandardScrapingOptions(**(options or {}))

            # ä½¿ç”¨å†…éƒ¨æ–¹æ³•è¿›è¡ŒæŠ“å–
            return self._scrape_comprehensive(
                product_url=product_url,
                include_attributes=include_attributes,
                **scraping_options.to_dict()
            )

        except Exception as e:
            raise DataExtractionException(
                field_name="erp_data",
                message=f"ERPæ•°æ®æŠ“å–å¤±è´¥: {str(e)}",
                context={'product_url': product_url, 'options': options},
                original_exception=e
            )

    # æ ‡å‡†scrapeæ¥å£å®ç°
    def scrape(self,
               target: Optional[str] = None,
               mode: Optional[ScrapingMode] = None,
               options: Optional[Dict[str, Any]] = None,
               **kwargs) -> ScrapingResult:
        """
        ç»Ÿä¸€çš„æŠ“å–æ¥å£ï¼ˆæ ‡å‡†æ¥å£å®ç°ï¼‰

        Args:
            target: æŠ“å–ç›®æ ‡ï¼ˆå•†å“URLï¼‰
            mode: æŠ“å–æ¨¡å¼
            options: æŠ“å–é€‰é¡¹é…ç½®
            **kwargs: é¢å¤–å‚æ•°

        Returns:
            ScrapingResult: æ ‡å‡†åŒ–æŠ“å–ç»“æœ

        Raises:
            ScrapingException: æŠ“å–å¼‚å¸¸
        """
        try:
            # è§£æé€‰é¡¹
            scraping_options = StandardScrapingOptions(**(options or {}))

            # æ ¹æ®æ¨¡å¼é€‰æ‹©æŠ“å–ç­–ç•¥
            if mode == ScrapingMode.ERP_DATA:
                return self.scrape_erp_data(
                    product_url=target,
                    include_attributes=kwargs.get('include_attributes', True),
                    options=options
                )
            elif mode == ScrapingMode.PRODUCT_ATTRIBUTES:
                return self.scrape_product_attributes(
                    product_url=target,
                    green_price=kwargs.get('green_price', None)
                )
            else:
                # é»˜è®¤ä½¿ç”¨ERPæ•°æ®æŠ“å–
                return self._scrape_comprehensive(
                    product_url=target,
                    include_attributes=kwargs.get('include_attributes', True),
                    **kwargs
                )

        except Exception as e:
            raise ScrapingException(
                message=f"æŠ“å–å¤±è´¥: {str(e)}",
                error_code="SCRAPING_FAILED",
                context={'target': target, 'mode': mode, 'options': options},
                original_exception=e
            )

    def _scrape_comprehensive(self,
                             product_url: Optional[str] = None,
                             include_attributes: bool = True,
                             **kwargs) -> ScrapingResult:
        """
        ç»¼åˆERPæ•°æ®æŠ“å–ï¼ˆå†…éƒ¨æ–¹æ³•ï¼Œä¿æŒå‘åå…¼å®¹ï¼‰

        Args:
            product_url: å¯é€‰çš„å•†å“URLï¼Œå¦‚æœæä¾›åˆ™å¯¼èˆªåˆ°è¯¥é¡µé¢ï¼Œå¦åˆ™ä»å½“å‰é¡µé¢æŠ“å–
            include_attributes: æ˜¯å¦åŒ…å«å•†å“å±æ€§
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            ScrapingResult: æŠ“å–ç»“æœï¼ŒåŒ…å«ç»“æ„åŒ–çš„ERPæ•°æ®
        """
        start_time = time.time()

        try:
            if product_url:
                # å¦‚æœæä¾›äº†URLï¼Œå¯¼èˆªå¹¶æŠ“å–é¡µé¢æ•°æ®
                success = self.navigate_to(product_url)
                if not success:
                    raise Exception("é¡µé¢å¯¼èˆªå¤±è´¥")

                # ç­‰å¾…é¡µé¢åŠ è½½
                self.wait(1)

            # æ™ºèƒ½ç­‰å¾…ERPæ’ä»¶åŠ è½½å®Œæˆ
            self._wait_for_erp_plugin_loaded()

            # è·å–é¡µé¢å†…å®¹ - ä½¿ç”¨åŒæ­¥æ–¹æ³•
            page_content = self.browser_service.evaluate_sync("() => document.documentElement.outerHTML")

            if not page_content:
                return ScrapingResult(
                    success=False,
                    data={},
                    error_message="æœªèƒ½è·å–å½“å‰é¡µé¢å†…å®¹",
                    execution_time=time.time() - start_time
                )

            # è§£æERPä¿¡æ¯
            erp_data = self._extract_erp_data()

            return ScrapingResult(
                success=True,
                data=erp_data,
                execution_time=time.time() - start_time
            )
            
        except Exception as e:
            self.logger.error(f"æŠ“å–ERPä¿¡æ¯å¤±è´¥: {e}")
            return ScrapingResult(
                success=False,
                data={},
                error_message=str(e),
                execution_time=time.time() - start_time
            )

    def _extract_erp_data(self, *args, **kwargs) -> Dict[str, Any]:
        """
        æå–ERPæ•°æ®çš„å…¥å£æ–¹æ³•ï¼ˆæµ‹è¯•æ¥å£å…¼å®¹æ€§ï¼‰

        Returns:
            Dict[str, Any]: æå–çš„ERPæ•°æ®
        """
        try:
            # è·å–é¡µé¢å†…å®¹
            page_content = self.browser_service.evaluate_sync("() => document.documentElement.outerHTML")
            if not page_content:
                return {}

            # è°ƒç”¨å®é™…çš„æå–æ–¹æ³•
            return self._extract_erp_data_from_content(page_content)
        except Exception as e:
            self.logger.error(f"æå–ERPæ•°æ®å¤±è´¥: {e}")
            return {}

    def _extract_erp_data_from_content(self, page_content: str) -> Dict[str, Any]:
        """
        ä»é¡µé¢å†…å®¹ä¸­æå–ERPæ•°æ®
        
        Args:
            page_content: é¡µé¢HTMLå†…å®¹
            
        Returns:
            Dict[str, Any]: æå–çš„ERPæ•°æ®
        """
        try:
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(page_content, 'html.parser')
            
            erp_data = {}
            
            # æŸ¥æ‰¾ERPæ’ä»¶åŒºåŸŸ
            erp_container = self._find_erp_container(soup)
            if not erp_container:
                self.logger.warning("æœªæ‰¾åˆ°ERPæ’ä»¶åŒºåŸŸ")
                return {}
            
            # æå–æ‰€æœ‰æ•°æ®å­—æ®µ
            for label_text, field_key in self.field_mappings.items():
                value = self._extract_field_value(erp_container, label_text)
                if value is not None:
                    erp_data[field_key] = value
            
            # ç‰¹æ®Šå¤„ç†ï¼šè§£æå°ºå¯¸ä¿¡æ¯
            if 'dimensions' in erp_data:
                dimensions = self._parse_dimensions(erp_data['dimensions'])
                erp_data.update(dimensions)
            
            # ç‰¹æ®Šå¤„ç†ï¼šè§£æä¸Šæ¶æ—¶é—´
            if 'listing_date' in erp_data:
                parsed_date = self._parse_listing_date(erp_data['listing_date'])
                erp_data.update(parsed_date)
            
            # ç‰¹æ®Šå¤„ç†ï¼šè§£æé‡é‡
            if 'weight' in erp_data:
                weight_value = self._parse_weight(erp_data['weight'])
                if weight_value is not None:
                    erp_data['weight'] = weight_value
            
            # ç‰¹æ®Šå¤„ç†ï¼šè§£ærFBSä½£é‡‘
            if 'rfbs_commission' in erp_data:
                commission_rates = self._parse_rfbs_commission(erp_data['rfbs_commission'])
                erp_data['rfbs_commission_rates'] = commission_rates
            
            return erp_data
            
        except Exception as e:
            self.logger.error(f"è§£æERPæ•°æ®å¤±è´¥: {e}")
            return {}

    def _find_erp_container(self, soup) -> Optional[Any]:
        """æŸ¥æ‰¾ERPæ’ä»¶å®¹å™¨"""
        from common.config.erp_selectors_config import get_erp_selectors_config

        # ä½¿ç”¨é…ç½®åŒ–çš„é€‰æ‹©å™¨ï¼Œè€Œä¸æ˜¯ç¡¬ç¼–ç 
        erp_config = get_erp_selectors_config()
        selectors = erp_config.erp_container_selectors
        
        for selector in selectors:
            container = soup.select_one(selector)
            if container:
                return container
        
        return None

    def _extract_field_value(self, container: Any, label_text: str) -> Optional[str]:
        """
        ä»å®¹å™¨ä¸­æå–æŒ‡å®šæ ‡ç­¾çš„å€¼
        
        Args:
            container: BeautifulSoupå®¹å™¨å¯¹è±¡
            label_text: æ ‡ç­¾æ–‡æœ¬
            
        Returns:
            Optional[str]: æå–çš„å€¼ï¼Œå¦‚æœæœªæ‰¾åˆ°è¿”å›None
        """
        try:
            # æŸ¥æ‰¾åŒ…å«æ ‡ç­¾æ–‡æœ¬çš„å…ƒç´ 
            label_elements = container.find_all(string=re.compile(f'{re.escape(label_text)}ï¼š?\\s*'))
            
            for label_element in label_elements:
                # è·å–çˆ¶å…ƒç´ 
                parent = label_element.parent
                if not parent:
                    continue
                
                # æŸ¥æ‰¾åŒçº§æˆ–å­çº§çš„å€¼å…ƒç´ 
                value_element = None
                
                # æ–¹æ³•1ï¼šæŸ¥æ‰¾åŒçº§spanå…ƒç´ 
                next_span = parent.find_next_sibling('span')
                if next_span:
                    value_element = next_span
                
                # æ–¹æ³•2ï¼šæŸ¥æ‰¾çˆ¶å…ƒç´ å†…çš„å…¶ä»–span
                if not value_element:
                    spans = parent.find_all('span')
                    for span in spans:
                        if span.get_text(strip=True) != label_text.rstrip('ï¼š'):
                            value_element = span
                            break
                
                # æ–¹æ³•3ï¼šæŸ¥æ‰¾çˆ¶å…ƒç´ çš„ä¸‹ä¸€ä¸ªdivä¸­çš„span
                if not value_element:
                    parent_div = parent.find_parent('div')
                    if parent_div:
                        next_div = parent_div.find_next_sibling('div')
                        if next_div:
                            value_span = next_div.find('span')
                            if value_span:
                                value_element = value_span
                
                if value_element:
                    value_text = value_element.get_text(strip=True)
                    # è¿‡æ»¤æ— æ•ˆå€¼
                    if value_text and value_text not in ['-', 'æ— æ•°æ®', 'N/A', '']:
                        return value_text
            
            return None
            
        except Exception as e:
            self.logger.error(f"æå–å­—æ®µ {label_text} å¤±è´¥: {e}")
            return None

    def _parse_dimensions(self, dimensions_str: str) -> Dict[str, Optional[float]]:
        """
        è§£æå°ºå¯¸å­—ç¬¦ä¸²
        
        Args:
            dimensions_str: å°ºå¯¸å­—ç¬¦ä¸²ï¼Œå¦‚ "50 x 37 x 43mm"
            
        Returns:
            Dict[str, Optional[float]]: åŒ…å«length, width, heightçš„å­—å…¸
        """
        result = {'length': None, 'width': None, 'height': None}
        
        try:
            if not dimensions_str:
                return result
            
            # ç§»é™¤å•ä½å¹¶åˆ†å‰²
            clean_str = re.sub(r'[a-zA-Z]+$', '', dimensions_str.strip())
            parts = re.split(r'\s*[xÃ—]\s*', clean_str)
            
            if len(parts) >= 3:
                result['length'] = float(parts[0])
                result['width'] = float(parts[1])
                result['height'] = float(parts[2])
            
        except (ValueError, IndexError) as e:
            self.logger.warning(f"è§£æå°ºå¯¸å¤±è´¥: {dimensions_str}, é”™è¯¯: {e}")
        
        return result

    def _parse_listing_date(self, date_str: str) -> Dict[str, Optional[Any]]:
        """
        è§£æä¸Šæ¶æ—¶é—´
        
        Args:
            date_str: æ—¶é—´å­—ç¬¦ä¸²ï¼Œå¦‚ "2024-09-23(415å¤©)"
            
        Returns:
            Dict[str, Optional[Any]]: åŒ…å«listing_date_parsedå’Œshelf_daysçš„å­—å…¸
        """
        result = {'listing_date_parsed': None, 'shelf_days': None}

        try:
            if not date_str:
                return result

            # æå–æ—¥æœŸéƒ¨åˆ†
            date_match = re.search(r'(\d{4}-\d{2}-\d{2})', date_str)
            if date_match:
                date_part = date_match.group(1)
                result['listing_date_parsed'] = date_part  # ç›´æ¥è¿”å›å­—ç¬¦ä¸²è€Œä¸æ˜¯dateå¯¹è±¡
            
            # æå–å¤©æ•°éƒ¨åˆ†
            days_match = re.search(r'\((\d+)å¤©\)', date_str)
            if days_match:
                result['shelf_days'] = int(days_match.group(1))
            
        except (ValueError, AttributeError) as e:
            self.logger.warning(f"è§£æä¸Šæ¶æ—¶é—´å¤±è´¥: {date_str}, é”™è¯¯: {e}")
        
        return result

    def _parse_weight(self, weight_str: str) -> Optional[float]:
        """
        è§£æé‡é‡å­—ç¬¦ä¸²
        
        Args:
            weight_str: é‡é‡å­—ç¬¦ä¸²ï¼Œå¦‚ "40g"
            
        Returns:
            Optional[float]: é‡é‡å€¼ï¼ˆå…‹ï¼‰ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            if not weight_str:
                return None
            
            # æå–æ•°å­—éƒ¨åˆ†
            weight_match = re.search(r'(\d+(?:\.\d+)?)', weight_str)
            if weight_match:
                return float(weight_match.group(1))
            
        except (ValueError, AttributeError) as e:
            self.logger.warning(f"è§£æé‡é‡å¤±è´¥: {weight_str}, é”™è¯¯: {e}")
        
        return None

    def _parse_rfbs_commission(self, commission_str: str) -> Optional[List[float]]:
        """
        è§£ærFBSä½£é‡‘å­—ç¬¦ä¸²

        Args:
            commission_str: ä½£é‡‘å­—ç¬¦ä¸²

        Returns:
            Optional[List[float]]: ä½£é‡‘ç‡åˆ—è¡¨ï¼Œå¦‚æœæ— æ³•æå–åˆ™è¿”å›None
        """
        try:
            if not commission_str:
                return None

            # å°è¯•ä»å­—ç¬¦ä¸²ä¸­æå–æ•°å­—
            rates = re.findall(r'(\d+(?:\.\d+)?)%?', commission_str)
            if rates:
                return [float(rate) for rate in rates]

            # å¦‚æœæ— æ³•æå–åˆ°æ•°å­—ï¼Œè¿”å›Noneè€Œä¸æ˜¯é»˜è®¤å€¼
            return None

        except Exception as e:
            self.logger.warning(f"è§£æä½£é‡‘ç‡å¤±è´¥: {commission_str}, é”™è¯¯: {e}")
            return None

    def _wait_for_erp_plugin_loaded(self, max_wait_seconds: int = 10) -> bool:
        """
        æ™ºèƒ½ç­‰å¾…ERPæ’ä»¶åŠ è½½å®Œæˆ

        Args:
            max_wait_seconds: æœ€å¤§ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰

        Returns:
            bool: æ˜¯å¦æˆåŠŸåŠ è½½
        """
        start_time = time.time()
        check_interval = 0.5  # æ¯0.5ç§’æ£€æŸ¥ä¸€æ¬¡

        while time.time() - start_time < max_wait_seconds:
            try:
                # è·å–é¡µé¢å†…å®¹ - ä½¿ç”¨åŒæ­¥æ–¹æ³•
                page_content = self.browser_service.evaluate_sync("() => document.documentElement.outerHTML")
                if not page_content:
                    self.wait(check_interval)
                    continue

                # æ£€æŸ¥ERPæ’ä»¶åŒºåŸŸæ˜¯å¦å­˜åœ¨
                from bs4 import BeautifulSoup
                soup = BeautifulSoup(page_content, 'html.parser')

                # ä½¿ç”¨å¤šç§é€‰æ‹©å™¨æ£€æŸ¥ERPåŒºåŸŸ
                erp_selectors = [
                    '[data-v-efec3aa9]',  # ä»HTMLä¸­è§‚å¯Ÿåˆ°çš„ç‰¹å¾å±æ€§
                    '.erp-plugin',
                    '[class*="erp"]',
                    '[id*="erp"]'
                ]

                for selector in erp_selectors:
                    erp_elements = soup.select(selector)
                    if erp_elements:
                        # æ£€æŸ¥æ˜¯å¦æœ‰å®é™…çš„æ•°æ®å†…å®¹ï¼ˆä¸åªæ˜¯ç©ºçš„å®¹å™¨ï¼‰
                        for element in erp_elements:
                            text_content = element.get_text(strip=True)
                            if text_content and len(text_content) > 50:  # æœ‰è¶³å¤Ÿçš„æ–‡æœ¬å†…å®¹
                                self.logger.info(f"âœ… ERPæ’ä»¶åŠ è½½å®Œæˆï¼Œè€—æ—¶: {time.time() - start_time:.2f}ç§’")
                                return True

                # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œç»§ç»­ç­‰å¾…
                self.wait(check_interval)

            except Exception as e:
                self.logger.debug(f"æ£€æŸ¥ERPæ’ä»¶çŠ¶æ€æ—¶å‡ºé”™: {e}")
                self.wait(check_interval)

        # è¶…æ—¶
        self.logger.warning(f"âš ï¸ ERPæ’ä»¶åŠ è½½è¶…æ—¶ï¼ˆ{max_wait_seconds}ç§’ï¼‰ï¼Œç»§ç»­å°è¯•æŠ“å–")
        return False

    def close(self):
        """å…³é—­èµ„æº - ä½¿ç”¨å…¨å±€å•ä¾‹æ—¶ä¸éœ€è¦å…³é—­"""
        # ä½¿ç”¨å…¨å±€å•ä¾‹æ—¶ä¸éœ€è¦ä¸»åŠ¨å…³é—­æµè§ˆå™¨æœåŠ¡
        # å…¨å±€å•ä¾‹çš„ç”Ÿå‘½å‘¨æœŸç”±åº”ç”¨ç¨‹åºç®¡ç†
        pass

    def __enter__(self):
        return self

    def scrape_product_attributes(self, product_url: str, green_price: Optional[float] = None) -> ScrapingResult:
        """
        æŠ“å–å•†å“å±æ€§ä¿¡æ¯

        Args:
            product_url: å•†å“é¡µé¢URL
            green_price: å•†å“ç»¿æ ‡ä»·æ ¼ï¼ˆç”¨äºä½£é‡‘ç‡è®¡ç®—ï¼‰

        Returns:
            ScrapingResult: æŠ“å–ç»“æœï¼ŒåŒ…å«å•†å“å±æ€§ä¿¡æ¯
        """
        start_time = time.time()

        try:
            # å¯¼èˆªåˆ°å•†å“é¡µé¢
            success = self.navigate_to(product_url)
            if not success:
                return ScrapingResult(
                    success=False,
                    data={},
                    error_message="é¡µé¢å¯¼èˆªå¤±è´¥",
                    execution_time=time.time() - start_time
                )

            # ç­‰å¾…é¡µé¢åŠ è½½
            self.wait(1)

            # æ™ºèƒ½ç­‰å¾…ERPæ’ä»¶åŠ è½½å®Œæˆ
            self._wait_for_erp_plugin_loaded()

            # è·å–é¡µé¢å†…å®¹
            page_content = self.browser_service.evaluate_sync("() => document.documentElement.outerHTML")
            if not page_content:
                return ScrapingResult(
                    success=False,
                    data={},
                    error_message="æœªèƒ½è·å–é¡µé¢å†…å®¹",
                    execution_time=time.time() - start_time
                )

            # è§£æERPæ•°æ®
            erp_data = self._extract_erp_data_from_content(page_content)

            # æå–éœ€è¦çš„å±æ€§ä¿¡æ¯
            attributes = {}

            # ä½£é‡‘ç‡
            if 'rfbs_commission_rates' in erp_data and erp_data['rfbs_commission_rates']:
                # ä½¿ç”¨ç¬¬ä¸€ä¸ªä½£é‡‘ç‡ä½œä¸ºé»˜è®¤å€¼
                attributes['commission_rate'] = erp_data['rfbs_commission_rates'][0]
            elif green_price:
                # å¦‚æœæ²¡æœ‰ä½£é‡‘ç‡ä½†æœ‰ç»¿æ ‡ä»·æ ¼ï¼Œå¯ä»¥æ ¹æ®ä»·æ ¼è®¡ç®—é»˜è®¤ä½£é‡‘ç‡
                attributes['commission_rate'] = self._calculate_commission_rate_by_price(green_price)
            else:
                # ä½¿ç”¨é»˜è®¤ä½£é‡‘ç‡
                attributes['commission_rate'] = 12.0  # é»˜è®¤12%

            # é‡é‡
            if 'weight' in erp_data and erp_data['weight']:
                attributes['weight'] = float(erp_data['weight'])

            # å°ºå¯¸ä¿¡æ¯
            if 'length' in erp_data and erp_data['length']:
                attributes['length'] = float(erp_data['length'])
            if 'width' in erp_data and erp_data['width']:
                attributes['width'] = float(erp_data['width'])
            if 'height' in erp_data and erp_data['height']:
                attributes['height'] = float(erp_data['height'])

            # ä¸Šæ¶å¤©æ•°
            if 'shelf_days' in erp_data and erp_data['shelf_days']:
                attributes['shelf_days'] = int(erp_data['shelf_days'])

            return ScrapingResult(
                success=True,
                data=attributes,
                execution_time=time.time() - start_time
            )

        except Exception as e:
            self.logger.error(f"æŠ“å–å•†å“å±æ€§å¤±è´¥: {e}")
            return ScrapingResult(
                success=False,
                data={},
                error_message=str(e),
                execution_time=time.time() - start_time
            )

    def _calculate_commission_rate_by_price(self, price: float) -> float:
        """
        æ ¹æ®ä»·æ ¼è®¡ç®—ä½£é‡‘ç‡

        Args:
            price: å•†å“ä»·æ ¼ï¼ˆå¢å¸ƒï¼‰

        Returns:
            float: ä½£é‡‘ç‡ï¼ˆç™¾åˆ†æ¯”ï¼‰
        """
        try:
            # ğŸ”§ é‡æ„ï¼šä½¿ç”¨ç¡¬ç¼–ç é˜ˆå€¼ï¼Œç¬¦åˆæ¶æ„åˆ†ç¦»åŸåˆ™
            if price <= 500:  # ä½ä»·å•†å“é˜ˆå€¼500å¢å¸ƒ
                return 15.0  # ä½ä»·å•†å“ä½£é‡‘ç‡15%
            elif price >= 2000:  # é«˜ä»·å•†å“é˜ˆå€¼2000å¢å¸ƒ
                return 8.0   # é«˜ä»·å•†å“ä½£é‡‘ç‡8%
            else:
                return 12.0  # ä¸­ç­‰ä»·æ ¼å•†å“ä½£é‡‘ç‡12%
        except Exception as e:
            self.logger.warning(f"è®¡ç®—ä½£é‡‘ç‡å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼: {e}")
            return 12.0

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close()

    # ========== æŠ½è±¡æ–¹æ³•å®ç° ==========

    def extract_data(self,
                    selectors: Optional[Dict[str, str]] = None,
                    options: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        ä»å½“å‰é¡µé¢æå–æ•°æ®ï¼ˆæŠ½è±¡æ–¹æ³•å®ç°ï¼‰

        Args:
            selectors: é€‰æ‹©å™¨æ˜ å°„
            options: æå–é€‰é¡¹

        Returns:
            Dict[str, Any]: æå–çš„æ•°æ®
        """
        try:
            # è·å–é¡µé¢å†…å®¹
            page_content = self.browser_service.evaluate_sync("() => document.documentElement.outerHTML")
            if not page_content:
                return {}

            # ä½¿ç”¨ç°æœ‰çš„ERPæ•°æ®æå–é€»è¾‘
            erp_data = self._extract_erp_data_from_content(page_content)

            return erp_data

        except Exception as e:
            self.logger.error(f"æ•°æ®æå–å¤±è´¥: {e}")
            return {}

    def validate_data(self, data: Dict[str, Any],
                     filters: Optional[List[Callable]] = None) -> bool:
        """
        éªŒè¯æå–çš„æ•°æ®ï¼ˆæŠ½è±¡æ–¹æ³•å®ç°ï¼‰

        Args:
            data: å¾…éªŒè¯çš„æ•°æ®
            filters: éªŒè¯è¿‡æ»¤å™¨åˆ—è¡¨

        Returns:
            bool: æ•°æ®æ˜¯å¦æœ‰æ•ˆ
        """
        try:
            # åŸºæœ¬éªŒè¯ï¼šæ•°æ®ä¸ä¸ºç©º
            if not data:
                return False

            # éªŒè¯ERPæ•°æ®çš„å…³é”®å­—æ®µ
            erp_fields = ['category', 'sku', 'brand_name', 'monthly_sales_volume', 'monthly_sales_amount']
            has_valid_field = False

            for field in erp_fields:
                if field in data and data[field] is not None:
                    has_valid_field = True
                    break

            if not has_valid_field:
                self.logger.warning("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ERPæ•°æ®å­—æ®µ")
                return False

            # éªŒè¯æ•°å€¼å­—æ®µçš„åˆç†æ€§
            numeric_fields = ['monthly_sales_volume', 'monthly_sales_amount', 'daily_sales_volume', 'daily_sales_amount']
            for field in numeric_fields:
                if field in data:
                    try:
                        value = float(data[field]) if data[field] is not None else 0
                        if value < 0:
                            self.logger.warning(f"å­—æ®µ {field} å€¼ä¸ºè´Ÿæ•°: {value}")
                            return False
                    except (ValueError, TypeError):
                        self.logger.warning(f"å­—æ®µ {field} å€¼æ— æ³•è½¬æ¢ä¸ºæ•°å­—: {data[field]}")

            # åº”ç”¨è‡ªå®šä¹‰è¿‡æ»¤å™¨
            if filters:
                for filter_func in filters:
                    if not filter_func(data):
                        return False

            return True

        except Exception as e:
            self.logger.error(f"æ•°æ®éªŒè¯å¤±è´¥: {e}")
            return False

    def get_health_status(self) -> Dict[str, Any]:
        """
        è·å–Scraperå¥åº·çŠ¶æ€ï¼ˆæŠ½è±¡æ–¹æ³•å®ç°ï¼‰

        Returns:
            Dict[str, Any]: å¥åº·çŠ¶æ€ä¿¡æ¯
        """
        try:
            status = {
                'scraper_name': 'ErpPluginScraper',
                'status': 'healthy',
                'browser_service_available': self.browser_service is not None,
                'selectors_config_loaded': self.selectors_config is not None,
                'field_mappings_count': len(self.field_mappings)
            }

            # æ£€æŸ¥æµè§ˆå™¨æœåŠ¡çŠ¶æ€
            if self.browser_service:
                try:
                    # ç®€å•æ£€æŸ¥æµè§ˆå™¨æ˜¯å¦å“åº”
                    page_url = self.browser_service.evaluate_sync("() => window.location.href")
                    status['browser_responsive'] = page_url is not None
                    status['current_url'] = page_url

                    # æ£€æŸ¥ERPæ’ä»¶æ˜¯å¦å­˜åœ¨
                    status['erp_plugin_detected'] = self._wait_for_erp_plugin_loaded(max_wait_seconds=1)
                except:
                    status['browser_responsive'] = False
                    status['status'] = 'degraded'
                    status['erp_plugin_detected'] = False
            else:
                status['status'] = 'unavailable'
                status['browser_responsive'] = False
                status['erp_plugin_detected'] = False

            return status

        except Exception as e:
            return {
                'scraper_name': 'ErpPluginScraper',
                'status': 'error',
                'error': str(e)
            }

    def wait_for_erp_plugin(self, timeout: int = 30) -> bool:
        """
        ç­‰å¾…ERPæ’ä»¶åŠ è½½å®Œæˆï¼ˆæŠ½è±¡æ–¹æ³•å®ç°ï¼‰

        Args:
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

        Returns:
            bool: æ’ä»¶æ˜¯å¦åŠ è½½æˆåŠŸ
        """
        try:
            return self._wait_for_erp_plugin_loaded(max_wait_seconds=timeout)
        except Exception as e:
            self.logger.error(f"ç­‰å¾…ERPæ’ä»¶åŠ è½½å¤±è´¥: {e}")
            return False
