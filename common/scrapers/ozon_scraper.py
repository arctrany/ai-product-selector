"""
OZONå¹³å°æŠ“å–å™¨

è´Ÿè´£ä»OZONå¹³å°æŠ“å–å•†å“ä»·æ ¼ä¿¡æ¯å’Œè·Ÿå–åº—é“ºæ•°æ®ã€‚
åŸºäºæ–°çš„browser_serviceæ¶æ„ã€‚

é‡æ„ç‰ˆæœ¬ï¼šé›†æˆCompetitorDetectionServiceï¼Œä½¿ç”¨ç»Ÿä¸€å·¥å…·ç±»
"""
import logging
import time
from typing import Dict, Any, List, Optional, Tuple
from bs4 import BeautifulSoup

from .base_scraper import BaseScraper
from rpa.browser.browser_service import SimplifiedBrowserService

from ..models import ScrapingResult
from ..config import GoodStoreSelectorConfig
from ..config.ozon_selectors_config import get_ozon_selectors_config, OzonSelectorsConfig
from ..config.currency_config import get_currency_config
# å»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯ä¾èµ–
def get_profit_evaluator():
    from common.business.profit_evaluator import ProfitEvaluator
    return ProfitEvaluator

def get_erp_plugin_scraper():
    from .erp_plugin_scraper import ErpPluginScraper
    return ErpPluginScraper
from ..utils.wait_utils import WaitUtils, wait_for_content_smart
from ..utils.scraping_utils import ScrapingUtils
from ..business.filter_manager import FilterManager


def _upd_competitor_cnt(data: Dict[str, Any], context: Optional[Dict[str, Any]] = None):
    if not context:
        return
    competitor_cnt = 0
    if data.get('competitor_data'):
      competitor_cnt = data['competitor_data'].get('competitor_cnt', 0)
    elif data.get('erp_data'):
       competitor_cnt = data['erp_data'].get('competitor_cnt', 0)
    context.update({'competitor_cnt': competitor_cnt})




class OzonScraper(BaseScraper):
    """
    OZONå¹³å°æŠ“å–å™¨ - åŸºäºbrowser_serviceæ¶æ„

    å®ç°IProductScraperæ¥å£ï¼Œæä¾›æ ‡å‡†åŒ–çš„å•†å“ä¿¡æ¯æŠ“å–åŠŸèƒ½
    """

    def __init__(self, config: Optional[GoodStoreSelectorConfig] = None,
                 selectors_config: Optional[OzonSelectorsConfig] = None,
                 browser_service=None):
        """åˆå§‹åŒ–OZONæŠ“å–å™¨"""
        super().__init__()
        self.config = config or GoodStoreSelectorConfig()
        self.selectors_config = selectors_config or get_ozon_selectors_config()
        self.currency_config = get_currency_config()
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        self.base_url = self.config.browser.ozon_base_url
        self.browser_service = browser_service or SimplifiedBrowserService.get_global_instance()
        # ğŸ”§ é‡æ„ï¼šé›†æˆç»Ÿä¸€å·¥å…·ç±»
        self.wait_utils = WaitUtils(self.browser_service, self.logger)
        self.scraping_utils = ScrapingUtils(self.logger)
        ErpPluginScraper = get_erp_plugin_scraper()
        self.erp_scraper = ErpPluginScraper(self.config, self.browser_service)
        
        # ğŸ¯ é›†æˆè¿‡æ»¤ç®¡ç†å™¨å’Œåˆ©æ¶¦è¯„ä¼°å™¨
        self.filter_manager = FilterManager(self.config)
        ProfitEvaluator = get_profit_evaluator()
        # æ³¨æ„ï¼šProfitEvaluatoréœ€è¦profit_calculator_pathï¼Œè¿™é‡Œå…ˆè®¾ä¸ºNoneï¼Œå®é™…ä½¿ç”¨æ—¶éœ€è¦ä¼ å…¥
        self.profit_evaluator = None

    # æ ‡å‡†scrapeæ¥å£å®ç°
    def scrape(self, target: str,
               context: Optional[Dict[str, Any]] = None, 
               include_competitor: bool = False,
               **kwargs) -> ScrapingResult:
        """ç»Ÿä¸€çš„æŠ“å–æ¥å£ - æ”¯æŒè·Ÿå–å•†å“åˆ†æ
        
        Args:
            target: å•†å“URL
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
            include_competitor: æ˜¯å¦åŒ…å«è·Ÿå–å•†å“åˆ†æ
            **kwargs: å…¶ä»–å‚æ•°
        """
        start_time = time.time()

        try:
            # ç›´æ¥å¯¼èˆªåˆ°ç›®æ ‡é¡µé¢
            if not self.navigate_to(target):
                return ScrapingResult(
                    success=False,
                    data={},
                    error_message=f"æ— æ³•å¯¼èˆªåˆ°å•†å“é¡µé¢: {target}",
                    execution_time=time.time() - start_time
                )

            # æ ¹æ®include_competitorå‚æ•°é€‰æ‹©å¤„ç†æ–¹å¼
            if include_competitor:
                data = self._scrape_with_competitor_analysis(target, context, **kwargs)
            else:
                # æ£€æŸ¥æ˜¯å¦è·³è¿‡è·Ÿå–ä¿¡æ¯æŠ“å–
                skip_competitors = kwargs.get('skip_competitors', False)
                data = self._extract_basic_product_info(target, context, skip_competitors=skip_competitors)

            return ScrapingResult(
                success=True,
                data=data,
                execution_time=time.time() - start_time
            )

        except ValueError as e:
            raise ValueError(f"å‚æ•°é”™è¯¯: {str(e)}")
        except Exception as e:
            raise RuntimeError(f"æŠ“å–å¤±è´¥: {str(e)}")
    
    def _scrape_with_competitor_analysis(self, target: str, context: Optional[Dict[str, Any]] = None, **kwargs) -> Dict[str, Any]:
        """å®Œæ•´çš„è·Ÿå–å•†å“åˆ†ææµç¨‹"""
        try:
            # 1. æŠ“å–åŸºç¡€å•†å“ä¿¡æ¯
            basic_data = self._extract_basic_product_info(target, context, skip_competitors=False)
            
            # 2. å•†å“è¿‡æ»¤æ£€æŸ¥
            if not self._should_analyze_competitor(basic_data):
                return {
                    "primary_product": basic_data,
                    "selected_product": basic_data,
                    "is_competitor": False,
                    "selection_reason": "å•†å“è¢«è¿‡æ»¤æˆ–æ— ä»·æ ¼ä¼˜åŠ¿",
                    "analysis_type": "filtered_out"
                }
            
            # 3. è·å–è·Ÿå–ä¿¡æ¯ - ä½¿ç”¨CompetitorScraper
            try:
                from .competitor_scraper import CompetitorScraper
                competitor_scraper = CompetitorScraper(browser_service=self.browser_service)
                competitor_result = competitor_scraper.scrape(
                    target, 
                    context=context, 
                    extract_first_product=True,
                    **kwargs
                )
                
                if not competitor_result.success:
                    return {
                        "primary_product": basic_data,
                        "selected_product": basic_data,
                        "is_competitor": False,
                        "selection_reason": f"è·Ÿå–ä¿¡æ¯æŠ“å–å¤±è´¥: {competitor_result.error_message}",
                        "analysis_type": "competitor_fetch_failed"
                    }
                
                competitor_data = competitor_result.data
                first_competitor_product_id = competitor_data.get('first_competitor_product_id')
                
                if not first_competitor_product_id:
                    return {
                        "primary_product": basic_data,
                        "selected_product": basic_data,
                        "is_competitor": False,
                        "selection_reason": "æœªæ‰¾åˆ°è·Ÿå–å•†å“ID",
                        "analysis_type": "no_competitor_id"
                    }
                
                # è¿”å›åŸå•†å“æ•°æ®å’Œfirst_competitor_product_idï¼Œç”±åè°ƒå™¨è´Ÿè´£åç»­å¤„ç†
                return {
                    "primary_product": basic_data,
                    "first_competitor_product_id": first_competitor_product_id,
                    "competitors": competitor_data.get('competitors', []),
                    "analysis_type": "ready_for_competitor_details"
                }
                
            except ImportError:
                self.logger.error("CompetitorScraperä¸å¯ç”¨")
                return {
                    "primary_product": basic_data,
                    "selected_product": basic_data,
                    "is_competitor": False,
                    "selection_reason": "CompetitorScraperä¸å¯ç”¨",
                    "analysis_type": "scraper_unavailable"
                }
                
        except Exception as e:
            self.logger.error(f"è·Ÿå–å•†å“åˆ†æå¤±è´¥: {e}")
            # é™çº§è¿”å›åŸºç¡€æ•°æ®
            basic_data = self._extract_basic_product_info(target, context, skip_competitors=True)
            return {
                "primary_product": basic_data,
                "selected_product": basic_data,
                "is_competitor": False,
                "selection_reason": f"åˆ†æå¼‚å¸¸: {str(e)}",
                "analysis_type": "analysis_failed"
            }
    
    def _should_analyze_competitor(self, product_data: Dict[str, Any]) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦è¿›è¡Œè·Ÿå–åˆ†æ"""
        try:
            # 1. å•†å“è¿‡æ»¤æ£€æŸ¥
            product_filter = self.filter_manager.get_product_filter_func()
            if not product_filter(product_data):
                self.logger.info("å•†å“æœªé€šè¿‡è¿‡æ»¤æ£€æŸ¥ï¼Œè·³è¿‡è·Ÿå–åˆ†æ")
                return False
            
            # 2. ä»·æ ¼ä¼˜åŠ¿åˆ¤æ–­ï¼ˆå¤ç”¨ç°æœ‰é€»è¾‘ï¼‰
            if self.profit_evaluator:
                try:
                    has_better_price = self.profit_evaluator.has_better_competitor_price({
                        'price_data': product_data
                    })
                    
                    if not has_better_price:
                        self.logger.info("è·Ÿå–ä»·æ ¼æ— ä¼˜åŠ¿ï¼Œè·³è¿‡è·Ÿå–åˆ†æ")
                        return False
                except Exception as e:
                    self.logger.warning(f"ä»·æ ¼ä¼˜åŠ¿åˆ¤æ–­å¤±è´¥ï¼Œç»§ç»­è·Ÿå–åˆ†æ: {e}")
            else:
                self.logger.warning("ProfitEvaluatoræœªåˆå§‹åŒ–ï¼Œè·³è¿‡ä»·æ ¼ä¼˜åŠ¿åˆ¤æ–­")
            
            return True
            
        except Exception as e:
            self.logger.error(f"åˆ¤æ–­è·Ÿå–åˆ†æå¿…è¦æ€§å¤±è´¥: {e}")
            return False

    def _extract_basic_product_info(self, url: str, context: Optional[Dict[str, Any]] = None, skip_competitors: bool = False) -> Dict[str, Any]:
        """ç›´æ¥æå–åŸºç¡€ä»·æ ¼æ•°æ®ï¼ˆæ‰å¹³åŒ–å®ç°ï¼‰"""
        try:
            page_content = self.scraping_utils.extract_data_with_js(self.browser_service,script="() => document.documentElement.outerHTML")
            soup = BeautifulSoup(page_content, 'html.parser')
            # è·å–æ’ä»¶æ•°æ®
            erp_data = self.erp_scraper.scrape(target=url, options={'soup': soup}).data
            # å¦‚æœè·å–å¤±è´¥ï¼Œåˆ™ç›´æ¥è¿”å›
            if not erp_data:
                return {}

            # è·å–å•†å“ä»·æ ¼ã€å•†å“å›¾ç‰‡
            data = {
                    'green_price': self.scraping_utils.extract_price_from_soup(soup, "green"),
                    'black_price': self.scraping_utils.extract_price_from_soup(soup, "black"),
                    'product_image': self._extract_product_image(soup),
                    'erp_data': erp_data,
                    }
            
            # æ ¹æ®skip_competitorså‚æ•°å†³å®šæ˜¯å¦æŠ“å–è·Ÿå–ä¿¡æ¯
            if not skip_competitors:
                data['competitor_data'] = self._extract_competitor_price(soup)
            else:
                self.logger.info("è·³è¿‡è·Ÿå–ä¿¡æ¯æŠ“å–ï¼ˆskip_competitors=Trueï¼‰")

            _upd_competitor_cnt(data,context)

            # æ¸…ç†ç©ºå€¼
            return {k: v for k, v in data.items() if v is not None}
        except Exception as e:
            self.logger.error(f"æå–åŸºç¡€ä»·æ ¼æ•°æ®å¤±è´¥: {e}")
            return {}

    # æ ¹æ®dataé‡Œçš„ä¿¡æ¯è®¾ç½®  competitor_cntï¼Œ å¯ä»¥ä»erp_dataé‡Œè·å– ä¹Ÿå¯ä»¥ ä»competitor_dataè·å–ï¼Œ è°å­˜åœ¨å°±ç”¨è°

    def _extract_competitor_price(self, soup) -> Optional[Dict[str, Any]]:

        try:
            # ä½¿ç”¨é…ç½®åŒ–çš„ç«äº‰è€…å®¹å™¨é€‰æ‹©å™¨
            result = wait_for_content_smart(self.selectors_config.competitor_area_selectors, self.browser_service, soup=soup)
            competitor_container=result['content']

            if not competitor_container:
                self.logger.warning("âš ï¸ æœªæ‰¾åˆ°ç«äº‰è€…ä¿¡æ¯å®¹å™¨")
                return None

            # ä½¿ç”¨é…ç½®åŒ–é€‰æ‹©å™¨å’Œå·¥å…·å¤ç”¨æå–ä»·æ ¼
            competitor_price = None
            for selector in self.selectors_config.store_price_selectors:
                try:
                    price_element = competitor_container.select_one(selector)
                    if price_element:
                        price_text = price_element.get_text(strip=True)
                        # å¤ç”¨ç°æœ‰çš„ä»·æ ¼æå–å·¥å…·
                        price = self.scraping_utils.extract_price(price_text)
                        if price:
                            competitor_price = price_text  # ä¿ç•™åŸå§‹ä»·æ ¼æ–‡æœ¬
                            self.logger.debug(f"âœ… æå–åˆ°ç«äº‰è€…ä»·æ ¼: {competitor_price}")
                            break
                except Exception as e:
                    self.logger.debug(f"ä»·æ ¼é€‰æ‹©å™¨å¤±è´¥: {e}")
                    continue

            # ä½¿ç”¨é…ç½®åŒ–é€‰æ‹©å™¨å’Œå·¥å…·å¤ç”¨æå–æ•°é‡
            competitor_count = None
            for selector in self.selectors_config.competitor_count_selectors:
                try:
                    count_element = competitor_container.select_one(selector)
                    if count_element:
                        count_text = count_element.get_text(strip=True)
                        # å¤ç”¨ç°æœ‰çš„æ•°å­—æå–å·¥å…·
                        count = self.scraping_utils.extract_number(count_text)
                        if count is not None:
                            competitor_count = count
                            self.logger.debug(f"âœ… æå–åˆ°ç«äº‰è€…æ•°é‡: {competitor_count}")
                            break
                except Exception as e:
                    self.logger.debug(f"æ•°é‡é€‰æ‹©å™¨å¤±è´¥: {e}")
                    continue

            # æ„å»ºè¿”å›æ•°æ®
            if competitor_price or competitor_count is not None:
                result = {}
                if competitor_price:
                    result["price"] = competitor_price
                if competitor_count is not None:
                    result["count"] = competitor_count

                self.logger.info(f"âœ… æˆåŠŸæå–ç«äº‰è€…æ•°æ®: {result}")
                return result

            self.logger.warning("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•ç«äº‰è€…æ•°æ®")
            return None

        except Exception as e:
            self.logger.error(f"âŒ æå–ç«äº‰è€…æ•°æ®å¤±è´¥: {e}")
            return None





    def _extract_product_image(self, soup) -> Optional[str]:
        """
        æ ¸å¿ƒå›¾ç‰‡æå–é€»è¾‘ - ä½¿ç”¨é€šç”¨æ–¹æ³•

        Args:
            soup: BeautifulSoupå¯¹è±¡

        Returns:
            str: å•†å“å›¾ç‰‡URLï¼Œå¦‚æœæå–å¤±è´¥è¿”å›None
        """
        try:
            # æ„å»ºOZONå¹³å°ç‰¹å®šçš„å›¾ç‰‡é…ç½®
            image_config = {
                'placeholder_patterns': [
                    'doodle_ozon_rus.png',
                    'doodle_ozone_rus.png',
                    'placeholder.png',
                    'no-image.png',
                    'default.png',
                    'loading.png'
                ],
                'valid_patterns': [
                    'multimedia',        # OZONçš„å•†å“å›¾ç‰‡é€šå¸¸åŒ…å«multimedia
                    's3/multimedia',     # å®Œæ•´çš„S3è·¯å¾„
                    'wc1000',           # é«˜æ¸…å›¾ç‰‡æ ‡è¯†
                    'wc750',            # ä¸­ç­‰åˆ†è¾¨ç‡å›¾ç‰‡
                    'wc500',            # æ ‡å‡†åˆ†è¾¨ç‡å›¾ç‰‡
                ],
                'valid_extensions': ['.jpg', '.jpeg', '.png', '.webp'],
                'valid_domains': ['ozon.ru', 'ozone.ru', 'ir.ozone.ru'],
                'conversion_config': {r'/wc\d+/': '/wc1000/'}
            }

            # ä½¿ç”¨é€šç”¨æ–¹æ³•æå–å›¾ç‰‡
            return self.scraping_utils.extract_product_image(
                soup,
                self.selectors_config.image_selectors,
                image_config
            )

        except Exception as e:
            self.logger.error(f"âŒ æå–å•†å“å›¾ç‰‡å¤±è´¥: {e}")
            return None



