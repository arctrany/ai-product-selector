"""
OZONå¹³å°æŠ“å–å™¨

è´Ÿè´£ä»OZONå¹³å°æŠ“å–å•†å“ä»·æ ¼ä¿¡æ¯å’Œè·Ÿå–åº—é“ºæ•°æ®ã€‚
åŸºäºæ–°çš„browser_serviceæ¶æ„ã€‚
"""

import asyncio
import logging
import time
from typing import Dict, Any, List, Optional, Tuple

from .base_scraper import BaseScraper
from .global_browser_singleton import get_global_browser_service
from .competitor_scraper import CompetitorScraper
from ..models import ProductInfo, CompetitorStore, clean_price_string, ScrapingResult
from ..config import GoodStoreSelectorConfig
from ..config.ozon_selectors import get_ozon_selectors_config, OzonSelectorsConfig
from ..business.profit_evaluator import ProfitEvaluator
from .erp_plugin_scraper import ErpPluginScraper


class OzonScraper(BaseScraper):
    """OZONå¹³å°æŠ“å–å™¨ - åŸºäºbrowser_serviceæ¶æ„"""

    def __init__(self, config: Optional[GoodStoreSelectorConfig] = None,
                 selectors_config: Optional[OzonSelectorsConfig] = None):
        """åˆå§‹åŒ–OZONæŠ“å–å™¨"""
        super().__init__()
        self.config = config or GoodStoreSelectorConfig()
        self.selectors_config = selectors_config or get_ozon_selectors_config()
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        self.base_url = self.config.scraping.ozon_base_url

        # ğŸ”§ æ€§èƒ½ä¼˜åŒ–ï¼šä½¿ç”¨å…±äº«çš„å…¨å±€æµè§ˆå™¨æœåŠ¡ï¼Œé¿å…é‡å¤åˆ›å»º
        self.browser_service = get_global_browser_service()

        # åˆ›å»ºè·Ÿå–æŠ“å–å™¨
        self.competitor_scraper = CompetitorScraper(selectors_config=self.selectors_config)

        # åˆ›å»ºåˆ©æ¶¦è¯„ä¼°å™¨
        self.profit_evaluator = ProfitEvaluator(
            profit_calculator_path=self.config.excel.profit_calculator_path,
            config=self.config
        )

        # åˆå§‹åŒ–ERPæ’ä»¶æŠ“å–å™¨ï¼ˆå…±äº«browser_serviceå®ä¾‹ï¼‰
        self.erp_scraper = ErpPluginScraper(self.config, self.browser_service)

    def scrape_product_prices(self, product_url: str) -> ScrapingResult:
        """
        æŠ“å–å•†å“ä»·æ ¼ä¿¡æ¯
        
        Args:
            product_url: å•†å“URL
            
        Returns:
            ScrapingResult: æŠ“å–ç»“æœï¼ŒåŒ…å«ä»·æ ¼ä¿¡æ¯
        """
        start_time = time.time()

        try:
            # ä½¿ç”¨æµè§ˆå™¨æœåŠ¡æŠ“å–æ•°æ®
            def extract_price_data(browser_service):
                """åŒæ­¥æå–ä»·æ ¼æ•°æ®"""
                try:
                    # ğŸ”§ æ€§èƒ½ä¼˜åŒ–ï¼šå‡å°‘ä¸å¿…è¦çš„ç­‰å¾…æ—¶é—´
                    time.sleep(0.5)

                    # è·å–é¡µé¢å†…å®¹ - ä½¿ç”¨åŒæ­¥æ–¹æ³•
                    page_content = browser_service.evaluate_sync("() => document.documentElement.outerHTML")
                    if not page_content:
                        self.logger.error("æœªèƒ½è·å–é¡µé¢å†…å®¹")
                        return {}

                    # è§£æä»·æ ¼ä¿¡æ¯ - ç›´æ¥è°ƒç”¨æ ¸å¿ƒæ–¹æ³•
                    from bs4 import BeautifulSoup
                    soup = BeautifulSoup(page_content, 'html.parser')
                    price_data = self._extract_price_data_core(soup)

                    # ä¿å­˜ä»·æ ¼æ•°æ®ä¾›ERPæŠ“å–ä½¿ç”¨
                    self._last_price_data = price_data

                    return price_data

                except Exception as e:
                    self.logger.error(f"æå–ä»·æ ¼æ•°æ®å¤±è´¥: {e}")
                    return {}

            # ä½¿ç”¨ç»§æ‰¿çš„æŠ“å–æ–¹æ³•
            result = self.scrape_page_data(product_url, extract_price_data)

            if result.success and result.data:
                return ScrapingResult(
                    success=True,
                    data=result.data,
                    execution_time=time.time() - start_time
                )
            else:
                return ScrapingResult(
                    success=False,
                    data={},
                    error_message=result.error_message or "æœªèƒ½æå–åˆ°ä»·æ ¼ä¿¡æ¯",
                    execution_time=time.time() - start_time
                )

        except Exception as e:
            self.logger.error(f"æŠ“å–å•†å“ä»·æ ¼å¤±è´¥: {e}")
            return ScrapingResult(
                success=False,
                data={},
                error_message=str(e),
                execution_time=time.time() - start_time
            )

    def scrape_competitor_stores(self, product_url: str, max_competitors: int = 10) -> ScrapingResult:
        """
        æŠ“å–è·Ÿå–åº—é“ºä¿¡æ¯

        Args:
            product_url: å•†å“URL
            max_competitors: æœ€å¤§è·Ÿå–åº—é“ºæ•°é‡ï¼Œé»˜è®¤10ä¸ª

        Returns:
            ScrapingResult: æŠ“å–ç»“æœï¼ŒåŒ…å«è·Ÿå–åº—é“ºä¿¡æ¯
        """
        start_time = time.time()

        try:
            def extract_competitor_data(browser_service):
                """åŒæ­¥æå–è·Ÿå–åº—é“ºæ•°æ®"""
                try:
                    # ğŸ”§ æ€§èƒ½ä¼˜åŒ–ï¼šå‡å°‘ä¸å¿…è¦çš„ç­‰å¾…æ—¶é—´
                    time.sleep(0.5)

                    # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨CompetitorScraperçš„ä¸¥æ ¼è·Ÿå–æ£€æµ‹æ–¹æ³•
                    page = browser_service.browser_driver.page
                    popup_result = self.competitor_scraper.open_competitor_popup(page)

                    # ğŸ¯ æ ¹æ®ä¸¥æ ¼æ£€æµ‹ç»“æœå†³å®šåç»­å¤„ç†
                    if not popup_result['success']:
                        self.logger.error(f"è·Ÿå–æ£€æµ‹å¤±è´¥: {popup_result['error_message']}")
                        return {'competitors': [], 'total_count': 0}

                    if not popup_result['has_competitors']:
                        self.logger.info("âœ… ç¡®è®¤æ— è·Ÿå–ï¼Œè·³è¿‡è·Ÿå–ä¿¡æ¯æå–")
                        return {'competitors': [], 'total_count': 0}

                    if not popup_result['popup_opened']:
                        self.logger.warning("âš ï¸ æœ‰è·Ÿå–ä½†æµ®å±‚æœªæ‰“å¼€ï¼Œè·³è¿‡è·Ÿå–ä¿¡æ¯æå–")
                        return {'competitors': [], 'total_count': 0}

                    # ğŸ”§ ä¿®å¤ï¼šè·å–æ£€æµ‹åˆ°çš„æ€»è·Ÿå–æ•°é‡ï¼ˆè€Œä¸æ˜¯å®é™…æå–çš„æ•°é‡ï¼‰
                    page = browser_service.get_page()
                    detected_total_count = self.competitor_scraper._get_competitor_count(page)

                    # è·å–é¡µé¢å†…å®¹ - ä½¿ç”¨åŒæ­¥æ–¹æ³•
                    page_content = browser_service.evaluate_sync("() => document.documentElement.outerHTML")

                    # è§£æè·Ÿå–åº—é“ºä¿¡æ¯ - ä¿®å¤ï¼šä½¿ç”¨CompetitorScraper
                    competitors = self.competitor_scraper.extract_competitors_from_content(page_content,
                                                                                                 max_competitors)

                    # ğŸ”§ ä¿®å¤ï¼šè¿”å›æ£€æµ‹åˆ°çš„æ€»æ•°é‡ï¼Œè€Œä¸æ˜¯å®é™…æå–çš„æ•°é‡
                    return {
                        'competitors': competitors,
                        'total_count': detected_total_count if detected_total_count is not None else len(competitors)
                    }

                except Exception as e:
                    self.logger.error(f"æå–è·Ÿå–åº—é“ºæ•°æ®å¤±è´¥: {e}")
                    return {'competitors': [], 'total_count': 0}

            # ä½¿ç”¨ç»§æ‰¿çš„æŠ“å–æ–¹æ³•
            result = self.scrape_page_data(product_url, extract_competitor_data)

            if result.success:
                return ScrapingResult(
                    success=True,
                    data=result.data,
                    execution_time=time.time() - start_time
                )
            else:
                return ScrapingResult(
                    success=False,
                    data={'competitors': [], 'total_count': 0},
                    error_message=result.error_message or "æ— æ³•æŠ“å–è·Ÿå–åº—é“ºä¿¡æ¯",
                    execution_time=time.time() - start_time
                )

        except Exception as e:
            self.logger.error(f"æŠ“å–è·Ÿå–åº—é“ºä¿¡æ¯å¤±è´¥: {e}")
            return ScrapingResult(
                success=False,
                data={'competitors': [], 'total_count': 0},
                error_message=str(e),
                execution_time=time.time() - start_time
            )

    # æŠ“å–å•†å“ä¿¡æ¯çš„ä¸»å…¥å£
    def scrape(self,
               product_url: str,
               include_competitors: bool = False,
               _recursion_depth: int = 0,
               _fetch_competitor_details: bool = True,
               **kwargs) -> ScrapingResult:
        """
        ç»¼åˆæŠ“å–å•†å“ä¿¡æ¯

        Args:
            product_url: å•†å“URL
            include_competitors: æ˜¯å¦åŒ…å«è·Ÿå–åº—é“ºä¿¡æ¯
            _recursion_depth: é€’å½’æ·±åº¦ï¼ˆå†…éƒ¨å‚æ•°ï¼Œé˜²æ­¢æ— é™é€’å½’ï¼‰
            _fetch_competitor_details: æ˜¯å¦æŠ“å–è·Ÿå–å•†å“è¯¦æƒ…ï¼ˆå†…éƒ¨å‚æ•°ï¼‰
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            ScrapingResult: æŠ“å–ç»“æœï¼ŒåŒ…å« product_id å’Œå¯é€‰çš„ first_competitor_details å­—æ®µ
        """
        start_time = time.time()

        try:
            # ğŸ”’ é€’å½’æ·±åº¦é™åˆ¶ï¼ˆç¡¬æ€§ä¿æŠ¤ï¼‰
            if _recursion_depth > 1:
                self.logger.error("é€’å½’æ·±åº¦è¶…é™ï¼Œç»ˆæ­¢æŠ“å–")
                return ScrapingResult(
                    success=False,
                    data={},
                    error_message="é€’å½’æ·±åº¦è¶…é™",
                    execution_time=time.time() - start_time
                )
            # æŠ“å–ä»·æ ¼ä¿¡æ¯
            price_result = self.scrape_product_prices(product_url)
            if not price_result.success:
                return price_result

            # ğŸ†” æå–å•†å“ ID
            product_id = self._extract_product_id(product_url)
            if product_id:
                self.logger.info(f"âœ… æå–åˆ°å•†å“ID: {product_id}")
            else:
                self.logger.warning(f"âš ï¸ æ— æ³•ä»URLæå–å•†å“ID: {product_url}")

            result_data = {
                'product_url': product_url,
                'product_id': product_id,
                'price_data': price_result.data,
                'include_competitors': include_competitors
            }


            # åˆ¤æ–­è·Ÿå–ä»·æ ¼æ¯”é»‘æ ‡ä»·æ ¼ã€ç»¿æ ‡ä»·æ ¼æ˜¯å¦æ›´ä½,ç»¿æ ‡ä»·æ ¼å¦‚æœä¸å­˜åœ¨åˆ™æ¯”ä»·é»‘æ ‡ä»·æ ¼å³å¯ï¼›
            has_better_price = self.profit_evaluator.has_better_competitor_price(result_data)

            # æŠ“å–ERPåŒºåŸŸä¿¡æ¯
            erp_result = self.scrape_erp_info()
            if erp_result.success:
                result_data['erp_data'] = erp_result.data
            else:
                self.logger.warning(f"æŠ“å–ERPä¿¡æ¯å¤±è´¥: {erp_result.error_message}")
                result_data['erp_data'] = {}


            # å¦‚æœéœ€è¦ï¼ŒæŠ“å–è·Ÿå–åº—é“ºä¿¡æ¯
            if include_competitors and has_better_price:
                competitors_result = self.scrape_competitor_stores(product_url)
                if competitors_result.success:
                    result_data['competitors'] = competitors_result.data['competitors']
                    # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨æ£€æµ‹åˆ°çš„æ€»è·Ÿå–æ•°é‡ï¼Œè€Œä¸æ˜¯å®é™…æå–çš„åº—é“ºæ•°é‡
                    result_data['competitor_count'] = competitors_result.data.get('total_count', len(
                        competitors_result.data['competitors']))


                else:
                    self.logger.warning(f"æŠ“å–è·Ÿå–åº—é“ºä¿¡æ¯å¤±è´¥: {competitors_result.error_message}")
                    result_data['competitors'] = []
                    result_data['competitor_count'] = 0

                # ğŸ”„ é€’å½’æŠ“å–ç¬¬ä¸€ä¸ªè·Ÿå–å•†å“è¯¦æƒ…
                if _fetch_competitor_details and _recursion_depth == 0:
                    try:
                        self.logger.info("ğŸ¯ å¼€å§‹æŠ“å–ç¬¬ä¸€ä¸ªè·Ÿå–å•†å“è¯¦æƒ…")

                        # ç‚¹å‡»ç¬¬ä¸€ä¸ªè·Ÿå–åº—é“ºå¹¶è·å–æ–°URLå’Œå•†å“ID
                        first_url, first_product_id = self._click_first_competitor()
                        self.logger.info(f"âœ… è·³è½¬åˆ°è·Ÿå–å•†å“: {first_url} (ID: {first_product_id})")

                        # é€’å½’è°ƒç”¨ scrape()ï¼Œç¦æ­¢å†æ¬¡é€’å½’
                        competitor_result = self.scrape(
                            first_url,
                            include_competitors=False,
                            _fetch_competitor_details=False,
                            _recursion_depth=1
                        )

                        if competitor_result.success:
                            result_data['first_competitor_details'] = competitor_result.data
                            self.logger.info("âœ… æˆåŠŸæŠ“å–è·Ÿå–å•†å“è¯¦æƒ…")
                        else:
                            self.logger.warning(f"âš ï¸ é€’å½’æŠ“å–å¤±è´¥ï¼Œä½†ä¸å½±å“ä¸»æµç¨‹: {competitor_result.error_message}")

                    except Exception as e:
                        self.logger.warning(f"âš ï¸ æŠ“å–è·Ÿå–å•†å“è¯¦æƒ…å¤±è´¥: {e}")
                        # ä¸å½±å“ä¸»æµç¨‹ï¼Œç»§ç»­è¿”å›åŸå•†å“æ•°æ®

            else:
                # å³ä½¿ä¸æŠ“å–è·Ÿå–åº—é“ºï¼Œä¹Ÿè¦è®¾ç½® competitor_count
                result_data['competitors'] = []
                result_data['competitor_count'] = 0

            # å¦‚æœinclude_competitors = False, å¹¶ä¸”include_competitors = Trueï¼Œå¹¶ä¸”result_dataé‡Œå­˜åœ¨itemUrlï¼Œåˆ™æŠ“å–scrapeå½“å‰å•†å“çš„ä¿¡æ¯
            # competitor_product_url = result_data.get('competitor_product_url')
            # competitor_item_result = None
            # if not include_competitors and competitor_product_url:
            #     competitor_item_result = self.scrape(competitor_product_url, include_competitors=False)
            #
            # # ç¼–å†™ä¸€ä¸ªå‡½æ•°chooseGoodItemæ ¹æ®competitor item resultå’ŒåŸå§‹çš„result_data
            # # è¿›è¡ŒåŠ å·¥å’ŒéªŒè¯ï¼Œè¿”å›ä¸€ä¸ªæ–°çš„result_dataï¼ŒåŒ…å«ä¸€ä¸ªå½“å‰å•†å“ä»¥åŠcompetitorå•†å“ï¼Œå…ˆä¸å®ç°é€»è¾‘æ‰“å°å³å¯ã€‚
            # if competitor_item_result and competitor_item_result.success:
            #     self.combine_item(competitor_item_result.data, result_data)

            return ScrapingResult(
                success=True,
                data=result_data,
                execution_time=time.time() - start_time
            )

        except Exception as e:
            self.logger.error(f"ç»¼åˆæŠ“å–å•†å“ä¿¡æ¯å¤±è´¥: {e}")
            return ScrapingResult(
                success=False,
                data={},
                error_message=str(e),
                execution_time=time.time() - start_time
            )

    def _extract_price_data_core(self, soup) -> Dict[str, Any]:
        """
        æ ¸å¿ƒä»·æ ¼æå–é€»è¾‘ - ç®€åŒ–ç‰ˆæœ¬

        Args:
            soup: BeautifulSoupå¯¹è±¡
            is_async: æ˜¯å¦å¼‚æ­¥è°ƒç”¨

        Returns:
            Dict[str, Any]: ä»·æ ¼æ•°æ®
        """
        try:
            price_data = {}

            # æå–å•†å“å›¾ç‰‡
            image_url = self._extract_product_image_core(soup)
            if image_url:
                price_data['image_url'] = image_url

            # æå–åŸºç¡€ä»·æ ¼ï¼ˆç»¿æ ‡ã€é»‘æ ‡ï¼‰
            basic_prices = self._extract_basic_prices(soup)
            price_data.update(basic_prices)

            # ğŸ”§ ä¿®å¤ï¼šç›´æ¥åœ¨ä¸»æµç¨‹ä¸­æ£€æµ‹è·Ÿå–å…³é”®è¯å¹¶æå–ä»·æ ¼
            page_text = soup.get_text()

            # æ£€æµ‹è·Ÿå–å…³é”®è¯
            for keyword in self.selectors_config.competitor_keywords:
                if keyword.lower() in page_text.lower():
                    self.logger.info(f"ğŸ” æ£€æµ‹åˆ°è·Ÿå–å…³é”®è¯: {keyword}")
                    price_data.update({
                        'has_competitors': True,
                        'competitor_keyword': keyword
                    })

                    # æå–è·Ÿå–ä»·æ ¼
                    competitor_price = self._extract_competitor_price_value(soup)
                    if competitor_price:
                        price_data['competitor_price'] = competitor_price
                        currency_symbol = self.currency_config.get_default_symbol()
                        self.logger.info(f"ğŸ’° è·Ÿå–ä»·æ ¼: {competitor_price}{currency_symbol}")
                    break

            return price_data

        except Exception as e:
            self._handle_extraction_error(e, "æå–ä»·æ ¼æ•°æ®")
            return {}

    def _extract_basic_prices(self, soup) -> Dict[str, Any]:
        """æå–åŸºç¡€ä»·æ ¼ï¼ˆç»¿æ ‡ã€é»‘æ ‡ï¼‰"""
        prices = {}
        green_price = None
        black_price = None

        # ğŸ”§ ä¿®å¤ï¼šä¸¥æ ¼æŒ‰ç…§é€‰æ‹©å™¨ç±»å‹æå–ä»·æ ¼ï¼Œé¿å…æ··æ·†
        for selector, price_type, priority in self.selectors_config.price_selectors:
            try:
                elements = soup.select(selector)
                self.logger.debug(f"ğŸ” ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' (ç±»å‹: {price_type}) æ‰¾åˆ° {len(elements)} ä¸ªå…ƒç´ ")

                for element in elements:
                    price = self._extract_price_from_element(element)

                    # ä½¿ç”¨ _validate_price éªŒè¯ä»·æ ¼
                    if not self._validate_price(price, price_type):
                        continue

                    # ğŸ”§ ä¿®å¤ï¼šä¸¥æ ¼æŒ‰ç…§ä»·æ ¼ç±»å‹åˆ†é…ï¼Œé¿å…é‡å¤èµ‹å€¼
                    if price_type == "green" and green_price is None:
                        green_price = price
                        currency_symbol = self.currency_config.get_default_symbol()
                        self.logger.info(f"âœ… ç»¿æ ‡ä»·æ ¼: {green_price}{currency_symbol}")
                        break  # æ‰¾åˆ°ç»¿æ ‡ä»·æ ¼åç«‹å³è·³å‡ºå†…å±‚å¾ªç¯
                    elif price_type == "black" and black_price is None:
                        black_price = price
                        currency_symbol = self.currency_config.get_default_symbol()
                        self.logger.info(f"âœ… é»‘æ ‡ä»·æ ¼: {black_price}{currency_symbol}")
                        break  # æ‰¾åˆ°é»‘æ ‡ä»·æ ¼åç«‹å³è·³å‡ºå†…å±‚å¾ªç¯

            except Exception as e:
                self.logger.debug(f"é€‰æ‹©å™¨ '{selector}' å¤„ç†å¤±è´¥: {e}")
                continue

        # ğŸ”§ ä¿®å¤ï¼šæ˜ç¡®è®°å½•ä»·æ ¼æå–ç»“æœ
        if green_price is None:
            self.logger.info("â„¹ï¸ æœªæ‰¾åˆ°ç»¿æ ‡ä»·æ ¼")
        if black_price is None:
            self.logger.warning("âš ï¸ æœªæ‰¾åˆ°é»‘æ ‡ä»·æ ¼")

        # ğŸ”§ ä¿®å¤ï¼šåªæœ‰å½“ä»·æ ¼ç¡®å®å­˜åœ¨æ—¶æ‰æ·»åŠ åˆ°è¿”å›æ•°æ®ä¸­
        if green_price is not None:
            prices['green_price'] = green_price
        if black_price is not None:
            prices['black_price'] = black_price

        self.logger.debug(f"ğŸ¯ æœ€ç»ˆæå–çš„ä»·æ ¼æ•°æ®: {prices}")
        return prices

    def _validate_price(self, price: Optional[float], price_type: str) -> bool:
        """
        éªŒè¯ä»·æ ¼æ˜¯å¦æœ‰æ•ˆ

        Args:
            price: ä»·æ ¼å€¼
            price_type: ä»·æ ¼ç±»å‹åç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰

        Returns:
            bool: ä»·æ ¼æ˜¯å¦æœ‰æ•ˆ
        """
        if price is None or price <= 0:
            self.logger.debug(f"âš ï¸ {price_type}ä»·æ ¼æ— æ•ˆ: {price}")
            return False
        return True

    def _handle_extraction_error(self, error: Exception, context: str) -> None:
        """
        ç»Ÿä¸€å¤„ç†æå–é”™è¯¯

        Args:
            error: å¼‚å¸¸å¯¹è±¡
            context: ä¸Šä¸‹æ–‡æè¿°
        """
        self.logger.error(f"âŒ {context}å¤±è´¥: {error}")

    def _extract_price_from_element(self, element) -> Optional[float]:
        """
        ä»å…ƒç´ ä¸­æå–ä»·æ ¼æ•°å€¼

        Args:
            element: BeautifulSoupå…ƒç´ 

        Returns:
            float: ä»·æ ¼æ•°å€¼ï¼Œå¦‚æœæå–å¤±è´¥è¿”å›None
        """
        try:
            if not element:
                return None

            # è·å–å…ƒç´ æ–‡æœ¬
            text = element.get_text(strip=True)
            if not text:
                return None

            # ä½¿ç”¨clean_price_stringå‡½æ•°æå–ä»·æ ¼
            price = clean_price_string(text, self.selectors_config)
            return price

        except Exception as e:
            self._handle_extraction_error(e, "ä»å…ƒç´ æå–ä»·æ ¼")
            return None

    def _extract_competitor_price_value(self, soup) -> Optional[float]:
        """æå–å…·ä½“çš„è·Ÿå–ä»·æ ¼æ•°å€¼ - ä½¿ç”¨é…ç½®çš„ç²¾ç¡®é€‰æ‹©å™¨"""
        try:
            # ğŸ¯ ä½¿ç”¨é…ç½®çš„ç²¾ç¡®è·Ÿå–ä»·æ ¼é€‰æ‹©å™¨
            competitor_price_selector = self.selectors_config.competitor_price_selector

            self.logger.debug(f"ğŸ” ä½¿ç”¨ç²¾ç¡®è·Ÿå–ä»·æ ¼é€‰æ‹©å™¨: {competitor_price_selector}")

            # æŸ¥æ‰¾è·Ÿå–ä»·æ ¼å…ƒç´ 
            competitor_elements = soup.select(competitor_price_selector)

            for element in competitor_elements:
                text = element.get_text(strip=True)
                self.logger.debug(f"ğŸ” æ‰¾åˆ°è·Ÿå–ä»·æ ¼å…ƒç´ æ–‡æœ¬: '{text}'")

                # ğŸ”§ ä¿®å¤ï¼šåªå¤„ç†åŒ…å«ä»·æ ¼ç¬¦å·çš„å…ƒç´ ï¼Œè¿‡æ»¤æ‰é…é€æ—¶é—´ç­‰éä»·æ ¼ä¿¡æ¯
                # ä½¿ç”¨é…ç½®åŒ–çš„è´§å¸ç¬¦å·æ£€æŸ¥
                has_currency = self.currency_config.is_currency_symbol(text)
                if not has_currency:
                    self.logger.debug(f"âš ï¸ è·³è¿‡éä»·æ ¼å…ƒç´ : '{text}'")
                    continue

                # æå–ä»·æ ¼æ•°å€¼ - å¤„ç† "From 3 800 â‚½" æ ¼å¼
                price = self._extract_price_from_element(element)
                if self._validate_price(price, "è·Ÿå–"):
                    self.logger.debug(f"ğŸ¯ æˆåŠŸæå–è·Ÿå–ä»·æ ¼: {price}â‚½")
                    return price

            self.logger.debug("âš ï¸ æœªæ‰¾åˆ°åŒ…å«ä»·æ ¼ç¬¦å·çš„è·Ÿå–ä»·æ ¼å…ƒç´ ")
            return None

        except Exception as e:
            self._handle_extraction_error(e, "æå–è·Ÿå–ä»·æ ¼")
            return None

    # ğŸ”§ ä¿®å¤ï¼šåˆ é™¤é‡å¤çš„è·Ÿå–åº—é“ºæå–é€»è¾‘ï¼Œè¿™äº›åŠŸèƒ½åº”è¯¥ç”± CompetitorScraper è´Ÿè´£
    # åˆ é™¤äº†å¤§é‡é‡å¤çš„è·Ÿå–åº—é“ºç›¸å…³ä»£ç ï¼ŒèŒè´£åˆ†ç¦»ï¼š
    # - OzonScraper: è´Ÿè´£ä»·æ ¼æå–
    # - CompetitorScraper: è´Ÿè´£è·Ÿå–åº—é“ºäº¤äº’å’Œæå–

    def _extract_product_image_core(self, soup) -> Optional[str]:
        """
        æ ¸å¿ƒå›¾ç‰‡æå–é€»è¾‘ - ç»Ÿä¸€å®ç°é¿å…é‡å¤ï¼ŒåŒ…å«å ä½ç¬¦è¿‡æ»¤

        Args:
            soup: BeautifulSoupå¯¹è±¡

        Returns:
            str: å•†å“å›¾ç‰‡URLï¼Œå¦‚æœæå–å¤±è´¥è¿”å›None
        """
        try:
            # å·²çŸ¥çš„å ä½ç¬¦å›¾ç‰‡æ¨¡å¼
            placeholder_patterns = [
                'doodle_ozon_rus.png',
                'doodle_ozone_rus.png',
                'placeholder.png',
                'no-image.png',
                'default.png',
                'loading.png'
            ]

            for selector in self.selectors_config.image_selectors:
                img_elements = soup.select(selector)
                self.logger.debug(f"ğŸ” é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(img_elements)} ä¸ªå›¾ç‰‡å…ƒç´ ")

                for img_element in img_elements:
                    src = img_element.get('src')
                    if not src:
                        continue

                    # è½¬æ¢ä¸ºé«˜æ¸…ç‰ˆæœ¬
                    high_res_url = self._convert_to_high_res_image(src)

                    # éªŒè¯å›¾ç‰‡URLæ˜¯å¦ä¸ºå ä½ç¬¦
                    if self._is_placeholder_image(high_res_url, placeholder_patterns):
                        self.logger.warning(f"âš ï¸ è·³è¿‡å ä½ç¬¦å›¾ç‰‡: {high_res_url}")
                        continue

                    # éªŒè¯å›¾ç‰‡URLæ˜¯å¦ä¸ºæœ‰æ•ˆçš„å•†å“å›¾ç‰‡
                    if self._is_valid_product_image(high_res_url):
                        self.logger.info(f"âœ… æˆåŠŸæå–å•†å“å›¾ç‰‡: {high_res_url}")
                        return high_res_url
                    else:
                        self.logger.debug(f"ğŸ” è·³è¿‡æ— æ•ˆå›¾ç‰‡: {high_res_url}")

            self.logger.warning("âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆçš„å•†å“å›¾ç‰‡")
            return None

        except Exception as e:
            self._handle_extraction_error(e, "æå–å•†å“å›¾ç‰‡")
            return None

    def _is_placeholder_image(self, image_url: str, placeholder_patterns: list) -> bool:
        """
        æ£€æŸ¥å›¾ç‰‡URLæ˜¯å¦ä¸ºå ä½ç¬¦å›¾ç‰‡

        Args:
            image_url: å›¾ç‰‡URL
            placeholder_patterns: å ä½ç¬¦å›¾ç‰‡æ¨¡å¼åˆ—è¡¨

        Returns:
            bool: Trueè¡¨ç¤ºæ˜¯å ä½ç¬¦å›¾ç‰‡ï¼ŒFalseè¡¨ç¤ºä¸æ˜¯
        """
        if not image_url:
            return True

        # æ£€æŸ¥URLä¸­æ˜¯å¦åŒ…å«å ä½ç¬¦æ¨¡å¼
        for pattern in placeholder_patterns:
            if pattern in image_url:
                return True

        # æ£€æŸ¥æ˜¯å¦åŒ…å«å…¶ä»–å·²çŸ¥çš„å ä½ç¬¦ç‰¹å¾
        placeholder_keywords = ['doodle', 'placeholder', 'default', 'no-image', 'loading']
        url_lower = image_url.lower()

        for keyword in placeholder_keywords:
            if keyword in url_lower:
                return True

        return False

    def _is_valid_product_image(self, image_url: str) -> bool:
        """
        éªŒè¯å›¾ç‰‡URLæ˜¯å¦ä¸ºæœ‰æ•ˆçš„å•†å“å›¾ç‰‡

        Args:
            image_url: å›¾ç‰‡URL

        Returns:
            bool: Trueè¡¨ç¤ºæ˜¯æœ‰æ•ˆå•†å“å›¾ç‰‡ï¼ŒFalseè¡¨ç¤ºæ— æ•ˆ
        """
        if not image_url:
            return False

        # æ£€æŸ¥æ˜¯å¦åŒ…å«æœ‰æ•ˆçš„å•†å“å›¾ç‰‡ç‰¹å¾
        valid_patterns = [
            'multimedia',        # OZONçš„å•†å“å›¾ç‰‡é€šå¸¸åŒ…å«multimedia
            's3/multimedia',     # å®Œæ•´çš„S3è·¯å¾„
            'wc1000',           # é«˜æ¸…å›¾ç‰‡æ ‡è¯†
            'wc750',            # ä¸­ç­‰åˆ†è¾¨ç‡å›¾ç‰‡
            'wc500',            # æ ‡å‡†åˆ†è¾¨ç‡å›¾ç‰‡
        ]

        url_lower = image_url.lower()

        # å¿…é¡»åŒ…å«è‡³å°‘ä¸€ä¸ªæœ‰æ•ˆæ¨¡å¼
        has_valid_pattern = any(pattern in url_lower for pattern in valid_patterns)

        # å¿…é¡»æ˜¯å›¾ç‰‡æ–‡ä»¶
        is_image_file = any(ext in url_lower for ext in ['.jpg', '.jpeg', '.png', '.webp'])

        # å¿…é¡»æ¥è‡ªOZON/OZONEåŸŸå
        is_ozon_domain = any(domain in url_lower for domain in ['ozon.ru', 'ozone.ru', 'ir.ozone.ru'])

        # ä¸èƒ½åŒ…å«æ˜æ˜¾çš„å ä½ç¬¦ç‰¹å¾
        has_placeholder_features = any(keyword in url_lower for keyword in ['doodle', 'placeholder', 'default', 'error'])

        return has_valid_pattern and is_image_file and is_ozon_domain and not has_placeholder_features

    def _convert_to_high_res_image(self, image_url: str) -> str:
        """
        å°†å›¾ç‰‡URLè½¬æ¢ä¸ºé«˜æ¸…ç‰ˆæœ¬

        Args:
            image_url: åŸå§‹å›¾ç‰‡URL

        Returns:
            str: é«˜æ¸…å›¾ç‰‡URL
        """
        try:
            import re
            # å°†wc50æˆ–wc100æ›¿æ¢ä¸ºwc1000
            high_res_url = re.sub(r'/wc\d+/', '/wc1000/', image_url)
            return high_res_url
        except Exception as e:
            self.logger.warning(f"è½¬æ¢é«˜æ¸…å›¾ç‰‡URLå¤±è´¥: {e}")
            return image_url

    def scrape_erp_info(self) -> ScrapingResult:
        """
        æŠ“å–ERPæ’ä»¶ä¿¡æ¯

        Returns:
            ScrapingResult: ERPæŠ“å–ç»“æœ
        """
        try:
            # ä½¿ç”¨å…±äº«çš„browser_serviceå®ä¾‹æŠ“å–ERPä¿¡æ¯
            return self.erp_scraper.scrape()

        except Exception as e:
            self.logger.error(f"æŠ“å–ERPä¿¡æ¯å¤±è´¥: {e}")
            return ScrapingResult(
                success=False,
                data={},
                error_message=str(e)
            )



    # def combine_item(self, data, result_data):

    def _extract_product_id(self, url: str) -> Optional[str]:
        """
        ä»URLä¸­æå–å•†å“ID
        
        æ”¯æŒçš„URLæ ¼å¼:
        - https://www.ozon.ru/product/xxx-1234567/
        - https://www.ozon.ru/seller/xxx/product/1234567/
        
        Args:
            url: å•†å“URL
            
        Returns:
            Optional[str]: å•†å“IDï¼Œæå–å¤±è´¥è¿”å›None

        Raises:
            Exception: å½“URLä¸ºNoneæ—¶æŠ›å‡ºå¼‚å¸¸
        """
        # ç‰¹æ®Šå¤„ç†Noneè¾“å…¥
        if url is None:
            raise Exception("URLä¸èƒ½ä¸ºNone")

        try:
            import re
            
            # é¦–å…ˆéªŒè¯æ˜¯å¦ä¸ºOZONåŸŸå
            if not url or not re.search(r'https?://[^/]*ozon\.ru/', url):
                self.logger.debug(f"URLä¸æ˜¯OZONåŸŸå: {url}")
                return None

            # åŒ¹é… /product/xxx-æ•°å­—/ æˆ– /product/æ•°å­—/ æ ¼å¼ (å…¼å®¹æœ‰æ— æœ«å°¾æ–œæ )
            patterns = [
                r'/product/[^/]+-(\d+)',     # xxx-1234567 (å…¼å®¹æœ‰æ— æ–œæ )
                r'/product/(\d+)',            # 1234567 (å…¼å®¹æœ‰æ— æ–œæ )
            ]
            
            for pattern in patterns:
                match = re.search(pattern, url)
                if match:
                    product_id = match.group(1)
                    return product_id
            
            self.logger.debug(f"æ— æ³•ä»URLæå–å•†å“ID: {url}")
            return None
            
        except Exception as e:
            self.logger.error(f"æå–å•†å“IDå¤±è´¥: {e}")
            return None

    def _click_first_competitor(self) -> Tuple[str, Optional[str]]:
        """
        ç‚¹å‡»ç¬¬ä¸€ä¸ªè·Ÿå–åº—é“ºå¡ç‰‡çš„å®‰å…¨åŒºåŸŸï¼ˆé¿å…åº—é“ºåç§°/Logoï¼‰
        è¿”å›è·³è½¬åçš„å•†å“è¯¦æƒ…é¡µURLå’Œå•†å“ID
        
        Returns:
            Tuple[str, Optional[str]]: (æ–°URL, å•†å“ID)
                - æ–°URL: è·³è½¬åçš„å•†å“è¯¦æƒ…é¡µURL
                - å•†å“ID: ä»URLä¸­æå–çš„product_idï¼Œæå–å¤±è´¥æ—¶ä¸ºNone
                
        Raises:
            Exception: ç‚¹å‡»å¤±è´¥ã€è·³è½¬å¤±è´¥ç­‰
        """
        try:
            page = self.browser_service.get_page()
            
            # 1. å®šä½ç¬¬ä¸€ä¸ªè·Ÿå–å¡ç‰‡ï¼ˆæ”¯æŒæ–°æ—§é€‰æ‹©å™¨ï¼‰
            card_selectors = ["div.pdp_bk3", "div.pdp_kb2"]
            first_card = None
            
            for selector in card_selectors:
                # ä½¿ç”¨åŒæ­¥æ–¹æ³•æ›¿ä»£å¼‚æ­¥çš„ Locator.count()
                elements = self.browser_service.query_selector_all_sync(selector)
                count = len(elements) if elements else 0
                if count > 0:
                    first_card = page.locator(selector).first
                    self.logger.info(f"âœ… æ‰¾åˆ°ç¬¬ä¸€ä¸ªè·Ÿå–å¡ç‰‡: {selector}")
                    break
            
            if not first_card:
                raise Exception("æœªæ‰¾åˆ°è·Ÿå–åº—é“ºå¡ç‰‡")
            
            # 2. ä¼˜å…ˆç‚¹å‡»çš„å®‰å…¨åŒºåŸŸé€‰æ‹©å™¨ï¼ˆé¿å¼€åº—é“ºåç§°/Logoï¼‰
            # æ³¨æ„ï¼šæ•´ä¸ªå¡ç‰‡æœ‰JSäº‹ä»¶ç›‘å¬ï¼Œç‚¹å‡»éåº—é“ºé“¾æ¥åŒºåŸŸä¼šè·³è½¬åˆ°å•†å“é¡µ
            safe_click_selectors = [
                # ğŸ¥‡ æœ€é«˜ä¼˜å…ˆçº§ï¼šä»·æ ¼åŒºåŸŸï¼ˆç”¨æˆ·å·²éªŒè¯ï¼‰
                "div.pdp_bk0",              # ä»·æ ¼å®¹å™¨
                "div.pdp_b1k",              # ä»·æ ¼æ–‡æœ¬
                
                # ğŸ¥ˆ é«˜ä¼˜å…ˆçº§ï¼šå…¶ä»–ä¿¡æ¯åŒºåŸŸ
                "div.pdp_kb1",              # Ozonå¡ç‰‡ä»·æ ¼
                "div.pdp_b3j",              # é…é€ä¿¡æ¯åŒºåŸŸ
                "div.pdp_jb3",              # é…é€æ–‡æœ¬åŒºåŸŸ
                
                # ğŸ¥‰ ä¸­ä¼˜å…ˆçº§ï¼šæŒ‰é’®åŒºåŸŸ
                "div.pdp_j6b",              # æŒ‰é’®å®¹å™¨
            ]
            
            # 3. æŸ¥æ‰¾å¯ç‚¹å‡»çš„å®‰å…¨åŒºåŸŸ
            clickable_element = None
            used_selector = None
            
            for selector in safe_click_selectors:
                # ä½¿ç”¨åŒæ­¥æ–¹æ³•æ›¿ä»£å¼‚æ­¥çš„ Locator.count()
                elements = self.browser_service.query_selector_all_sync(selector)
                count = len(elements) if elements else 0
                if count > 0:
                    clickable_element = first_card.locator(selector).first
                    used_selector = selector
                    self.logger.info(f"âœ… æ‰¾åˆ°å®‰å…¨ç‚¹å‡»åŒºåŸŸ: {selector}")
                    break
            
            if not clickable_element:
                raise Exception("æœªæ‰¾åˆ°å®‰å…¨çš„å¯ç‚¹å‡»åŒºåŸŸ")
            
            # 4. è®°å½•åŸURL
            original_url = page.url
            self.logger.info(f"åŸå§‹URL: {original_url}")
            
            # 5. æ»šåŠ¨åˆ°å…ƒç´ å¯è§ä½ç½®
            clickable_element.scroll_into_view_if_needed()
            time.sleep(0.2)
            
            # 6. ç‚¹å‡»å…ƒç´ 
            clickable_element.click()
            self.logger.info(f"âœ… å·²ç‚¹å‡»è·Ÿå–å¡ç‰‡çš„ {used_selector} åŒºåŸŸ")
            
            # 7. ç­‰å¾…é¡µé¢è·³è½¬
            time.sleep(2)
            
            # 8. è·å–è·³è½¬åçš„URL
            new_url = page.url
            self.logger.info(f"è·³è½¬åURL: {new_url}")
            
            # 9. éªŒè¯è·³è½¬
            if new_url == original_url:
                raise Exception("é¡µé¢æœªè·³è½¬ï¼Œç‚¹å‡»å¯èƒ½å¤±è´¥")
            
            # 10. éªŒè¯è·³è½¬åˆ°å•†å“é¡µï¼ˆè€Œéåº—é“ºé¦–é¡µï¼‰
            if '/product/' not in new_url:
                self.logger.warning(f"âš ï¸ å¯èƒ½è·³è½¬åˆ°äº†åº—é“ºé¦–é¡µ: {new_url}")
                # ä¸æŠ›å¼‚å¸¸ï¼Œå› ä¸ºå¯èƒ½æ˜¯é¢„æœŸè¡Œä¸º
            
            # 11. ç«‹å³ä»æ–°URLæå–product_id
            product_id = self._extract_product_id(new_url)
            if product_id:
                self.logger.info(f"âœ… æå–åˆ°è·Ÿå–å•†å“ID: {product_id}")
            else:
                self.logger.warning(f"âš ï¸ æ— æ³•ä»URLæå–å•†å“ID: {new_url}")
            
            return new_url, product_id
            
        except Exception as e:
            self.logger.error(f"ç‚¹å‡»ç¬¬ä¸€ä¸ªè·Ÿå–åº—é“ºå¤±è´¥: {e}")
            raise
    #     pass
