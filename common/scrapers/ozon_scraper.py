"""
OZONå¹³å°æŠ“å–å™¨

è´Ÿè´£ä»OZONå¹³å°æŠ“å–å•†å“ä»·æ ¼ä¿¡æ¯å’Œè·Ÿå–åº—é“ºæ•°æ®ã€‚
åŸºäºæ–°çš„browser_serviceæ¶æ„ã€‚
"""

import asyncio
import logging
import time
from typing import Dict, Any, List, Optional, Tuple

from .xuanping_browser_service import XuanpingBrowserServiceSync
from .competitor_scraper import CompetitorScraper
from ..models import ProductInfo, CompetitorStore, clean_price_string, ScrapingResult
from ..config import GoodStoreSelectorConfig
from ..config.ozon_selectors import get_ozon_selectors_config, OzonSelectorsConfig
from ..business.profit_evaluator import ProfitEvaluator
from .erp_plugin_scraper import ErpPluginScraper


class OzonScraper:
    """OZONå¹³å°æŠ“å–å™¨ - åŸºäºbrowser_serviceæ¶æ„"""

    def __init__(self, config: Optional[GoodStoreSelectorConfig] = None,
                 selectors_config: Optional[OzonSelectorsConfig] = None):
        """åˆå§‹åŒ–OZONæŠ“å–å™¨"""
        self.config = config or GoodStoreSelectorConfig()
        self.selectors_config = selectors_config or get_ozon_selectors_config()
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        self.base_url = self.config.scraping.ozon_base_url

        # ğŸ”§ æ€§èƒ½ä¼˜åŒ–ï¼šä½¿ç”¨å…±äº«çš„æµè§ˆå™¨æœåŠ¡ï¼Œé¿å…é‡å¤åˆ›å»º
        self.browser_service = XuanpingBrowserServiceSync()

        # åˆ›å»ºè·Ÿå–æŠ“å–å™¨
        self.competitor_scraper = CompetitorScraper(selectors_config=self.selectors_config)

        # åˆ›å»ºåˆ©æ¶¦è¯„ä¼°å™¨
        self.profit_evaluator = ProfitEvaluator(
            profit_calculator_path=self.config.excel.profit_calculator_path,
            config=self.config
        )

        # åˆå§‹åŒ–ERPæ’ä»¶æŠ“å–å™¨ï¼ˆå…±äº«browser_serviceå®ä¾‹ï¼‰
        self.erp_scraper = ErpPluginScraper(self.config, self.browser_service)

    def scrape_product_prices(self, product_url: str) -> ScrapingResult:
        """
        æŠ“å–å•†å“ä»·æ ¼ä¿¡æ¯
        
        Args:
            product_url: å•†å“URL
            
        Returns:
            ScrapingResult: æŠ“å–ç»“æœï¼ŒåŒ…å«ä»·æ ¼ä¿¡æ¯
        """
        start_time = time.time()

        try:
            # ä½¿ç”¨æµè§ˆå™¨æœåŠ¡æŠ“å–æ•°æ®
            async def extract_price_data(browser_service):
                """å¼‚æ­¥æå–ä»·æ ¼æ•°æ®"""
                try:
                    # ğŸ”§ æ€§èƒ½ä¼˜åŒ–ï¼šå‡å°‘ä¸å¿…è¦çš„ç­‰å¾…æ—¶é—´
                    await asyncio.sleep(0.5)

                    # è·å–é¡µé¢å†…å®¹
                    page_content = await browser_service.get_page_content()
                    if not page_content:
                        self.logger.error("æœªèƒ½è·å–é¡µé¢å†…å®¹")
                        return {}

                    # è§£æä»·æ ¼ä¿¡æ¯ - ç›´æ¥è°ƒç”¨æ ¸å¿ƒæ–¹æ³•
                    from bs4 import BeautifulSoup
                    soup = BeautifulSoup(page_content, 'html.parser')
                    price_data = self._extract_price_data_core(soup)

                    # ä¿å­˜ä»·æ ¼æ•°æ®ä¾›ERPæŠ“å–ä½¿ç”¨
                    self._last_price_data = price_data

                    return price_data

                except Exception as e:
                    self.logger.error(f"æå–ä»·æ ¼æ•°æ®å¤±è´¥: {e}")
                    return {}

            # ä½¿ç”¨æµè§ˆå™¨æœåŠ¡æŠ“å–é¡µé¢æ•°æ®
            result = self.browser_service.scrape_page_data(product_url, extract_price_data)

            if result.success and result.data:
                return ScrapingResult(
                    success=True,
                    data=result.data,
                    execution_time=time.time() - start_time
                )
            else:
                return ScrapingResult(
                    success=False,
                    data={},
                    error_message=result.error_message or "æœªèƒ½æå–åˆ°ä»·æ ¼ä¿¡æ¯",
                    execution_time=time.time() - start_time
                )

        except Exception as e:
            self.logger.error(f"æŠ“å–å•†å“ä»·æ ¼å¤±è´¥: {e}")
            return ScrapingResult(
                success=False,
                data={},
                error_message=str(e),
                execution_time=time.time() - start_time
            )

    def scrape_competitor_stores(self, product_url: str, max_competitors: int = 10) -> ScrapingResult:
        """
        æŠ“å–è·Ÿå–åº—é“ºä¿¡æ¯

        Args:
            product_url: å•†å“URL
            max_competitors: æœ€å¤§è·Ÿå–åº—é“ºæ•°é‡ï¼Œé»˜è®¤10ä¸ª

        Returns:
            ScrapingResult: æŠ“å–ç»“æœï¼ŒåŒ…å«è·Ÿå–åº—é“ºä¿¡æ¯
        """
        start_time = time.time()

        try:
            async def extract_competitor_data(browser_service):
                """å¼‚æ­¥æå–è·Ÿå–åº—é“ºæ•°æ®"""
                try:
                    # ğŸ”§ æ€§èƒ½ä¼˜åŒ–ï¼šå‡å°‘ä¸å¿…è¦çš„ç­‰å¾…æ—¶é—´
                    await asyncio.sleep(0.5)

                    # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨CompetitorScraperçš„ä¸¥æ ¼è·Ÿå–æ£€æµ‹æ–¹æ³•
                    page = browser_service.browser_driver.page
                    popup_result = await self.competitor_scraper.open_competitor_popup(page)

                    # ğŸ¯ æ ¹æ®ä¸¥æ ¼æ£€æµ‹ç»“æœå†³å®šåç»­å¤„ç†
                    if not popup_result['success']:
                        self.logger.error(f"è·Ÿå–æ£€æµ‹å¤±è´¥: {popup_result['error_message']}")
                        return {'competitors': [], 'total_count': 0}

                    if not popup_result['has_competitors']:
                        self.logger.info("âœ… ç¡®è®¤æ— è·Ÿå–ï¼Œè·³è¿‡è·Ÿå–ä¿¡æ¯æå–")
                        return {'competitors': [], 'total_count': 0}

                    if not popup_result['popup_opened']:
                        self.logger.warning("âš ï¸ æœ‰è·Ÿå–ä½†æµ®å±‚æœªæ‰“å¼€ï¼Œè·³è¿‡è·Ÿå–ä¿¡æ¯æå–")
                        return {'competitors': [], 'total_count': 0}

                    # ğŸ”§ ä¿®å¤ï¼šè·å–æ£€æµ‹åˆ°çš„æ€»è·Ÿå–æ•°é‡ï¼ˆè€Œä¸æ˜¯å®é™…æå–çš„æ•°é‡ï¼‰
                    page = browser_service.browser_driver.page
                    detected_total_count = await self.competitor_scraper._get_competitor_count(page)

                    # è·å–é¡µé¢å†…å®¹
                    page_content = await browser_service.get_page_content()

                    # è§£æè·Ÿå–åº—é“ºä¿¡æ¯ - ä¿®å¤ï¼šä½¿ç”¨CompetitorScraper
                    competitors = await self.competitor_scraper.extract_competitors_from_content(page_content,
                                                                                                 max_competitors)

                    # ğŸ”§ ä¿®å¤ï¼šè¿”å›æ£€æµ‹åˆ°çš„æ€»æ•°é‡ï¼Œè€Œä¸æ˜¯å®é™…æå–çš„æ•°é‡
                    return {
                        'competitors': competitors,
                        'total_count': detected_total_count if detected_total_count is not None else len(competitors)
                    }

                except Exception as e:
                    self.logger.error(f"æå–è·Ÿå–åº—é“ºæ•°æ®å¤±è´¥: {e}")
                    return {'competitors': [], 'total_count': 0}

            # ä½¿ç”¨æµè§ˆå™¨æœåŠ¡æŠ“å–é¡µé¢æ•°æ®
            result = self.browser_service.scrape_page_data(product_url, extract_competitor_data)

            if result.success:
                return ScrapingResult(
                    success=True,
                    data=result.data,
                    execution_time=time.time() - start_time
                )
            else:
                return ScrapingResult(
                    success=False,
                    data={'competitors': [], 'total_count': 0},
                    error_message=result.error_message or "æ— æ³•æŠ“å–è·Ÿå–åº—é“ºä¿¡æ¯",
                    execution_time=time.time() - start_time
                )

        except Exception as e:
            self.logger.error(f"æŠ“å–è·Ÿå–åº—é“ºä¿¡æ¯å¤±è´¥: {e}")
            return ScrapingResult(
                success=False,
                data={'competitors': [], 'total_count': 0},
                error_message=str(e),
                execution_time=time.time() - start_time
            )

    # æŠ“å–å•†å“ä¿¡æ¯çš„ä¸»å…¥å£
    def scrape(self, product_url: str, include_competitors: bool = False, **kwargs) -> ScrapingResult:
        """
        ç»¼åˆæŠ“å–å•†å“ä¿¡æ¯
        
        Args:
            product_url: å•†å“URL
            include_competitors: æ˜¯å¦åŒ…å«è·Ÿå–åº—é“ºä¿¡æ¯
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            ScrapingResult: æŠ“å–ç»“æœ
        """
        start_time = time.time()

        try:
            # æŠ“å–ä»·æ ¼ä¿¡æ¯
            price_result = self.scrape_product_prices(product_url)
            if not price_result.success:
                return price_result

            result_data = {
                'product_url': product_url,
                'price_data': price_result.data,
                'include_competitors': include_competitors
            }


            # åˆ¤æ–­è·Ÿå–ä»·æ ¼æ¯”é»‘æ ‡ä»·æ ¼ã€ç»¿æ ‡ä»·æ ¼æ˜¯å¦æ›´ä½,ç»¿æ ‡ä»·æ ¼å¦‚æœä¸å­˜åœ¨åˆ™æ¯”ä»·é»‘æ ‡ä»·æ ¼å³å¯ï¼›
            has_better_price = self.profit_evaluator.has_better_competitor_price(result_data)

            # æŠ“å–ERPåŒºåŸŸä¿¡æ¯
            erp_result = self.scrape_erp_info()
            if erp_result.success:
                result_data['erp_data'] = erp_result.data
            else:
                self.logger.warning(f"æŠ“å–ERPä¿¡æ¯å¤±è´¥: {erp_result.error_message}")
                result_data['erp_data'] = {}


            # å¦‚æœéœ€è¦ï¼ŒæŠ“å–è·Ÿå–åº—é“ºä¿¡æ¯
            if include_competitors and has_better_price:
                competitors_result = self.scrape_competitor_stores(product_url)
                if competitors_result.success:
                    result_data['competitors'] = competitors_result.data['competitors']
                    # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨æ£€æµ‹åˆ°çš„æ€»è·Ÿå–æ•°é‡ï¼Œè€Œä¸æ˜¯å®é™…æå–çš„åº—é“ºæ•°é‡
                    result_data['competitor_count'] = competitors_result.data.get('total_count', len(
                        competitors_result.data['competitors']))


                else:
                    self.logger.warning(f"æŠ“å–è·Ÿå–åº—é“ºä¿¡æ¯å¤±è´¥: {competitors_result.error_message}")
                    result_data['competitors'] = []
                    result_data['competitor_count'] = 0
            else:
                # å³ä½¿ä¸æŠ“å–è·Ÿå–åº—é“ºï¼Œä¹Ÿè¦è®¾ç½® competitor_count
                result_data['competitors'] = []
                result_data['competitor_count'] = 0

            # å¦‚æœinclude_competitors = False, å¹¶ä¸”include_competitors = Trueï¼Œå¹¶ä¸”result_dataé‡Œå­˜åœ¨itemUrlï¼Œåˆ™æŠ“å–scrapeå½“å‰å•†å“çš„ä¿¡æ¯
            # competitor_product_url = result_data.get('competitor_product_url')
            # competitor_item_result = None
            # if not include_competitors and competitor_product_url:
            #     competitor_item_result = self.scrape(competitor_product_url, include_competitors=False)
            #
            # # ç¼–å†™ä¸€ä¸ªå‡½æ•°chooseGoodItemæ ¹æ®competitor item resultå’ŒåŸå§‹çš„result_data
            # # è¿›è¡ŒåŠ å·¥å’ŒéªŒè¯ï¼Œè¿”å›ä¸€ä¸ªæ–°çš„result_dataï¼ŒåŒ…å«ä¸€ä¸ªå½“å‰å•†å“ä»¥åŠcompetitorå•†å“ï¼Œå…ˆä¸å®ç°é€»è¾‘æ‰“å°å³å¯ã€‚
            # if competitor_item_result and competitor_item_result.success:
            #     self.combine_item(competitor_item_result.data, result_data)

            return ScrapingResult(
                success=True,
                data=result_data,
                execution_time=time.time() - start_time
            )

        except Exception as e:
            self.logger.error(f"ç»¼åˆæŠ“å–å•†å“ä¿¡æ¯å¤±è´¥: {e}")
            return ScrapingResult(
                success=False,
                data={},
                error_message=str(e),
                execution_time=time.time() - start_time
            )

    def _extract_price_data_core(self, soup) -> Dict[str, Any]:
        """
        æ ¸å¿ƒä»·æ ¼æå–é€»è¾‘ - ç®€åŒ–ç‰ˆæœ¬

        Args:
            soup: BeautifulSoupå¯¹è±¡
            is_async: æ˜¯å¦å¼‚æ­¥è°ƒç”¨

        Returns:
            Dict[str, Any]: ä»·æ ¼æ•°æ®
        """
        try:
            price_data = {}

            # æå–å•†å“å›¾ç‰‡
            image_url = self._extract_product_image_core(soup)
            if image_url:
                price_data['image_url'] = image_url

            # æå–åŸºç¡€ä»·æ ¼ï¼ˆç»¿æ ‡ã€é»‘æ ‡ï¼‰
            basic_prices = self._extract_basic_prices(soup)
            price_data.update(basic_prices)

            # ğŸ”§ ä¿®å¤ï¼šç›´æ¥åœ¨ä¸»æµç¨‹ä¸­æ£€æµ‹è·Ÿå–å…³é”®è¯å¹¶æå–ä»·æ ¼
            page_text = soup.get_text()

            # æ£€æµ‹è·Ÿå–å…³é”®è¯
            for keyword in self.selectors_config.COMPETITOR_KEYWORDS:
                if keyword.lower() in page_text.lower():
                    self.logger.info(f"ğŸ” æ£€æµ‹åˆ°è·Ÿå–å…³é”®è¯: {keyword}")
                    price_data.update({
                        'has_competitors': True,
                        'competitor_keyword': keyword
                    })

                    # æå–è·Ÿå–ä»·æ ¼
                    competitor_price = self._extract_competitor_price_value(soup)
                    if competitor_price:
                        price_data['competitor_price'] = competitor_price
                        self.logger.info(f"ğŸ’° è·Ÿå–ä»·æ ¼: {competitor_price}â‚½")
                    break

            return price_data

        except Exception as e:
            self._handle_extraction_error(e, "æå–ä»·æ ¼æ•°æ®")
            return {}

    def _extract_basic_prices(self, soup) -> Dict[str, Any]:
        """æå–åŸºç¡€ä»·æ ¼ï¼ˆç»¿æ ‡ã€é»‘æ ‡ï¼‰"""
        prices = {}
        green_price = None
        black_price = None

        # ğŸ”§ ä¿®å¤ï¼šä¸¥æ ¼æŒ‰ç…§é€‰æ‹©å™¨ç±»å‹æå–ä»·æ ¼ï¼Œé¿å…æ··æ·†
        for selector, price_type in self.selectors_config.PRICE_SELECTORS:
            try:
                elements = soup.select(selector)
                self.logger.debug(f"ğŸ” ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' (ç±»å‹: {price_type}) æ‰¾åˆ° {len(elements)} ä¸ªå…ƒç´ ")

                for element in elements:
                    price = self._extract_price_from_element(element)

                    # ä½¿ç”¨ _validate_price éªŒè¯ä»·æ ¼
                    if not self._validate_price(price, price_type):
                        continue

                    # ğŸ”§ ä¿®å¤ï¼šä¸¥æ ¼æŒ‰ç…§ä»·æ ¼ç±»å‹åˆ†é…ï¼Œé¿å…é‡å¤èµ‹å€¼
                    if price_type == "green" and green_price is None:
                        green_price = price
                        self.logger.info(f"âœ… ç»¿æ ‡ä»·æ ¼: {green_price}â‚½")
                        break  # æ‰¾åˆ°ç»¿æ ‡ä»·æ ¼åç«‹å³è·³å‡ºå†…å±‚å¾ªç¯
                    elif price_type == "black" and black_price is None:
                        black_price = price
                        self.logger.info(f"âœ… é»‘æ ‡ä»·æ ¼: {black_price}â‚½")
                        break  # æ‰¾åˆ°é»‘æ ‡ä»·æ ¼åç«‹å³è·³å‡ºå†…å±‚å¾ªç¯

            except Exception as e:
                self.logger.debug(f"é€‰æ‹©å™¨ '{selector}' å¤„ç†å¤±è´¥: {e}")
                continue

        # ğŸ”§ ä¿®å¤ï¼šæ˜ç¡®è®°å½•ä»·æ ¼æå–ç»“æœ
        if green_price is None:
            self.logger.info("â„¹ï¸ æœªæ‰¾åˆ°ç»¿æ ‡ä»·æ ¼")
        if black_price is None:
            self.logger.warning("âš ï¸ æœªæ‰¾åˆ°é»‘æ ‡ä»·æ ¼")

        # ğŸ”§ ä¿®å¤ï¼šåªæœ‰å½“ä»·æ ¼ç¡®å®å­˜åœ¨æ—¶æ‰æ·»åŠ åˆ°è¿”å›æ•°æ®ä¸­
        if green_price is not None:
            prices['green_price'] = green_price
        if black_price is not None:
            prices['black_price'] = black_price

        self.logger.debug(f"ğŸ¯ æœ€ç»ˆæå–çš„ä»·æ ¼æ•°æ®: {prices}")
        return prices

    def _validate_price(self, price: Optional[float], price_type: str) -> bool:
        """
        éªŒè¯ä»·æ ¼æ˜¯å¦æœ‰æ•ˆ

        Args:
            price: ä»·æ ¼å€¼
            price_type: ä»·æ ¼ç±»å‹åç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰

        Returns:
            bool: ä»·æ ¼æ˜¯å¦æœ‰æ•ˆ
        """
        if price is None or price <= 0:
            self.logger.debug(f"âš ï¸ {price_type}ä»·æ ¼æ— æ•ˆ: {price}")
            return False
        return True

    def _handle_extraction_error(self, error: Exception, context: str) -> None:
        """
        ç»Ÿä¸€å¤„ç†æå–é”™è¯¯

        Args:
            error: å¼‚å¸¸å¯¹è±¡
            context: ä¸Šä¸‹æ–‡æè¿°
        """
        self.logger.error(f"âŒ {context}å¤±è´¥: {error}")

    def _extract_price_from_element(self, element) -> Optional[float]:
        """
        ä»å…ƒç´ ä¸­æå–ä»·æ ¼æ•°å€¼

        Args:
            element: BeautifulSoupå…ƒç´ 

        Returns:
            float: ä»·æ ¼æ•°å€¼ï¼Œå¦‚æœæå–å¤±è´¥è¿”å›None
        """
        try:
            if not element:
                return None

            # è·å–å…ƒç´ æ–‡æœ¬
            text = element.get_text(strip=True)
            if not text:
                return None

            # ä½¿ç”¨clean_price_stringå‡½æ•°æå–ä»·æ ¼
            price = clean_price_string(text, self.selectors_config)
            return price

        except Exception as e:
            self._handle_extraction_error(e, "ä»å…ƒç´ æå–ä»·æ ¼")
            return None

    def _extract_competitor_price_value(self, soup) -> Optional[float]:
        """æå–å…·ä½“çš„è·Ÿå–ä»·æ ¼æ•°å€¼ - ä½¿ç”¨é…ç½®çš„ç²¾ç¡®é€‰æ‹©å™¨"""
        try:
            # ğŸ¯ ä½¿ç”¨é…ç½®çš„ç²¾ç¡®è·Ÿå–ä»·æ ¼é€‰æ‹©å™¨
            competitor_price_selector = self.selectors_config.COMPETITOR_PRICE_SELECTOR

            self.logger.debug(f"ğŸ” ä½¿ç”¨ç²¾ç¡®è·Ÿå–ä»·æ ¼é€‰æ‹©å™¨: {competitor_price_selector}")

            # æŸ¥æ‰¾è·Ÿå–ä»·æ ¼å…ƒç´ 
            competitor_elements = soup.select(competitor_price_selector)

            for element in competitor_elements:
                text = element.get_text(strip=True)
                self.logger.debug(f"ğŸ” æ‰¾åˆ°è·Ÿå–ä»·æ ¼å…ƒç´ æ–‡æœ¬: '{text}'")

                # ğŸ”§ ä¿®å¤ï¼šåªå¤„ç†åŒ…å«ä»·æ ¼ç¬¦å·çš„å…ƒç´ ï¼Œè¿‡æ»¤æ‰é…é€æ—¶é—´ç­‰éä»·æ ¼ä¿¡æ¯
                # ä½¿ç”¨é…ç½®åŒ–çš„è´§å¸ç¬¦å·æ£€æŸ¥
                has_currency = any(symbol.lower() in text.lower() for symbol in self.selectors_config.CURRENCY_SYMBOLS)
                if not has_currency:
                    self.logger.debug(f"âš ï¸ è·³è¿‡éä»·æ ¼å…ƒç´ : '{text}'")
                    continue

                # æå–ä»·æ ¼æ•°å€¼ - å¤„ç† "From 3 800 â‚½" æ ¼å¼
                price = self._extract_price_from_element(element)
                if self._validate_price(price, "è·Ÿå–"):
                    self.logger.debug(f"ğŸ¯ æˆåŠŸæå–è·Ÿå–ä»·æ ¼: {price}â‚½")
                    return price

            self.logger.debug("âš ï¸ æœªæ‰¾åˆ°åŒ…å«ä»·æ ¼ç¬¦å·çš„è·Ÿå–ä»·æ ¼å…ƒç´ ")
            return None

        except Exception as e:
            self._handle_extraction_error(e, "æå–è·Ÿå–ä»·æ ¼")
            return None

    # ğŸ”§ ä¿®å¤ï¼šåˆ é™¤é‡å¤çš„è·Ÿå–åº—é“ºæå–é€»è¾‘ï¼Œè¿™äº›åŠŸèƒ½åº”è¯¥ç”± CompetitorScraper è´Ÿè´£
    # åˆ é™¤äº†å¤§é‡é‡å¤çš„è·Ÿå–åº—é“ºç›¸å…³ä»£ç ï¼ŒèŒè´£åˆ†ç¦»ï¼š
    # - OzonScraper: è´Ÿè´£ä»·æ ¼æå–
    # - CompetitorScraper: è´Ÿè´£è·Ÿå–åº—é“ºäº¤äº’å’Œæå–

    def _extract_product_image_core(self, soup) -> Optional[str]:
        """
        æ ¸å¿ƒå›¾ç‰‡æå–é€»è¾‘ - ç»Ÿä¸€å®ç°é¿å…é‡å¤

        Args:
            soup: BeautifulSoupå¯¹è±¡

        Returns:
            str: å•†å“å›¾ç‰‡URLï¼Œå¦‚æœæå–å¤±è´¥è¿”å›None
        """
        try:
            for selector in self.selectors_config.IMAGE_SELECTORS:
                img_element = soup.select_one(selector)
                if img_element:
                    src = img_element.get('src')
                    if src:
                        high_res_url = self._convert_to_high_res_image(src)
                        self.logger.info(f"âœ… æˆåŠŸæå–å•†å“å›¾ç‰‡: {high_res_url}")
                        return high_res_url

            self.logger.warning("âš ï¸ æœªæ‰¾åˆ°å•†å“å›¾ç‰‡")
            return None

        except Exception as e:
            self._handle_extraction_error(e, "æå–å•†å“å›¾ç‰‡")
            return None

    def _convert_to_high_res_image(self, image_url: str) -> str:
        """
        å°†å›¾ç‰‡URLè½¬æ¢ä¸ºé«˜æ¸…ç‰ˆæœ¬

        Args:
            image_url: åŸå§‹å›¾ç‰‡URL

        Returns:
            str: é«˜æ¸…å›¾ç‰‡URL
        """
        try:
            import re
            # å°†wc50æˆ–wc100æ›¿æ¢ä¸ºwc1000
            high_res_url = re.sub(r'/wc\d+/', '/wc1000/', image_url)
            return high_res_url
        except Exception as e:
            self.logger.warning(f"è½¬æ¢é«˜æ¸…å›¾ç‰‡URLå¤±è´¥: {e}")
            return image_url

    def scrape_erp_info(self) -> ScrapingResult:
        """
        æŠ“å–ERPæ’ä»¶ä¿¡æ¯

        Returns:
            ScrapingResult: ERPæŠ“å–ç»“æœ
        """
        try:
            # ä½¿ç”¨å…±äº«çš„browser_serviceå®ä¾‹æŠ“å–ERPä¿¡æ¯
            return self.erp_scraper.scrape()

        except Exception as e:
            self.logger.error(f"æŠ“å–ERPä¿¡æ¯å¤±è´¥: {e}")
            return ScrapingResult(
                success=False,
                data={},
                error_message=str(e)
            )

    def close(self):
        """
        å…³é—­æŠ“å–å™¨ï¼Œæ¸…ç†èµ„æº
        """
        try:
            if hasattr(self, 'browser_service') and self.browser_service:
                self.browser_service.close()
                self.logger.info("ğŸ”’ OzonScraper å·²å…³é—­")
            if hasattr(self, 'erp_scraper') and self.erp_scraper:
                self.erp_scraper.close()
        except Exception as e:
            self.logger.error(f"å…³é—­ OzonScraper æ—¶å‘ç”Ÿé”™è¯¯: {e}")

    def __del__(self):
        """
        ææ„å‡½æ•°ï¼Œç¡®ä¿èµ„æºè¢«æ­£ç¡®é‡Šæ”¾
        """
        try:
            self.close()
        except:
            pass

    # def combine_item(self, data, result_data):
    #     pass
