"""
OZONå¹³å°æŠ“å–å™¨

è´Ÿè´£ä»OZONå¹³å°æŠ“å–å•†å“ä»·æ ¼ä¿¡æ¯å’Œè·Ÿå–åº—é“ºæ•°æ®ã€‚
åŸºäºæ–°çš„browser_serviceæ¶æ„ã€‚

é‡æ„ç‰ˆæœ¬ï¼šé›†æˆCompetitorDetectionServiceï¼Œä½¿ç”¨ç»Ÿä¸€å·¥å…·ç±»
"""
import logging
import time
from typing import Dict, Any, List, Optional, Tuple
from bs4 import BeautifulSoup

from .base_scraper import BaseScraper
from rpa.browser.browser_service import SimplifiedBrowserService

from ..models import ScrapingResult
from ..config import GoodStoreSelectorConfig
from ..config.ozon_selectors_config import get_ozon_selectors_config, OzonSelectorsConfig
from ..config.currency_config import get_currency_config
# å»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯ä¾èµ–
def get_profit_evaluator():
    from common.business.profit_evaluator import ProfitEvaluator
    return ProfitEvaluator

def get_erp_plugin_scraper():
    from .erp_plugin_scraper import ErpPluginScraper
    return ErpPluginScraper
from ..utils.wait_utils import WaitUtils, wait_for_content_smart
from ..utils.scraping_utils import ScrapingUtils


def _upd_competitor_cnt(data: Dict[str, Any], context: Optional[Dict[str, Any]] = None):
    if not context:
        return
    competitor_cnt = 0
    if data.get('competitor_data'):
      competitor_cnt = data['competitor_data'].get('competitor_cnt', 0)
    elif data.get('erp_data'):
       competitor_cnt = data['erp_data'].get('competitor_cnt', 0)
    context.update({'competitor_cnt': competitor_cnt})




class OzonScraper(BaseScraper):
    """
    OZONå¹³å°æŠ“å–å™¨ - åŸºäºbrowser_serviceæ¶æ„

    å®ç°IProductScraperæ¥å£ï¼Œæä¾›æ ‡å‡†åŒ–çš„å•†å“ä¿¡æ¯æŠ“å–åŠŸèƒ½
    """

    def __init__(self, config: Optional[GoodStoreSelectorConfig] = None,
                 selectors_config: Optional[OzonSelectorsConfig] = None,
                 browser_service=None):
        """åˆå§‹åŒ–OZONæŠ“å–å™¨"""
        super().__init__()
        self.config = config or GoodStoreSelectorConfig()
        self.selectors_config = selectors_config or get_ozon_selectors_config()
        self.currency_config = get_currency_config()
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        self.base_url = self.config.browser.ozon_base_url
        self.browser_service = browser_service or SimplifiedBrowserService.get_global_instance()
        # ğŸ”§ é‡æ„ï¼šé›†æˆç»Ÿä¸€å·¥å…·ç±»
        self.wait_utils = WaitUtils(self.browser_service, self.logger)
        self.scraping_utils = ScrapingUtils(self.logger)
        ErpPluginScraper = get_erp_plugin_scraper()
        self.erp_scraper = ErpPluginScraper(self.config, self.browser_service)

    # æ ‡å‡†scrapeæ¥å£å®ç°
    def scrape(self, target: str,
               context: Optional[Dict[str, Any]] = None, **kwargs) -> ScrapingResult:
        """ç»Ÿä¸€çš„æŠ“å–æ¥å£ - æ‰å¹³åŒ–å®ç°"""
        start_time = time.time()

        try:
            # ç›´æ¥å¯¼èˆªåˆ°ç›®æ ‡é¡µé¢
            if not self.navigate_to(target):
                return ScrapingResult(
                    success=False,
                    data={},
                    error_message=f"æ— æ³•å¯¼èˆªåˆ°å•†å“é¡µé¢: {target}",
                    execution_time=time.time() - start_time
                )

            return ScrapingResult(
                success=True,
                data=self._extract_basic_product_info(target),
                execution_time=time.time() - start_time
            )

        except ValueError as e:
            raise ValueError(f"å‚æ•°é”™è¯¯: {str(e)}")
        except Exception as e:
            raise RuntimeError(f"æŠ“å–å¤±è´¥: {str(e)}")

    def _extract_basic_product_info(self, url: str, context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """ç›´æ¥æå–åŸºç¡€ä»·æ ¼æ•°æ®ï¼ˆæ‰å¹³åŒ–å®ç°ï¼‰"""
        try:
            page_content = self.scraping_utils.extract_data_with_js(self.browser_service,script="() => document.documentElement.outerHTML")
            soup = BeautifulSoup(page_content, 'html.parser')
            # è·å–æ’ä»¶æ•°æ®
            erp_data = self.erp_scraper.scrape(target=url, options={'soup': soup}).data
            # å¦‚æœè·å–å¤±è´¥ï¼Œåˆ™ç›´æ¥è¿”å›
            if not erp_data:
                return {}

            # è·å–å•†å“ä»·æ ¼ã€å•†å“å›¾ç‰‡
            data = {
                    'green_price': self.scraping_utils.extract_price_from_soup(soup, "green"),
                    'black_price': self.scraping_utils.extract_price_from_soup(soup, "black"),
                    'product_image': self._extract_product_image(soup),
                    'erp_data': erp_data,
                    'competitor_data': self._extract_competitor_price(soup),
                    }

            _upd_competitor_cnt(data,context)

            # æ¸…ç†ç©ºå€¼
            return {k: v for k, v in data.items() if v is not None}
        except Exception as e:
            self.logger.error(f"æå–åŸºç¡€ä»·æ ¼æ•°æ®å¤±è´¥: {e}")
            return {}

    # æ ¹æ®dataé‡Œçš„ä¿¡æ¯è®¾ç½®  competitor_cntï¼Œ å¯ä»¥ä»erp_dataé‡Œè·å– ä¹Ÿå¯ä»¥ ä»competitor_dataè·å–ï¼Œ è°å­˜åœ¨å°±ç”¨è°

    def _extract_competitor_price(self, soup) -> Optional[Dict[str, Any]]:

        try:
            # ä½¿ç”¨é…ç½®åŒ–çš„ç«äº‰è€…å®¹å™¨é€‰æ‹©å™¨
            result = wait_for_content_smart(self.selectors_config.competitor_area_selectors, self.browser_service, soup=soup)
            competitor_container=result['content']

            if not competitor_container:
                self.logger.warning("âš ï¸ æœªæ‰¾åˆ°ç«äº‰è€…ä¿¡æ¯å®¹å™¨")
                return None

            # ä½¿ç”¨é…ç½®åŒ–é€‰æ‹©å™¨å’Œå·¥å…·å¤ç”¨æå–ä»·æ ¼
            competitor_price = None
            for selector in self.selectors_config.store_price_selectors:
                try:
                    price_element = competitor_container.select_one(selector)
                    if price_element:
                        price_text = price_element.get_text(strip=True)
                        # å¤ç”¨ç°æœ‰çš„ä»·æ ¼æå–å·¥å…·
                        price = self.scraping_utils.extract_price(price_text)
                        if price:
                            competitor_price = price_text  # ä¿ç•™åŸå§‹ä»·æ ¼æ–‡æœ¬
                            self.logger.debug(f"âœ… æå–åˆ°ç«äº‰è€…ä»·æ ¼: {competitor_price}")
                            break
                except Exception as e:
                    self.logger.debug(f"ä»·æ ¼é€‰æ‹©å™¨å¤±è´¥: {e}")
                    continue

            # ä½¿ç”¨é…ç½®åŒ–é€‰æ‹©å™¨å’Œå·¥å…·å¤ç”¨æå–æ•°é‡
            competitor_count = None
            for selector in self.selectors_config.competitor_count_selectors:
                try:
                    count_element = competitor_container.select_one(selector)
                    if count_element:
                        count_text = count_element.get_text(strip=True)
                        # å¤ç”¨ç°æœ‰çš„æ•°å­—æå–å·¥å…·
                        count = self.scraping_utils.extract_number(count_text)
                        if count is not None:
                            competitor_count = count
                            self.logger.debug(f"âœ… æå–åˆ°ç«äº‰è€…æ•°é‡: {competitor_count}")
                            break
                except Exception as e:
                    self.logger.debug(f"æ•°é‡é€‰æ‹©å™¨å¤±è´¥: {e}")
                    continue

            # æ„å»ºè¿”å›æ•°æ®
            if competitor_price or competitor_count is not None:
                result = {}
                if competitor_price:
                    result["price"] = competitor_price
                if competitor_count is not None:
                    result["count"] = competitor_count

                self.logger.info(f"âœ… æˆåŠŸæå–ç«äº‰è€…æ•°æ®: {result}")
                return result

            self.logger.warning("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•ç«äº‰è€…æ•°æ®")
            return None

        except Exception as e:
            self.logger.error(f"âŒ æå–ç«äº‰è€…æ•°æ®å¤±è´¥: {e}")
            return None





    def _extract_product_image(self, soup) -> Optional[str]:
        """
        æ ¸å¿ƒå›¾ç‰‡æå–é€»è¾‘ - ä½¿ç”¨é€šç”¨æ–¹æ³•

        Args:
            soup: BeautifulSoupå¯¹è±¡

        Returns:
            str: å•†å“å›¾ç‰‡URLï¼Œå¦‚æœæå–å¤±è´¥è¿”å›None
        """
        try:
            # æ„å»ºOZONå¹³å°ç‰¹å®šçš„å›¾ç‰‡é…ç½®
            image_config = {
                'placeholder_patterns': [
                    'doodle_ozon_rus.png',
                    'doodle_ozone_rus.png',
                    'placeholder.png',
                    'no-image.png',
                    'default.png',
                    'loading.png'
                ],
                'valid_patterns': [
                    'multimedia',        # OZONçš„å•†å“å›¾ç‰‡é€šå¸¸åŒ…å«multimedia
                    's3/multimedia',     # å®Œæ•´çš„S3è·¯å¾„
                    'wc1000',           # é«˜æ¸…å›¾ç‰‡æ ‡è¯†
                    'wc750',            # ä¸­ç­‰åˆ†è¾¨ç‡å›¾ç‰‡
                    'wc500',            # æ ‡å‡†åˆ†è¾¨ç‡å›¾ç‰‡
                ],
                'valid_extensions': ['.jpg', '.jpeg', '.png', '.webp'],
                'valid_domains': ['ozon.ru', 'ozone.ru', 'ir.ozone.ru'],
                'conversion_config': {r'/wc\d+/': '/wc1000/'}
            }

            # ä½¿ç”¨é€šç”¨æ–¹æ³•æå–å›¾ç‰‡
            return self.scraping_utils.extract_product_image(
                soup,
                self.selectors_config.image_selectors,
                image_config
            )

        except Exception as e:
            self.logger.error(f"âŒ æå–å•†å“å›¾ç‰‡å¤±è´¥: {e}")
            return None



