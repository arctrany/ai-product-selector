"""
OZONå¹³å°æŠ“å–å™¨

è´Ÿè´£ä»OZONå¹³å°æŠ“å–å•†å“ä»·æ ¼ä¿¡æ¯å’Œè·Ÿå–åº—é“ºæ•°æ®ã€‚
åŸºäºæ–°çš„browser_serviceæ¶æ„ã€‚

é‡æ„ç‰ˆæœ¬ï¼šé›†æˆCompetitorDetectionServiceï¼Œä½¿ç”¨ç»Ÿä¸€å·¥å…·ç±»
"""

import asyncio
import logging
import time
from typing import Dict, Any, List, Optional, Tuple

from .base_scraper import BaseScraper
from .global_browser_singleton import get_global_browser_service

from ..models import CompetitorStore, clean_price_string, ScrapingResult
from ..config import GoodStoreSelectorConfig
from ..config.ozon_selectors_config import get_ozon_selectors_config, OzonSelectorsConfig
from ..config.currency_config import get_currency_config
# å»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯ä¾èµ–
def get_profit_evaluator():
    from business.profit_evaluator import ProfitEvaluator
    return ProfitEvaluator

def get_erp_plugin_scraper():
    from .erp_plugin_scraper import ErpPluginScraper
    return ErpPluginScraper
from ..services.competitor_detection_service import CompetitorDetectionService
from ..utils.wait_utils import WaitUtils
from ..utils.scraping_utils import ScrapingUtils
from ..interfaces.scraper_interface import IProductScraper, ScrapingMode, StandardScrapingOptions
from ..exceptions.scraping_exceptions import ScrapingException, NavigationException, DataExtractionException


class OzonScraper(BaseScraper, IProductScraper):
    """
    OZONå¹³å°æŠ“å–å™¨ - åŸºäºbrowser_serviceæ¶æ„

    å®ç°IProductScraperæ¥å£ï¼Œæä¾›æ ‡å‡†åŒ–çš„å•†å“ä¿¡æ¯æŠ“å–åŠŸèƒ½
    """

    def __init__(self, config: Optional[GoodStoreSelectorConfig] = None,
                 selectors_config: Optional[OzonSelectorsConfig] = None):
        """åˆå§‹åŒ–OZONæŠ“å–å™¨"""
        super().__init__()
        self.config = config or GoodStoreSelectorConfig()
        self.selectors_config = selectors_config or get_ozon_selectors_config()
        self.currency_config = get_currency_config()
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        self.base_url = self.config.scraping.ozon_base_url

        # ğŸ”§ æ€§èƒ½ä¼˜åŒ–ï¼šä½¿ç”¨å…±äº«çš„å…¨å±€æµè§ˆå™¨æœåŠ¡ï¼Œé¿å…é‡å¤åˆ›å»º
        self.browser_service = get_global_browser_service()

        # ğŸ”§ é‡æ„ï¼šé›†æˆç»Ÿä¸€å·¥å…·ç±»
        self.wait_utils = WaitUtils(self.browser_service, self.logger)
        self.scraping_utils = ScrapingUtils(self.logger)
        


        # å»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯ä¾èµ–
        ProfitEvaluator = get_profit_evaluator()
        ErpPluginScraper = get_erp_plugin_scraper()

        # åˆ›å»ºåˆ©æ¶¦è¯„ä¼°å™¨
        self.profit_evaluator = ProfitEvaluator(
            profit_calculator_path=self.config.excel.profit_calculator_path,
            config=self.config
        )

        # åˆå§‹åŒ–ERPæ’ä»¶æŠ“å–å™¨ï¼ˆå…±äº«browser_serviceå®ä¾‹ï¼‰
        self.erp_scraper = ErpPluginScraper(self.config, self.browser_service)
        
        # ğŸ”§ é‡æ„ï¼šåˆå§‹åŒ–ç»Ÿä¸€å·¥å…·ç±»
        self.wait_utils = WaitUtils(self.browser_service, self.logger)
        self.scraping_utils = ScrapingUtils(self.logger)

    def scrape_product_info(self,
                           product_url: str,
                           include_prices: bool = True,
                           include_reviews: bool = False,
                           options: Optional[Dict[str, Any]] = None) -> ScrapingResult:
        """
        æŠ“å–å•†å“åŸºæœ¬ä¿¡æ¯ï¼ˆæ ‡å‡†æ¥å£å®ç°ï¼‰

        Args:
            product_url: å•†å“URL
            include_prices: æ˜¯å¦åŒ…å«ä»·æ ¼ä¿¡æ¯
            include_reviews: æ˜¯å¦åŒ…å«è¯„ä»·ä¿¡æ¯
            options: æŠ“å–é€‰é¡¹

        Returns:
            ScrapingResult: å•†å“ä¿¡æ¯æŠ“å–ç»“æœ

        Raises:
            NavigationException: é¡µé¢å¯¼èˆªå¤±è´¥
            DataExtractionException: æ•°æ®æå–å¤±è´¥
        """
        try:
            # è§£æé€‰é¡¹
            scraping_options = StandardScrapingOptions(**(options or {}))

            # å¦‚æœåªéœ€è¦ä»·æ ¼ä¿¡æ¯ï¼Œä½¿ç”¨ä¼˜åŒ–çš„ä»·æ ¼æŠ“å–æ–¹æ³•
            if include_prices and not include_reviews:
                return self.scrape_product_prices(product_url)

            # å®Œæ•´çš„å•†å“ä¿¡æ¯æŠ“å–ï¼ˆçº¯å•†å“ä¿¡æ¯ï¼‰
            return self._scrape_comprehensive(
                product_url=product_url,
                **scraping_options.to_dict()
            )

        except Exception as e:
            raise DataExtractionException(
                field_name="product_info",
                message=f"å•†å“ä¿¡æ¯æŠ“å–å¤±è´¥: {str(e)}",
                context={'product_url': product_url, 'options': options},
                original_exception=e
            )

    def scrape_product_prices(self, product_url: str) -> ScrapingResult:
        """
        æŠ“å–å•†å“ä»·æ ¼ä¿¡æ¯ï¼ˆå…¼å®¹æ–¹æ³•ï¼‰

        Args:
            product_url: å•†å“URL

        Returns:
            ScrapingResult: æŠ“å–ç»“æœï¼ŒåŒ…å«ä»·æ ¼ä¿¡æ¯

        Raises:
            NavigationException: é¡µé¢å¯¼èˆªå¤±è´¥
            DataExtractionException: æ•°æ®æå–å¤±è´¥
        """
        start_time = time.time()

        try:
            # ä½¿ç”¨æµè§ˆå™¨æœåŠ¡æŠ“å–æ•°æ®
            def extract_price_data(browser_service):
                """åŒæ­¥æå–ä»·æ ¼æ•°æ®"""
                try:
                    # ğŸ”§ æ€§èƒ½ä¼˜åŒ–ï¼šå‡å°‘ä¸å¿…è¦çš„ç­‰å¾…æ—¶é—´
                    self.wait_utils.smart_wait(0.5)

                    # è·å–é¡µé¢å†…å®¹ - ä½¿ç”¨åŒæ­¥æ–¹æ³•
                    page_content = browser_service.evaluate_sync("() => document.documentElement.outerHTML")
                    if not page_content:
                        self.logger.error("æœªèƒ½è·å–é¡µé¢å†…å®¹")
                        return {}

                    # è§£æä»·æ ¼ä¿¡æ¯ - ç›´æ¥è°ƒç”¨æ ¸å¿ƒæ–¹æ³•
                    from bs4 import BeautifulSoup
                    soup = BeautifulSoup(page_content, 'html.parser')
                    price_data = self._extract_price_data_core(soup)

                    # ä¿å­˜ä»·æ ¼æ•°æ®ä¾›ERPæŠ“å–ä½¿ç”¨
                    self._last_price_data = price_data

                    return price_data

                except Exception as e:
                    self.logger.error(f"æå–ä»·æ ¼æ•°æ®å¤±è´¥: {e}")
                    return {}

            # ä½¿ç”¨ç»§æ‰¿çš„æŠ“å–æ–¹æ³•
            result = self.scrape_page_data(product_url, extract_price_data)

            if result.success and result.data:
                return ScrapingResult(
                    success=True,
                    data=result.data,
                    execution_time=time.time() - start_time
                )
            else:
                return ScrapingResult(
                    success=False,
                    data={},
                    error_message=result.error_message or "æœªèƒ½æå–åˆ°ä»·æ ¼ä¿¡æ¯",
                    execution_time=time.time() - start_time
                )

        except Exception as e:
            self.logger.error(f"æŠ“å–å•†å“ä»·æ ¼å¤±è´¥: {e}")
            return ScrapingResult(
                success=False,
                data={},
                error_message=str(e),
                execution_time=time.time() - start_time
            )



    # æ ‡å‡†scrapeæ¥å£å®ç°
    def scrape(self,
               target: str,
               mode: Optional[ScrapingMode] = None,
               options: Optional[Dict[str, Any]] = None,
               **kwargs) -> ScrapingResult:
        """
        ç»Ÿä¸€çš„æŠ“å–æ¥å£ï¼ˆæ ‡å‡†æ¥å£å®ç°ï¼‰

        Args:
            target: æŠ“å–ç›®æ ‡ï¼ˆå•†å“URLï¼‰
            mode: æŠ“å–æ¨¡å¼
            options: æŠ“å–é€‰é¡¹é…ç½®
            **kwargs: é¢å¤–å‚æ•°

        Returns:
            ScrapingResult: æ ‡å‡†åŒ–æŠ“å–ç»“æœ

        Raises:
            ScrapingException: æŠ“å–å¼‚å¸¸
        """
        try:
            # è§£æé€‰é¡¹
            scraping_options = StandardScrapingOptions(**(options or {}))

            # æ ¹æ®æ¨¡å¼é€‰æ‹©æŠ“å–ç­–ç•¥
            if mode == ScrapingMode.PRODUCT_DATA:
                return self.scrape_product_info(
                    product_url=target,
                    include_prices=True,
                    include_reviews=False,
                    options=options
                )
            else:
                # é»˜è®¤ä½¿ç”¨ç»¼åˆæŠ“å–æ–¹æ³•ï¼ˆçº¯å•†å“ä¿¡æ¯ï¼‰
                return self._scrape_comprehensive(
                    product_url=target,
                    **kwargs
                )

        except Exception as e:
            raise ScrapingException(
                message=f"æŠ“å–å¤±è´¥: {str(e)}",
                error_code="SCRAPING_FAILED",
                context={'target': target, 'mode': mode, 'options': options},
                original_exception=e
            )

    def _scrape_comprehensive(self,
                             product_url: str,
                             **kwargs) -> ScrapingResult:
        """
        ç»¼åˆæŠ“å–å•†å“ä¿¡æ¯ï¼ˆå†…éƒ¨æ–¹æ³•ï¼Œä¿æŒå‘åå…¼å®¹ï¼‰

        é‡æ„ç‰ˆæœ¬ï¼šä¸“æ³¨äºçº¯å•†å“ä¿¡æ¯æŠ“å–ï¼Œè·Ÿå–åŠŸèƒ½å§”æ‰˜ç»™ä¸“é—¨çš„æœåŠ¡å¤„ç†

        Args:
            product_url: å•†å“URL
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            ScrapingResult: æŠ“å–ç»“æœï¼ŒåŒ…å«å•†å“åŸºæœ¬ä¿¡æ¯å’ŒERPæ•°æ®
        """
        start_time = time.time()

        try:

            # æŠ“å–ä»·æ ¼ä¿¡æ¯
            price_result = self.scrape_product_prices(product_url)
            if not price_result.success:
                return price_result

            # ğŸ†” æå–å•†å“ ID
            product_id = self._extract_product_id(product_url)
            if product_id:
                self.logger.info(f"âœ… æå–åˆ°å•†å“ID: {product_id}")
            else:
                self.logger.warning(f"âš ï¸ æ— æ³•ä»URLæå–å•†å“ID: {product_url}")

            result_data = {
                'product_url': product_url,
                'product_id': product_id,
                'price_data': price_result.data
            }




            # æŠ“å–ERPåŒºåŸŸä¿¡æ¯
            erp_result = self.scrape_erp_info()
            if erp_result.success:
                result_data['erp_data'] = erp_result.data
            else:
                self.logger.warning(f"æŠ“å–ERPä¿¡æ¯å¤±è´¥: {erp_result.error_message}")
                result_data['erp_data'] = {}



                competitors_result = self.scrape_competitor_stores(product_url)
                if competitors_result.success:
                    result_data['competitors'] = competitors_result.data['competitors']
                    # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨æ£€æµ‹åˆ°çš„æ€»è·Ÿå–æ•°é‡ï¼Œè€Œä¸æ˜¯å®é™…æå–çš„åº—é“ºæ•°é‡
                    result_data['competitor_count'] = competitors_result.data.get('total_count', len(
                        competitors_result.data['competitors']))




            return ScrapingResult(
                success=True,
                data=result_data,
                execution_time=time.time() - start_time
            )

        except Exception as e:
            self.logger.error(f"ç»¼åˆæŠ“å–å•†å“ä¿¡æ¯å¤±è´¥: {e}")
            return ScrapingResult(
                success=False,
                data={},
                error_message=str(e),
                execution_time=time.time() - start_time
            )

    def _extract_price_data_core(self, soup) -> Dict[str, Any]:
        """
        æ ¸å¿ƒä»·æ ¼æå–é€»è¾‘ - é‡æ„ç‰ˆæœ¬ï¼Œä¸“æ³¨äºå•†å“åŸºæœ¬ä¿¡æ¯

        Args:
            soup: BeautifulSoupå¯¹è±¡

        Returns:
            Dict[str, Any]: ä»·æ ¼æ•°æ®
        """
        try:
            price_data = {}

            # æå–å•†å“å›¾ç‰‡
            image_url = self._extract_product_image_core(soup)
            if image_url:
                price_data['image_url'] = image_url

            # æå–åŸºç¡€ä»·æ ¼ï¼ˆç»¿æ ‡ã€é»‘æ ‡ï¼‰
            basic_prices = self._extract_basic_prices(soup)
            price_data.update(basic_prices)

            return price_data

        except Exception as e:
            self._handle_extraction_error(e, "æå–ä»·æ ¼æ•°æ®")
            return {}

    def _extract_basic_prices(self, soup) -> Dict[str, Any]:
        """æå–åŸºç¡€ä»·æ ¼ï¼ˆç»¿æ ‡ã€é»‘æ ‡ï¼‰"""
        prices = {}
        green_price = None
        black_price = None

        # ğŸ”§ ä¿®å¤ï¼šä¸¥æ ¼æŒ‰ç…§é€‰æ‹©å™¨ç±»å‹æå–ä»·æ ¼ï¼Œé¿å…æ··æ·†
        for selector, price_type, priority in self.selectors_config.price_selectors:
            try:
                elements = soup.select(selector)
                self.logger.debug(f"ğŸ” ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' (ç±»å‹: {price_type}) æ‰¾åˆ° {len(elements)} ä¸ªå…ƒç´ ")

                for element in elements:
                    price = self._extract_price_from_element(element)

                    # ä½¿ç”¨ _validate_price éªŒè¯ä»·æ ¼
                    if not self._validate_price(price, price_type):
                        continue

                    # ğŸ”§ ä¿®å¤ï¼šä¸¥æ ¼æŒ‰ç…§ä»·æ ¼ç±»å‹åˆ†é…ï¼Œé¿å…é‡å¤èµ‹å€¼
                    if price_type == "green" and green_price is None:
                        green_price = price
                        currency_symbol = self.currency_config.get_default_symbol()
                        self.logger.info(f"âœ… ç»¿æ ‡ä»·æ ¼: {green_price}{currency_symbol}")
                        break  # æ‰¾åˆ°ç»¿æ ‡ä»·æ ¼åç«‹å³è·³å‡ºå†…å±‚å¾ªç¯
                    elif price_type == "black" and black_price is None:
                        black_price = price
                        currency_symbol = self.currency_config.get_default_symbol()
                        self.logger.info(f"âœ… é»‘æ ‡ä»·æ ¼: {black_price}{currency_symbol}")
                        break  # æ‰¾åˆ°é»‘æ ‡ä»·æ ¼åç«‹å³è·³å‡ºå†…å±‚å¾ªç¯

            except Exception as e:
                self.logger.debug(f"é€‰æ‹©å™¨ '{selector}' å¤„ç†å¤±è´¥: {e}")
                continue

        # ğŸ”§ ä¿®å¤ï¼šæ˜ç¡®è®°å½•ä»·æ ¼æå–ç»“æœ
        if green_price is None:
            self.logger.info("â„¹ï¸ æœªæ‰¾åˆ°ç»¿æ ‡ä»·æ ¼")
        if black_price is None:
            self.logger.warning("âš ï¸ æœªæ‰¾åˆ°é»‘æ ‡ä»·æ ¼")

        # ğŸ”§ ä¿®å¤ï¼šåªæœ‰å½“ä»·æ ¼ç¡®å®å­˜åœ¨æ—¶æ‰æ·»åŠ åˆ°è¿”å›æ•°æ®ä¸­
        if green_price is not None:
            prices['green_price'] = green_price
        if black_price is not None:
            prices['black_price'] = black_price

        self.logger.debug(f"ğŸ¯ æœ€ç»ˆæå–çš„ä»·æ ¼æ•°æ®: {prices}")
        return prices

    def _validate_price(self, price: Optional[float], price_type: str) -> bool:
        """
        éªŒè¯ä»·æ ¼æ˜¯å¦æœ‰æ•ˆ

        Args:
            price: ä»·æ ¼å€¼
            price_type: ä»·æ ¼ç±»å‹åç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰

        Returns:
            bool: ä»·æ ¼æ˜¯å¦æœ‰æ•ˆ
        """
        if price is None or price <= 0:
            self.logger.debug(f"âš ï¸ {price_type}ä»·æ ¼æ— æ•ˆ: {price}")
            return False
        return True

    def _handle_extraction_error(self, error: Exception, context: str) -> None:
        """
        ç»Ÿä¸€å¤„ç†æå–é”™è¯¯

        Args:
            error: å¼‚å¸¸å¯¹è±¡
            context: ä¸Šä¸‹æ–‡æè¿°
        """
        self.logger.error(f"âŒ {context}å¤±è´¥: {error}")

    def _extract_price_from_element(self, element) -> Optional[float]:
        """
        ä»å…ƒç´ ä¸­æå–ä»·æ ¼æ•°å€¼

        Args:
            element: BeautifulSoupå…ƒç´ 

        Returns:
            float: ä»·æ ¼æ•°å€¼ï¼Œå¦‚æœæå–å¤±è´¥è¿”å›None
        """
        try:
            if not element:
                return None

            # è·å–å…ƒç´ æ–‡æœ¬
            text = element.get_text(strip=True)
            if not text:
                return None

            # ğŸ”§ é‡æ„ï¼šä½¿ç”¨ScrapingUtilsç»Ÿä¸€å¤„ç†ä»·æ ¼æå–
            price = self.scraping_utils.extract_price(text)
            return price

        except Exception as e:
            self._handle_extraction_error(e, "ä»å…ƒç´ æå–ä»·æ ¼")
            return None



    # ğŸ”§ ä¿®å¤ï¼šåˆ é™¤é‡å¤çš„è·Ÿå–åº—é“ºæå–é€»è¾‘ï¼Œè¿™äº›åŠŸèƒ½åº”è¯¥ç”± CompetitorScraper è´Ÿè´£
    # åˆ é™¤äº†å¤§é‡é‡å¤çš„è·Ÿå–åº—é“ºç›¸å…³ä»£ç ï¼ŒèŒè´£åˆ†ç¦»ï¼š
    # - OzonScraper: è´Ÿè´£ä»·æ ¼æå–
    # - CompetitorScraper: è´Ÿè´£è·Ÿå–åº—é“ºäº¤äº’å’Œæå–

    def _extract_product_image_core(self, soup) -> Optional[str]:
        """
        æ ¸å¿ƒå›¾ç‰‡æå–é€»è¾‘ - ç»Ÿä¸€å®ç°é¿å…é‡å¤ï¼ŒåŒ…å«å ä½ç¬¦è¿‡æ»¤

        Args:
            soup: BeautifulSoupå¯¹è±¡

        Returns:
            str: å•†å“å›¾ç‰‡URLï¼Œå¦‚æœæå–å¤±è´¥è¿”å›None
        """
        try:
            # å·²çŸ¥çš„å ä½ç¬¦å›¾ç‰‡æ¨¡å¼
            placeholder_patterns = [
                'doodle_ozon_rus.png',
                'doodle_ozone_rus.png',
                'placeholder.png',
                'no-image.png',
                'default.png',
                'loading.png'
            ]

            for selector in self.selectors_config.image_selectors:
                img_elements = soup.select(selector)
                self.logger.debug(f"ğŸ” é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(img_elements)} ä¸ªå›¾ç‰‡å…ƒç´ ")

                for img_element in img_elements:
                    src = img_element.get('src')
                    if not src:
                        continue

                    # è½¬æ¢ä¸ºé«˜æ¸…ç‰ˆæœ¬
                    high_res_url = self._convert_to_high_res_image(src)

                    # éªŒè¯å›¾ç‰‡URLæ˜¯å¦ä¸ºå ä½ç¬¦
                    if self._is_placeholder_image(high_res_url, placeholder_patterns):
                        self.logger.warning(f"âš ï¸ è·³è¿‡å ä½ç¬¦å›¾ç‰‡: {high_res_url}")
                        continue

                    # éªŒè¯å›¾ç‰‡URLæ˜¯å¦ä¸ºæœ‰æ•ˆçš„å•†å“å›¾ç‰‡
                    if self._is_valid_product_image(high_res_url):
                        self.logger.info(f"âœ… æˆåŠŸæå–å•†å“å›¾ç‰‡: {high_res_url}")
                        return high_res_url
                    else:
                        self.logger.debug(f"ğŸ” è·³è¿‡æ— æ•ˆå›¾ç‰‡: {high_res_url}")

            self.logger.warning("âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆçš„å•†å“å›¾ç‰‡")
            return None

        except Exception as e:
            self._handle_extraction_error(e, "æå–å•†å“å›¾ç‰‡")
            return None

    def _is_placeholder_image(self, image_url: str, placeholder_patterns: list) -> bool:
        """
        æ£€æŸ¥å›¾ç‰‡URLæ˜¯å¦ä¸ºå ä½ç¬¦å›¾ç‰‡

        Args:
            image_url: å›¾ç‰‡URL
            placeholder_patterns: å ä½ç¬¦å›¾ç‰‡æ¨¡å¼åˆ—è¡¨

        Returns:
            bool: Trueè¡¨ç¤ºæ˜¯å ä½ç¬¦å›¾ç‰‡ï¼ŒFalseè¡¨ç¤ºä¸æ˜¯
        """
        if not image_url:
            return True

        # æ£€æŸ¥URLä¸­æ˜¯å¦åŒ…å«å ä½ç¬¦æ¨¡å¼
        for pattern in placeholder_patterns:
            if pattern in image_url:
                return True

        # æ£€æŸ¥æ˜¯å¦åŒ…å«å…¶ä»–å·²çŸ¥çš„å ä½ç¬¦ç‰¹å¾
        placeholder_keywords = ['doodle', 'placeholder', 'default', 'no-image', 'loading']
        url_lower = image_url.lower()

        for keyword in placeholder_keywords:
            if keyword in url_lower:
                return True

        return False

    def _is_valid_product_image(self, image_url: str) -> bool:
        """
        éªŒè¯å›¾ç‰‡URLæ˜¯å¦ä¸ºæœ‰æ•ˆçš„å•†å“å›¾ç‰‡

        Args:
            image_url: å›¾ç‰‡URL

        Returns:
            bool: Trueè¡¨ç¤ºæ˜¯æœ‰æ•ˆå•†å“å›¾ç‰‡ï¼ŒFalseè¡¨ç¤ºæ— æ•ˆ
        """
        if not image_url:
            return False

        # æ£€æŸ¥æ˜¯å¦åŒ…å«æœ‰æ•ˆçš„å•†å“å›¾ç‰‡ç‰¹å¾
        valid_patterns = [
            'multimedia',        # OZONçš„å•†å“å›¾ç‰‡é€šå¸¸åŒ…å«multimedia
            's3/multimedia',     # å®Œæ•´çš„S3è·¯å¾„
            'wc1000',           # é«˜æ¸…å›¾ç‰‡æ ‡è¯†
            'wc750',            # ä¸­ç­‰åˆ†è¾¨ç‡å›¾ç‰‡
            'wc500',            # æ ‡å‡†åˆ†è¾¨ç‡å›¾ç‰‡
        ]

        url_lower = image_url.lower()

        # å¿…é¡»åŒ…å«è‡³å°‘ä¸€ä¸ªæœ‰æ•ˆæ¨¡å¼
        has_valid_pattern = any(pattern in url_lower for pattern in valid_patterns)

        # å¿…é¡»æ˜¯å›¾ç‰‡æ–‡ä»¶
        is_image_file = any(ext in url_lower for ext in ['.jpg', '.jpeg', '.png', '.webp'])

        # å¿…é¡»æ¥è‡ªOZON/OZONEåŸŸå
        is_ozon_domain = any(domain in url_lower for domain in ['ozon.ru', 'ozone.ru', 'ir.ozone.ru'])

        # ä¸èƒ½åŒ…å«æ˜æ˜¾çš„å ä½ç¬¦ç‰¹å¾
        has_placeholder_features = any(keyword in url_lower for keyword in ['doodle', 'placeholder', 'default', 'error'])

        return has_valid_pattern and is_image_file and is_ozon_domain and not has_placeholder_features

    def _convert_to_high_res_image(self, image_url: str) -> str:
        """
        å°†å›¾ç‰‡URLè½¬æ¢ä¸ºé«˜æ¸…ç‰ˆæœ¬

        Args:
            image_url: åŸå§‹å›¾ç‰‡URL

        Returns:
            str: é«˜æ¸…å›¾ç‰‡URL
        """
        try:
            import re
            # å°†wc50æˆ–wc100æ›¿æ¢ä¸ºwc1000
            high_res_url = re.sub(r'/wc\d+/', '/wc1000/', image_url)
            return high_res_url
        except Exception as e:
            self.logger.warning(f"è½¬æ¢é«˜æ¸…å›¾ç‰‡URLå¤±è´¥: {e}")
            return image_url

    def scrape_erp_info(self) -> ScrapingResult:
        """
        æŠ“å–ERPæ’ä»¶ä¿¡æ¯

        Returns:
            ScrapingResult: ERPæŠ“å–ç»“æœ
        """
        try:
            # ä½¿ç”¨å…±äº«çš„browser_serviceå®ä¾‹æŠ“å–ERPä¿¡æ¯
            return self.erp_scraper.scrape()

        except Exception as e:
            self.logger.error(f"æŠ“å–ERPä¿¡æ¯å¤±è´¥: {e}")
            return ScrapingResult(
                success=False,
                data={},
                error_message=str(e)
            )



    def scrape_competitor_stores(self, product_url: str, max_competitors: int = 10) -> ScrapingResult:
        """
        æŠ“å–è·Ÿå–åº—é“ºä¿¡æ¯ï¼ˆå‘åå…¼å®¹æ–¹æ³•ï¼‰

        Args:
            product_url: å•†å“URL
            max_competitors: æœ€å¤§è·Ÿå–æ•°é‡ï¼Œé»˜è®¤10

        Returns:
            ScrapingResult: è·Ÿå–åº—é“ºæŠ“å–ç»“æœ
        """
        start_time = time.time()

        try:
            self.logger.info(f"ğŸ” å¼€å§‹æŠ“å–è·Ÿå–åº—é“º: {product_url}")

            # æ„å»ºè·Ÿå–æ•°æ®ç»“æ„
            competitors_data = {
                'competitors': [],
                'total_count': 0,
                'target_url': product_url,
                'scraped_at': time.time()
            }

            # è¿”å›æˆåŠŸç»“æœï¼ˆå½“å‰è¿”å›ç©ºæ•°æ®ï¼Œä¿æŒæ¥å£å…¼å®¹æ€§ï¼‰
            return ScrapingResult(
                success=True,
                data=competitors_data,
                execution_time=time.time() - start_time
            )

        except Exception as e:
            self.logger.error(f"æŠ“å–è·Ÿå–åº—é“ºå¤±è´¥: {e}")
            return ScrapingResult(
                success=False,
                data={},
                error_message=str(e),
                execution_time=time.time() - start_time
            )

    # def combine_item(self, data, result_data):

    def _extract_product_id(self, url: str) -> Optional[str]:
        """
        ä»URLä¸­æå–å•†å“ID
        
        æ”¯æŒçš„URLæ ¼å¼:
        - https://www.ozon.ru/product/xxx-1234567/
        - https://www.ozon.ru/seller/xxx/product/1234567/
        
        Args:
            url: å•†å“URL
            
        Returns:
            Optional[str]: å•†å“IDï¼Œæå–å¤±è´¥è¿”å›None

        Raises:
            Exception: å½“URLä¸ºNoneæ—¶æŠ›å‡ºå¼‚å¸¸
        """
        # ç‰¹æ®Šå¤„ç†Noneè¾“å…¥
        if url is None:
            raise Exception("URLä¸èƒ½ä¸ºNone")

        try:
            import re
            
            # é¦–å…ˆéªŒè¯æ˜¯å¦ä¸ºOZONåŸŸå
            if not url or not re.search(r'https?://[^/]*ozon\.ru/', url):
                self.logger.debug(f"URLä¸æ˜¯OZONåŸŸå: {url}")
                return None

            # åŒ¹é… /product/xxx-æ•°å­—/ æˆ– /product/æ•°å­—/ æ ¼å¼ (å…¼å®¹æœ‰æ— æœ«å°¾æ–œæ )
            patterns = [
                r'/product/[^/]+-(\d+)',     # xxx-1234567 (å…¼å®¹æœ‰æ— æ–œæ )
                r'/product/(\d+)',            # 1234567 (å…¼å®¹æœ‰æ— æ–œæ )
            ]
            
            for pattern in patterns:
                match = re.search(pattern, url)
                if match:
                    product_id = match.group(1)
                    return product_id
            
            self.logger.debug(f"æ— æ³•ä»URLæå–å•†å“ID: {url}")
            return None
            
        except Exception as e:
            self.logger.error(f"æå–å•†å“IDå¤±è´¥: {e}")
            return None


