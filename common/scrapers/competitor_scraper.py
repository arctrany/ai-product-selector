"""
OZONè·Ÿå–åº—é“ºæŠ“å–å™¨

ä¸“é—¨å¤„ç†OZONå¹³å°è·Ÿå–åº—é“ºçš„äº¤äº’é€»è¾‘ï¼ŒåŒ…æ‹¬ï¼š
1. æ‰“å¼€è·Ÿå–æµ®å±‚
2. æå–è·Ÿå–åº—é“ºåˆ—è¡¨
3. ç‚¹å‡»è·Ÿå–åº—é“ºè·³è½¬
4. è·Ÿå–ä»·æ ¼è¯†åˆ«å’Œè¿‡æ»¤

é‡æ„ç‰ˆæœ¬ï¼šç®€åŒ–ä»£ç ç»“æ„ï¼Œæ¶ˆé™¤ç¡¬ç¼–ç ï¼Œæé«˜å¯ç»´æŠ¤æ€§
"""

import time
import logging
from typing import Dict, Any, List, Optional, Tuple
from bs4 import BeautifulSoup

# ğŸ”§ é‡æ„åçš„å¯¼å…¥ï¼šä½¿ç”¨æ–°çš„æ•°æ®æ¨¡å‹å’Œç»Ÿä¸€å·¥å…·ç±»
from common.utils.scraping_utils import clean_price_string
from common.models.scraping_result import ScrapingResult
from common.utils.wait_utils import WaitUtils, wait_for_content_smart
from common.utils.scraping_utils import ScrapingUtils
from .base_scraper import BaseScraper
from common.config.ozon_selectors_config import *


# å¼‚å¸¸ç±»å¯¼å…¥å·²ç§»é™¤ï¼Œä½¿ç”¨é€šç”¨å¼‚å¸¸å¤„ç†


class CompetitorScraper(BaseScraper):
    """
    OZONè·Ÿå–åº—é“ºæŠ“å–å™¨ - é‡æ„ç‰ˆæœ¬

    å®ç°ICompetitorScraperæ¥å£ï¼Œæä¾›æ ‡å‡†åŒ–çš„è·Ÿå–æ£€æµ‹å’ŒæŠ“å–åŠŸèƒ½
    
    ä¸“æ³¨äºè·Ÿå–åº—é“ºæ•°æ®æŠ“å–ï¼Œä½¿ç”¨ç»Ÿä¸€å·¥å…·ç±»
    """

    def __init__(self, selectors_config: Optional[OzonSelectorsConfig] = None,
                 browser_service=None):
        """
        åˆå§‹åŒ–è·Ÿå–æŠ“å–å™¨
        
        Args:
            selectors_config: é€‰æ‹©å™¨é…ç½®
            browser_service: æµè§ˆå™¨æœåŠ¡å®ä¾‹ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨å…¨å±€å•ä¾‹ï¼‰
        """
        from rpa.browser.browser_service import SimplifiedBrowserService
        from common.config.timeout_config import get_timing_config

        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        self.selectors_config = selectors_config or get_ozon_selectors_config()
        self.timing_config = get_timing_config()

        # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨ç°ä»£æµè§ˆå™¨æœåŠ¡APIï¼Œç¡®ä¿æµè§ˆå™¨å…¨å±€å”¯ä¸€æ€§
        if browser_service is None:
            self.browser_service = SimplifiedBrowserService.get_global_instance()
        else:
            self.browser_service = browser_service
        # ğŸ”§ é‡æ„ï¼šåˆå§‹åŒ–ç»Ÿä¸€å·¥å…·ç±»
        self.wait_utils = WaitUtils(self.browser_service, self.logger)
        self.scraping_utils = ScrapingUtils(self.logger)

    def _present_competitor_popup(self, expand: bool) -> Dict[str, Any]:
        """
        å¤„ç†ç«å“å¼¹çª—çš„å®Œæ•´æµç¨‹
        
        1. ç‚¹å‡»ç«å“å®¹å™¨åŒºåŸŸå¼¹å‡ºå¼¹çª—
        2. ç­‰å¾…å¼¹çª—åŠ è½½å®Œæˆ
        3. å¦‚æœéœ€è¦ï¼Œå±•å¼€æ›´å¤šç«å“ä¿¡æ¯
        4. è¿”å›å¼¹çª—å®¹å™¨å’Œç›¸å…³ä¿¡æ¯
        
        Args:
            expand: æ˜¯å¦éœ€è¦å±•å¼€æ›´å¤šç«å“
            
        Returns:
            DictåŒ…å«: success, popup_container, expandedç­‰ä¿¡æ¯
        """
        try:
            self.logger.info("ğŸ” å¼€å§‹å¤„ç†ç«å“å®¹å™¨ç‚¹å‡»å’Œå¼¹çª—åŠ è½½...")

            # è·å–æµè§ˆå™¨é¡µé¢å®ä¾‹
            page = self.browser_service.get_page()
            if not page:
                self.logger.error("âŒ æ— æ³•è·å–æµè§ˆå™¨é¡µé¢å®ä¾‹")
                return {"success": False, "error": "æµè§ˆå™¨é¡µé¢ä¸å¯ç”¨"}

            # æç®€åŒ–ï¼šç‚¹å‡»ä»»æ„ç«å“å®¹å™¨åŒºåŸŸå°±ä¼šå¼¹å‡ºpop_layer
            self.logger.info("ğŸ¯ ç‚¹å‡»ç«å“å®¹å™¨å¼¹å‡ºpop_layer...")

            # ä½¿ç”¨é…ç½®åŒ–çš„é€‰æ‹©å™¨ç­–ç•¥ï¼ŒæŒ‰ä¼˜å…ˆçº§å°è¯•ç‚¹å‡»
            click_selectors = self.selectors_config.competitor_area_click_selectors
            clicked = False

            for i, selector in enumerate(click_selectors):
                try:
                    self.logger.info(f"ğŸ¯ å°è¯•ç‚¹å‡»é€‰æ‹©å™¨ {i+1}/{len(click_selectors)}: {selector}")

                    # å…ˆæ£€æŸ¥å…ƒç´ æ˜¯å¦å­˜åœ¨ï¼ˆä½¿ç”¨åŒæ­¥æ–¹æ³•ï¼‰
                    try:
                        check_timeout = self.timing_config.timeout.element_wait_timeout_ms
                        if selector.startswith("//"):
                            element_exists = self.browser_service.query_selector_sync(f"xpath={selector}", timeout=check_timeout) is not None
                        else:
                            element_exists = self.browser_service.query_selector_sync(selector, timeout=check_timeout) is not None
                        
                        if not element_exists:
                            self.logger.debug(f"â­ï¸  é€‰æ‹©å™¨ {selector} å¯¹åº”çš„å…ƒç´ ä¸å­˜åœ¨ï¼Œè·³è¿‡")
                            continue
                    except TimeoutError:
                        self.logger.debug(f"â­ï¸  é€‰æ‹©å™¨ {selector} è¶…æ—¶ï¼Œå…ƒç´ ä¸å­˜åœ¨")
                        continue
                    except Exception as check_e:
                        self.logger.debug(f"â­ï¸  æ£€æŸ¥é€‰æ‹©å™¨ {selector} æ—¶å‡ºé”™: {check_e}")
                        continue

                    # åˆ¤æ–­é€‰æ‹©å™¨ç±»å‹å¹¶ç›¸åº”å¤„ç†
                    click_timeout = self.timing_config.timeout.get_timeout_ms('element_wait') * 3
                    if selector.startswith("//"):
                        # XPathé€‰æ‹©å™¨
                        self.browser_service.click_sync(f"xpath={selector}", timeout=click_timeout)
                    else:
                        # CSSé€‰æ‹©å™¨
                        self.browser_service.click_sync(selector, timeout=click_timeout)

                    self.logger.info(f"âœ… æˆåŠŸç‚¹å‡»ç«å“åŒºåŸŸï¼Œä½¿ç”¨é€‰æ‹©å™¨: {selector}")
                    clicked = True
                    break
                except TimeoutError:
                    self.logger.debug(f"â­ï¸  é€‰æ‹©å™¨ {selector} è¶…æ—¶")
                    continue
                except Exception as e:
                    self.logger.warning(f"âš ï¸ é€‰æ‹©å™¨ {selector} ç‚¹å‡»å¤±è´¥: {str(e)}")
                    continue

            if not clicked:
                self.logger.warning("âš ï¸ æœªæ‰¾åˆ°å¯ç‚¹å‡»çš„ç«å“å®¹å™¨ï¼Œè¯¥å•†å“å¯èƒ½æ²¡æœ‰è·Ÿå–ä¿¡æ¯")
                return {
                    "success": False,
                    "error": "no_competitors",
                    "popup_container": None,
                    "expanded": False
                }

            # ç­‰å¾…å¼¹çª—åŠ è½½
            self.logger.info("â³ ç­‰å¾…ç«å“å¼¹çª—åŠ è½½...")

            wait_for_content_smart(self.selectors_config.competitor_popup_selectors, 
                                  browser_service=self.browser_service)

            # å¦‚æœéœ€è¦å±•å¼€æ›´å¤šå†…å®¹
            if expand:
                self.logger.info("ğŸ”„ å¼€å§‹å±•å¼€æ›´å¤šç«å“ä¿¡æ¯...")
                expand_success = self._expand_competitor_list()
                if expand_success:
                    self.logger.info("âœ… æˆåŠŸå±•å¼€æ›´å¤šç«å“")
                    # å±•å¼€åéœ€è¦æ›´é•¿æ—¶é—´ç­‰å¾…æ–°å†…å®¹åŠ è½½
                    self.logger.info("â³ ç­‰å¾…å±•å¼€åçš„å†…å®¹åŠ è½½...")
                    self.wait_utils.smart_wait(5.0)
                else:
                    self.logger.warning("âš ï¸ å±•å¼€æ“ä½œå¤±è´¥æˆ–æ— éœ€å±•å¼€")

            # è·å–æœ€ç»ˆçš„é¡µé¢å†…å®¹
            try:
                # ä½¿ç”¨åŒæ­¥APIè·å–é¡µé¢å†…å®¹
                content_timeout = self.timing_config.timeout.get_timeout_s('data_extraction')
                page_content = self.browser_service.get_page_content_sync(timeout=content_timeout)
                if not page_content:
                    self.logger.error("âŒ è·å–é¡µé¢å†…å®¹å¤±è´¥")
                    return {"success": False, "error": "è·å–é¡µé¢å†…å®¹å¤±è´¥"}
                
                popup_soup = BeautifulSoup(page_content, 'html.parser')

                # æŸ¥æ‰¾å¼¹çª—å®¹å™¨
                popup_container = None
                for selector in self.selectors_config.competitor_popup_selectors:
                    popup_container = popup_soup.select_one(selector)
                    if popup_container:
                        self.logger.info(f"âœ… æ‰¾åˆ°å¼¹çª—å®¹å™¨: {selector}")
                        break

                result = {
                    "success": True,
                    "popup_container": popup_container,
                    "expanded": expand
                }

                self.logger.info("ğŸ‰ ç«å“å®¹å™¨ç‚¹å‡»å’Œå¼¹çª—åŠ è½½å®Œæˆ")
                return result

            except TimeoutError:
                self.logger.error(f"âŒ è·å–é¡µé¢å†…å®¹è¶…æ—¶")
                return {
                    "success": False,
                    "error": "è·å–é¡µé¢å†…å®¹è¶…æ—¶",
                    "popup_container": None,
                    "expanded": False
                }
            except Exception as content_error:
                self.logger.error(f"âŒ è·å–é¡µé¢å†…å®¹å¤±è´¥: {content_error}")
                return {
                    "success": False,
                    "error": f"è·å–å†…å®¹å¤±è´¥: {content_error}",
                    "popup_container": None,
                    "expanded": False
                }

        except TimeoutError as e:
            self.logger.error(f"âŒ ç«å“å¼¹çª—å¤„ç†è¶…æ—¶: {e}")
            return {
                "success": False,
                "error": f"æ“ä½œè¶…æ—¶: {e}",
                "popup_container": None,
                "expanded": False
            }
        except Exception as e:
            self.logger.error(f"âŒ ç«å“å¼¹çª—å¤„ç†å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "popup_container": None,
                "expanded": False
            }

    def _find_element_by_selectors(self, selectors: List[str], timeout: Optional[int] = None) -> Optional[Any]:
        """
        é€šç”¨çš„é€‰æ‹©å™¨æŸ¥æ‰¾æ–¹æ³•
        
        Args:
            selectors: é€‰æ‹©å™¨åˆ—è¡¨
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é…ç½®çš„é»˜è®¤å€¼
            
        Returns:
            æ‰¾åˆ°çš„å…ƒç´ æˆ–None
        """
        if timeout is None:
            timeout = self.timing_config.timeout.element_wait_timeout_ms
        
        for selector in selectors:
            try:
                element = self.browser_service.query_selector_sync(selector, timeout=timeout)
                if element:
                    return element
            except (TimeoutError, Exception) as e:
                self.logger.debug(f"é€‰æ‹©å™¨ {selector} æŸ¥æ‰¾å¤±è´¥: {e.__class__.__name__}")
                continue
        return None

    def _expand_competitor_list(self) -> bool:
        """åœ¨pop_layerä¸­ç‚¹å‡»å±•å¼€æŒ‰é’®ï¼Œå±•ç¤ºæ›´å¤šç«å“ä¿¡æ¯"""
        try:
            page = self.browser_service.get_page()
            if not page:
                self.logger.warning("âš ï¸ æ— æ³•è·å–é¡µé¢å¯¹è±¡ï¼Œå±•å¼€æ“ä½œå¤±è´¥")
                return False

            self.logger.info("ğŸ” å¼€å§‹æŸ¥æ‰¾å±•å¼€æŒ‰é’®...")
            click_timeout = self.timing_config.timeout.get_timeout_ms('element_wait') * 3
            
            # å°è¯•æ‰€æœ‰å±•å¼€é€‰æ‹©å™¨
            for i, selector in enumerate(self.selectors_config.expand_selectors):
                try:
                    self.logger.debug(f"ğŸ¯ å°è¯•å±•å¼€é€‰æ‹©å™¨ {i+1}/{len(self.selectors_config.expand_selectors)}: {selector}")
                    
                    # å…ˆæ£€æŸ¥å…ƒç´ æ˜¯å¦å­˜åœ¨
                    element = self.browser_service.query_selector_sync(selector, timeout=1000)
                    if not element:
                        self.logger.debug(f"â­ï¸  é€‰æ‹©å™¨ {selector} æœªæ‰¾åˆ°å…ƒç´ ")
                        continue
                    
                    self.logger.info(f"âœ… æ‰¾åˆ°å±•å¼€æŒ‰é’®: {selector}")
                    
                    # ç‚¹å‡»å±•å¼€æŒ‰é’®
                    self.browser_service.click_sync(selector, timeout=click_timeout)
                    self.logger.info(f"ğŸ‰ æˆåŠŸç‚¹å‡»å±•å¼€æŒ‰é’®")
                    
                    # ç­‰å¾…å±•å¼€å†…å®¹åŠ è½½
                    wait_time = self.timing_config.timeout.short_wait_s
                    self.logger.info(f"â³ ç­‰å¾… {wait_time}s åŠ è½½å±•å¼€å†…å®¹...")
                    self.wait_utils.smart_wait(wait_time)
                    
                    return True
                    
                except TimeoutError:
                    self.logger.debug(f"â­ï¸  é€‰æ‹©å™¨ {selector} è¶…æ—¶")
                    continue
                except Exception as e:
                    self.logger.debug(f"â­ï¸  é€‰æ‹©å™¨ {selector} å¤±è´¥: {e.__class__.__name__}")
                    continue

            # æ‰¾ä¸åˆ°å±•å¼€æŒ‰é’®
            self.logger.info("â„¹ï¸  æœªæ‰¾åˆ°å±•å¼€æŒ‰é’®ï¼Œå¯èƒ½å·²å…¨éƒ¨æ˜¾ç¤ºæˆ–æ— éœ€å±•å¼€")
            return True  # è¿”å›Trueï¼Œå› ä¸ºå¯èƒ½å·²ç»å…¨éƒ¨æ˜¾ç¤º
            
        except TimeoutError:
            self.logger.warning("âš ï¸ å±•å¼€æ“ä½œè¶…æ—¶")
            return False
        except Exception as e:
            self.logger.warning(f"âš ï¸ å±•å¼€æ“ä½œå¤±è´¥: {e.__class__.__name__}: {e}")
            return False

    def extract_competitors_from_content(self, full_pop_layer, max_competitors: int = 10) -> List[
        Dict[str, Any]]:
        """ä»pop_layerå–åº—é“ºä¿¡æ¯,åŒ…æ‹¬åº—é“ºIDï¼Œå•†å“ID"""
        try:
            self.logger.info("ğŸ” æå–è·Ÿå–åº—é“ºä¿¡æ¯...")
            if not full_pop_layer:
                self.logger.warning("âš ï¸ æœªæ‰¾åˆ°è·Ÿå–åº—é“ºå®¹å™¨")
                return []

            # æŸ¥æ‰¾åº—é“ºå…ƒç´ 
            elements, selector = self._find_competitor_elements_in_soup(full_pop_layer)
            if not elements:
                self.logger.warning("âš ï¸ æœªæ‰¾åˆ°è·Ÿå–åº—é“ºå…ƒç´ ")
                return []

            self.logger.info(f"ğŸ¯ æ‰¾åˆ° {len(elements)} ä¸ªè·Ÿå–åº—é“ºå…ƒç´  (é€‰æ‹©å™¨: {selector})")

            # æå–åº—é“ºä¿¡æ¯
            competitors = []
            for i, element in enumerate(elements[:max_competitors]):
                try:
                    competitor_data = self._extract_competitor_from_element(element, i + 1)
                    if competitor_data:
                        competitors.append(competitor_data)
                        # ä»é…ç½®è·å–è´§å¸ç¬¦å·ï¼Œé¿å…ç¡¬ç¼–ç 
                        currency_symbol = getattr(self.selectors_config, 'currency_symbol', "â‚½")
                        self.logger.info(
                            f"âœ… æå–åº—é“º{i + 1}: {competitor_data.get('store_name', 'N/A')} - {competitor_data.get('price', 'N/A')}{currency_symbol}")
                except Exception as e:
                    self.logger.warning(f"æå–ç¬¬{i + 1}ä¸ªåº—é“ºå¤±è´¥: {e.__class__.__name__}: {e}")
                    continue

            self.logger.info(f"ğŸ‰ æˆåŠŸæå–{len(competitors)}ä¸ªè·Ÿå–åº—é“º")
            return competitors

        except Exception as e:
            self.logger.error(f"æå–è·Ÿå–åº—é“ºå¤±è´¥: {e.__class__.__name__}: {e}")
            return []

    def _find_competitor_elements_in_soup(self, container) -> Tuple[List, Optional[str]]:
        """
          åœ¨å®¹å™¨ä¸­æŸ¥æ‰¾è·Ÿå–åº—é“ºå…ƒç´ 
        """

        best_elements = []
        best_selector = None

        for selector in self.selectors_config.competitor_element_selectors:
            try:
                elements = container.select(selector)
                if elements and len(elements) > len(best_elements):
                    best_elements = elements
                    best_selector = selector
                    self.logger.debug(f"âœ… ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(elements)} ä¸ªè·Ÿå–åº—é“ºå…ƒç´ ")
                    # ğŸ”§ å…³é”®ä¿®å¤ï¼šç»§ç»­å°è¯•å…¶ä»–é€‰æ‹©å™¨çœ‹æ˜¯å¦èƒ½æ‰¾åˆ°æ›´å¤š
            except Exception as e:
                self.logger.debug(f"é€‰æ‹©å™¨ '{selector}' å¤±è´¥: {e.__class__.__name__}")
                continue

        return best_elements, best_selector

    def _extract_competitor_from_element(self, element, ranking: int) -> Optional[Dict[str, Any]]:
        """ä»å…ƒç´ ä¸­æå–è·Ÿå–åº—é“ºä¿¡æ¯ - ğŸ”§ ä¿®å¤ï¼šæ¢å¤å®Œæ•´çš„æå–é€»è¾‘ï¼Œç¡®ä¿èƒ½æå–å¤šä¸ªåº—é“º"""
        try:
            self.logger.debug(f"ğŸ” å¼€å§‹æå–ç¬¬{ranking}ä¸ªè·Ÿå–åº—é“ºä¿¡æ¯...")
            competitor_data = {'ranking': ranking}

            # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨é…ç½®çš„åº—é“ºåç§°é€‰æ‹©å™¨ï¼ŒåŒ…å«å›é€€é€»è¾‘
            name_selectors = self.selectors_config.store_name_selectors
            store_name = None

            for selector in name_selectors:
                try:
                    name_element = element.select_one(selector)
                    if name_element:
                        store_name = name_element.get_text(strip=True)
                        if store_name and len(store_name) > 0:
                            competitor_data['store_name'] = store_name
                            self.logger.debug(f"âœ… æå–åˆ°åº—é“ºåç§°: {store_name}")
                            break
                except:
                    continue

            # ğŸ”§ ä¿®å¤ï¼šå¦‚æœä»æœªæ‰¾åˆ°åº—é“ºåç§°ï¼Œå°è¯•æŸ¥æ‰¾æ‰€æœ‰åŒ…å«æ–‡æœ¬çš„å…ƒç´ 
            if 'store_name' not in competitor_data:
                try:
                    text_elements = element.find_all(text=True)
                    for text in text_elements:
                        stripped_text = text.strip()
                        if (stripped_text and
                                len(stripped_text) > 1 and
                                'â‚½' not in stripped_text and
                                not stripped_text.replace('.', '').replace(',', '').isdigit()):
                            competitor_data['store_name'] = stripped_text
                            self.logger.debug(f"âœ… é€šè¿‡æ–‡æœ¬æŸ¥æ‰¾æå–åˆ°åº—é“ºåç§°: {stripped_text}")
                            break
                except:
                    pass

            # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨é…ç½®çš„ä»·æ ¼é€‰æ‹©å™¨ï¼ŒåŒ…å«å›é€€é€»è¾‘
            price_selectors = self.selectors_config.store_price_selectors
            price = None

            for selector in price_selectors:
                try:
                    price_element = element.select_one(selector)
                    if price_element:
                        price_text = price_element.get_text(strip=True)
                        self.logger.debug(f"ğŸ” å°è¯•è§£æä»·æ ¼æ–‡æœ¬: '{price_text}'")
                        price = clean_price_string(price_text, self.selectors_config)
                        if price and price > 0:
                            competitor_data['price'] = price
                            self.logger.debug(f"âœ… æå–åˆ°åº—é“ºä»·æ ¼: {price}â‚½")
                            break
                except:
                    continue

            # ğŸ”§ ä¿®å¤ï¼šå¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»·æ ¼ï¼Œå°è¯•æŸ¥æ‰¾åŒ…å«â‚½ç¬¦å·çš„æ–‡æœ¬
            if not price:
                try:
                    price_elements = element.find_all(text=lambda text: text and 'â‚½' in text)
                    for price_text in price_elements:
                        price = clean_price_string(str(price_text), self.selectors_config)
                        if price and price > 0:
                            competitor_data['price'] = price
                            self.logger.debug(f"âœ… é€šè¿‡æ–‡æœ¬æŸ¥æ‰¾æå–åˆ°åº—é“ºä»·æ ¼: {price}â‚½")
                            break
                except:
                    pass

            # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨é…ç½®çš„é“¾æ¥é€‰æ‹©å™¨
            link_element = None
            link_selectors = self.selectors_config.store_link_selectors

            for selector in link_selectors:
                try:
                    link_element = element.select_one(selector)
                    if link_element and link_element.get('href'):
                        href = link_element.get('href')
                        if href and len(href) > 0:
                            self.logger.debug(f"ğŸ” æ‰¾åˆ°åº—é“ºé“¾æ¥: {href}")
                            break
                    link_element = None
                except:
                    continue

            if link_element and link_element.get('href'):
                href = link_element.get('href')
                store_id = self._extract_store_id_from_url(href)
                if store_id:
                    competitor_data['store_id'] = store_id
                    self.logger.debug(f"âœ… æå–åˆ°åº—é“ºID: {store_id}")
                else:
                    competitor_data['store_id'] = f"store_{ranking}"
                    self.logger.debug(f"âš ï¸ æœªæ‰¾åˆ°åº—é“ºIDï¼Œä½¿ç”¨é»˜è®¤ID: store_{ranking}")
            else:
                competitor_data['store_id'] = f"store_{ranking}"
                self.logger.debug(f"âš ï¸ æœªæ‰¾åˆ°åº—é“ºé“¾æ¥ï¼Œä½¿ç”¨é»˜è®¤ID: store_{ranking}")

            # ğŸ”§ ä¿®å¤ï¼šå¦‚æœæ²¡æœ‰æå–åˆ°åº—é“ºåç§°ï¼Œä½¿ç”¨é»˜è®¤åç§°
            if 'store_name' not in competitor_data or not competitor_data['store_name']:
                competitor_data['store_name'] = f"åº—é“º{ranking}"
                self.logger.debug(f"âš ï¸ æœªæå–åˆ°åº—é“ºåç§°ï¼Œä½¿ç”¨é»˜è®¤åç§°: {competitor_data['store_name']}")

            # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ”¾å®½éªŒè¯æ¡ä»¶ï¼Œåªè¦æœ‰åŸºæœ¬ä¿¡æ¯å°±è¿”å›ï¼Œé¿å…è¿‡æ»¤æ‰åº—é“º
            if competitor_data.get('store_name') or competitor_data.get('price') or competitor_data.get('store_id'):
                self.logger.debug(f"âœ… ç¬¬{ranking}ä¸ªè·Ÿå–åº—é“ºä¿¡æ¯æå–å®Œæˆ: {competitor_data}")
                return competitor_data
            else:
                self.logger.warning(f"âš ï¸ ç¬¬{ranking}ä¸ªè·Ÿå–åº—é“ºä¿¡æ¯å®Œå…¨ä¸ºç©ºï¼Œè·³è¿‡")
                return None

        except Exception as e:
            self.logger.warning(f"ä»å…ƒç´ æå–è·Ÿå–åº—é“ºä¿¡æ¯å¤±è´¥: {e.__class__.__name__}: {e}")
            # ğŸ”§ ä¿®å¤ï¼šå³ä½¿å‡ºé”™ä¹Ÿè¿”å›åŸºæœ¬ä¿¡æ¯ï¼Œé¿å…å®Œå…¨ä¸¢å¤±åº—é“º
            return {
                'ranking': ranking,
                'store_name': f"åº—é“º{ranking}",
                'store_id': f"store_{ranking}",
                'price': None
            }



    def _extract_store_id_from_url(self, href: str) -> Optional[str]:
        """
        ä»URLä¸­æå–åº—é“ºID

        æ³¨æ„ï¼šæ­¤æ–¹æ³•å·²é‡æ„ä¸ºå§”æ‰˜ç»™ scraping_utils ä¸­çš„é€šç”¨æ–¹æ³•ï¼Œ
        å»ºè®®ç›´æ¥ä½¿ç”¨ self.scraping_utils.extract_store_id_from_url()
        """
        return self.scraping_utils.extract_store_id_from_url(href)

    

    def _get_first_competitor_product(self, popup_container, ranking: int = 1) -> Optional[Dict[str, Any]]:
        """
        è·å–æŒ‡å®šæ’ååº—é“ºçš„å•†å“ID
        
        å®ç°ç­–ç•¥ï¼š
        1. ä¼˜å…ˆä»DOMä¸­æå–å•†å“é“¾æ¥ï¼ˆå¿«é€Ÿï¼‰
        2. å¦‚æœæ‰¾ä¸åˆ°ï¼Œåˆ™ç‚¹å‡»è·³è½¬æå–ï¼ˆæ…¢é€Ÿï¼‰
        
        Args:
            popup_container: BeautifulSoupè§£æçš„å¼¹çª—å®¹å™¨
            ranking: åº—é“ºæ’åï¼Œé»˜è®¤1ï¼ˆç¬¬ä¸€ä¸ªåº—é“ºï¼‰
            
        Returns:
            DictåŒ…å«: success, product_id, product_url, methodç­‰ä¿¡æ¯
        """
        try:
            self.logger.info(f"ğŸ¯ å¼€å§‹è·å–æ’å{ranking}çš„åº—é“ºå•†å“ID...")
            
            # 1. æŸ¥æ‰¾æŒ‡å®šæ’åçš„åº—é“ºå…ƒç´ 
            elements, selector = self._find_competitor_elements_in_soup(popup_container)
            if not elements or len(elements) < ranking:
                self.logger.warning(f"âš ï¸ æœªæ‰¾åˆ°æ’å{ranking}çš„åº—é“ºå…ƒç´ ")
                return {
                    "success": False,
                    "error": f"åº—é“ºå…ƒç´ ä¸è¶³ï¼Œå½“å‰åªæœ‰{len(elements)}ä¸ª",
                    "product_id": None
                }
            
            target_element = elements[ranking - 1]
            self.logger.info(f"âœ… æ‰¾åˆ°æ’å{ranking}çš„åº—é“ºå…ƒç´ ")
            
            # 2. ç­–ç•¥Aï¼šå°è¯•ä»DOMä¸­æå–å•†å“é“¾æ¥
            product_info = self._extract_product_link_from_element(target_element, ranking)
            if product_info and product_info.get("product_id"):
                self.logger.info(f"âœ… é€šè¿‡DOMæå–åˆ°å•†å“ID: {product_info['product_id']}")
                return {
                    "success": True,
                    "product_id": product_info["product_id"],
                    "product_url": product_info.get("product_url"),
                    "method": "dom_extraction",
                    "ranking": ranking
                }
            
            # 3. ç­–ç•¥Bï¼šç‚¹å‡»è·³è½¬æå–ï¼ˆå¦‚æœDOMæå–å¤±è´¥ï¼‰
            self.logger.info("âš ï¸ DOMä¸­æœªæ‰¾åˆ°å•†å“é“¾æ¥ï¼Œå°è¯•ç‚¹å‡»è·³è½¬...")
            product_info = self._click_and_extract_product_id(target_element, ranking)
            if product_info and product_info.get("product_id"):
                self.logger.info(f"âœ… é€šè¿‡ç‚¹å‡»è·³è½¬æå–åˆ°å•†å“ID: {product_info['product_id']}")
                return {
                    "success": True,
                    "product_id": product_info["product_id"],
                    "product_url": product_info.get("product_url"),
                    "method": "click_navigation",
                    "ranking": ranking
                }
            
            # 4. ä¸¤ç§ç­–ç•¥éƒ½å¤±è´¥
            self.logger.error(f"âŒ æ— æ³•è·å–æ’å{ranking}åº—é“ºçš„å•†å“ID")
            return {
                "success": False,
                "error": "æ‰€æœ‰æå–ç­–ç•¥éƒ½å¤±è´¥",
                "product_id": None
            }
            
        except Exception as e:
            self.logger.error(f"âŒ è·å–å•†å“IDå¤±è´¥: {e.__class__.__name__}: {e}")
            return {
                "success": False,
                "error": str(e),
                "product_id": None
            }
    
    def _extract_product_link_from_element(self, element, ranking: int) -> Optional[Dict[str, Any]]:
        """
        ä»åº—é“ºå…ƒç´ ä¸­æå–å•†å“é“¾æ¥å’ŒIDï¼ˆDOMæ–¹æ³•ï¼‰
        
        æŸ¥æ‰¾ç­–ç•¥ï¼š
        1. æŸ¥æ‰¾åŒ…å«/product/çš„é“¾æ¥
        2. æ’é™¤åº—é“ºé“¾æ¥(/seller/)
        3. å¤ç”¨å·¥å…·ç±»ä»URLä¸­æå–å•†å“ID
        """
        try:
            # æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
            all_links = element.find_all('a', href=True)
            
            for link in all_links:
                href = link.get('href', '')
                
                # è·³è¿‡åº—é“ºé“¾æ¥
                if '/seller/' in href:
                    continue
                
                # æŸ¥æ‰¾å•†å“é“¾æ¥
                if '/product/' in href:
                    self.logger.debug(f"ğŸ” æ‰¾åˆ°å•†å“é“¾æ¥: {href}")
                    
                    # ğŸ”§ å…³é”®ä¿®å¤ï¼šå¤ç”¨å·¥å…·ç±»æå–å•†å“ID
                    product_id = self.scraping_utils.extract_product_id_from_url(href)
                    if product_id:
                        return {
                            "product_id": product_id,
                            "product_url": href if href.startswith('http') else f"https://www.ozon.ru{href}"
                        }
            
            self.logger.debug(f"âš ï¸ æ’å{ranking}çš„åº—é“ºå…ƒç´ ä¸­æœªæ‰¾åˆ°å•†å“é“¾æ¥")
            return None
            
        except Exception as e:
            self.logger.debug(f"ä»DOMæå–å•†å“é“¾æ¥å¤±è´¥: {e}")
            return None
    
    
    
    def _click_and_extract_product_id(self, element, ranking: int) -> Optional[Dict[str, Any]]:
        """
        é€šè¿‡ç‚¹å‡»åº—é“ºå…ƒç´ è·³è½¬å¹¶æå–å•†å“IDï¼ˆç‚¹å‡»æ–¹æ³•ï¼‰
        
        å®ç°ç­–ç•¥ï¼š
        1. åœ¨å¼¹çª—ä¸­å®šä½åˆ°æŒ‡å®šæ’åçš„åº—é“ºå…ƒç´ 
        2. ç‚¹å‡»ä»·æ ¼åŒºåŸŸï¼ˆdiv.pdp_b3kï¼‰æˆ–æ•´ä¸ªåº—é“ºè¡Œ
        3. ç­‰å¾…é¡µé¢è·³è½¬åˆ°å•†å“è¯¦æƒ…é¡µ
        4. ä»æ–°é¡µé¢URLæå–å•†å“ID
        5. è¿”å›å•†å“IDå’ŒURL
        
        Args:
            element: BeautifulSoupåº—é“ºå…ƒç´ ï¼ˆç”¨äºç¡®è®¤æ’åï¼‰
            ranking: åº—é“ºæ’å
            
        Returns:
            DictåŒ…å«product_idå’Œproduct_urlï¼Œå¤±è´¥è¿”å›None
        """
        try:
            start_time = time.time()
            self.logger.info(f"ğŸ–±ï¸ å¼€å§‹ç‚¹å‡»è·³è½¬æå–æ’å{ranking}çš„å•†å“ID...")
            
            # è·å–å½“å‰é¡µé¢URLä½œä¸ºåŸºå‡†
            page = self.browser_service.get_page()
            if not page:
                self.logger.error("âŒ æ— æ³•è·å–é¡µé¢å¯¹è±¡")
                return None
            
            original_url = page.url
            self.logger.debug(f"ğŸ“ å½“å‰é¡µé¢URL: {original_url}")
            
            # æ„å»ºç‚¹å‡»é€‰æ‹©å™¨ï¼šå®šä½åˆ°å¼¹çª—ä¸­ç¬¬Nä¸ªåº—é“ºçš„ä»·æ ¼åŒºåŸŸ
            # ä½¿ç”¨CSSé€‰æ‹©å™¨å®šä½ï¼š#seller-listä¸­çš„ç¬¬Nä¸ªåº—é“ºé¡¹çš„ä»·æ ¼åŒºåŸŸ
            click_selectors = [
                f"#seller-list > div > div:nth-child({ranking}) div.pdp_b3k",  # ä»·æ ¼åŒºåŸŸ
                f"#seller-list > div > div:nth-child({ranking}) div.pdp_b2k.pdp_b3k",  # å®Œæ•´ä»·æ ¼åŒºåŸŸè·¯å¾„
                f"#seller-list > div > div:nth-child({ranking})",  # æ•´ä¸ªåº—é“ºè¡Œï¼ˆåå¤‡æ–¹æ¡ˆï¼‰
            ]
            
            clicked = False
            for selector in click_selectors:
                try:
                    self.logger.debug(f"ğŸ¯ å°è¯•ç‚¹å‡»é€‰æ‹©å™¨: {selector}")
                    
                    # æ£€æŸ¥å…ƒç´ æ˜¯å¦å­˜åœ¨
                    element_exists = self.browser_service.query_selector_sync(
                        selector, 
                        timeout=self.timing_config.timeout.element_wait_timeout_ms
                    )
                    
                    if not element_exists:
                        self.logger.debug(f"â­ï¸  é€‰æ‹©å™¨ {selector} å…ƒç´ ä¸å­˜åœ¨")
                        continue
                    
                    # ç‚¹å‡»å…ƒç´ 
                    click_timeout = self.timing_config.timeout.get_timeout_ms('element_wait') * 3
                    self.browser_service.click_sync(selector, timeout=click_timeout)
                    self.logger.info(f"âœ… æˆåŠŸç‚¹å‡»å…ƒç´ : {selector}")
                    clicked = True
                    break
                    
                except TimeoutError:
                    self.logger.debug(f"â­ï¸  é€‰æ‹©å™¨ {selector} è¶…æ—¶")
                    continue
                except Exception as e:
                    self.logger.debug(f"â­ï¸  é€‰æ‹©å™¨ {selector} å¤±è´¥: {e.__class__.__name__}")
                    continue
            
            if not clicked:
                self.logger.warning(f"âš ï¸ æ— æ³•ç‚¹å‡»æ’å{ranking}çš„åº—é“ºå…ƒç´ ")
                return None
            
            # ç­‰å¾…é¡µé¢è·³è½¬
            self.logger.info("â³ ç­‰å¾…é¡µé¢è·³è½¬...")
            max_wait_time = 10  # æœ€å¤šç­‰å¾…10ç§’
            wait_interval = 0.5
            elapsed_time = 0
            
            while elapsed_time < max_wait_time:
                current_url = page.url
                if current_url != original_url and '/product/' in current_url:
                    self.logger.info(f"âœ… é¡µé¢å·²è·³è½¬: {current_url}")
                    
                    # ä»æ–°é¡µé¢URLæå–å•†å“ID
                    product_id = self.scraping_utils.extract_product_id_from_url(current_url)
                    if product_id:
                        execution_time = time.time() - start_time
                        self.logger.info(f"ğŸ‰ æˆåŠŸæå–å•†å“ID: {product_id} (è€—æ—¶: {execution_time:.2f}s)")
                        return {
                            "product_id": product_id,
                            "product_url": current_url
                        }
                    else:
                        self.logger.warning(f"âš ï¸ é¡µé¢å·²è·³è½¬ä½†æ— æ³•æå–å•†å“ID: {current_url}")
                        return None
                
                time.sleep(wait_interval)
                elapsed_time += wait_interval
            
            # è¶…æ—¶
            self.logger.error(f"âŒ é¡µé¢è·³è½¬è¶…æ—¶({max_wait_time}s)")
            return None
            
        except TimeoutError:
            self.logger.error(f"âŒ ç‚¹å‡»è·³è½¬è¶…æ—¶")
            return None
        except Exception as e:
            self.logger.error(f"âŒ ç‚¹å‡»è·³è½¬æå–å¤±è´¥: {e.__class__.__name__}: {e}")
            return None

    # æ ‡å‡†scrapeæ¥å£å®ç°
    def scrape(self,
               url: str,
               context: Optional[Dict[str, Any]] = None,
               **kwargs) -> ScrapingResult:
        """
        ç»Ÿä¸€çš„æŠ“å–æ¥å£ï¼ˆæ ‡å‡†æ¥å£å®ç°ï¼‰

        Args:
            target: æŠ“å–ç›®æ ‡ï¼ˆå•†å“URLï¼‰
            mode: æŠ“å–æ¨¡å¼
            options: æŠ“å–é€‰é¡¹é…ç½®
            **kwargs: é¢å¤–å‚æ•°

        Returns:
            ScrapingResult: æ ‡å‡†åŒ–æŠ“å–ç»“æœ

        Raises:
            ScrapingException: æŠ“å–å¼‚å¸¸
            :param url:
            :param target:
            :param mode:
            :param context:
        """

        # ğŸ”§ å…³é”®ä¿®å¤ï¼šåœ¨ä»»ä½•æŠ“å–æ“ä½œå‰ç¡®ä¿æµè§ˆå™¨å·²æ­£ç¡®å¯åŠ¨
        try:
            self.logger.info("ğŸŒ å‡†å¤‡å¼€å§‹æŠ“å–ï¼Œé¦–å…ˆç¡®ä¿æµè§ˆå™¨å·²å¯åŠ¨...")
            # self._ensure_browser_initialized()

            # ğŸ”§ å…³é”®ä¿®å¤ï¼šå¯¼èˆªåˆ°ç›®æ ‡é¡µé¢
            self.logger.info(f"ğŸ¯ å¯¼èˆªåˆ°ç›®æ ‡é¡µé¢: {url}")
            nav_success = self.browser_service.navigate_to_sync(url, wait_until="domcontentloaded")
            if not nav_success:
                return ScrapingResult(
                    success=False,
                    data={},
                    error_message=f"æ— æ³•å¯¼èˆªåˆ°ç›®æ ‡é¡µé¢: {url}",
                    execution_time=0
                )
            self.logger.info("âœ… é¡µé¢å¯¼èˆªæˆåŠŸ")

        except TimeoutError as e:
            self.logger.error(f"âŒ æµè§ˆå™¨å¯åŠ¨æˆ–é¡µé¢å¯¼èˆªè¶…æ—¶: {e}")
            return ScrapingResult(
                success=False,
                data={},
                error_message=f"æµè§ˆå™¨å¯åŠ¨è¶…æ—¶: {str(e)}",
                execution_time=0
            )
        except Exception as e:
            self.logger.error(f"âŒ æµè§ˆå™¨å¯åŠ¨æˆ–é¡µé¢å¯¼èˆªå¤±è´¥: {e.__class__.__name__}: {e}")
            return ScrapingResult(
                success=False,
                data={},
                error_message=f"æµè§ˆå™¨å¯åŠ¨å¤±è´¥: {str(e)}",
                execution_time=0
            )

        # å¦‚æœ context é‡Œçš„ competitor_cnt = 0 æˆ–ä¸ºç©ºåˆ™ç›´æ¥è¿”å›
        if context and ('competitor_cnt' in context and context['competitor_cnt'] == 0):
            return ScrapingResult(
                success=True,
                data={'competitors': [], 'total_count': 0, 'scraped_at': time.time(), 'target_url': url},
                execution_time=0
            )

        # å¦‚æœ context é‡Œçš„ competitor_cnt > 5 åˆ™è¿›è¡Œexpand
        # é»˜è®¤æƒ…å†µä¸‹ï¼ˆcontextä¸ºNoneæˆ–æ²¡æœ‰competitor_cntå­—æ®µï¼‰ï¼Œä¸è¿›è¡Œexpand
        expand_pop_layer = False
        if context and 'competitor_cnt' in context:
            expand_pop_layer = context['competitor_cnt'] > 5
            self.logger.info(f"ğŸ“Š Context competitor_cnt={context['competitor_cnt']}, expand_pop_layer={expand_pop_layer}")

        try:
            # ä»kwargsä¸­æå–å‚æ•°ï¼Œé¿å…é‡å¤ä¼ é€’
            max_competitors = kwargs.pop('max_competitors', 10)
            expand_pop_layer_param = kwargs.pop('expand_pop_layer', expand_pop_layer)
            
            self.logger.info(f"ğŸ¯ å¼€å§‹æŠ“å–: max_competitors={max_competitors}, expand={expand_pop_layer_param}")

            # é»˜è®¤ä½¿ç”¨è·Ÿå–æ•°æ®æŠ“å–
            return self._scrape(
                target_url=url,
                max_competitors=max_competitors,
                expand_pop_layer=expand_pop_layer_param,
                **kwargs
            )

        except TimeoutError as e:
            self.logger.error(f"âŒ æŠ“å–è¶…æ—¶: {e}")
            raise RuntimeError(f"æŠ“å–è¶…æ—¶: {str(e)}")
        except Exception as e:
            self.logger.error(f"âŒ æŠ“å–å¤±è´¥: {e.__class__.__name__}: {e}")
            raise RuntimeError(f"æŠ“å–å¤±è´¥: {str(e)}")

    def _scrape(self,
                target_url: str,
                static_soup: Optional[BeautifulSoup] = None,
                max_competitors: int = 10,
                expand_pop_layer: bool = False,
                **kwargs) -> ScrapingResult:
        """
        ç»¼åˆè·Ÿå–æŠ“å–ï¼ˆå†…éƒ¨æ–¹æ³•ï¼Œä¿æŒå‘åå…¼å®¹ï¼‰

        Args:
            target_url: ç›®æ ‡å•†å“URL
            max_competitors: æœ€å¤§è·Ÿå–æ•°é‡
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            ScrapingResult: è·Ÿå–æŠ“å–ç»“æœ
        """
        start_time = time.time()

        try:
            # è°ƒç”¨å®é™…çš„æŠ“å–æµç¨‹ - å®‰å…¨è·å–é€‰æ‹©å™¨é…ç½®
            open_popup_selectors = getattr(self.selectors_config, 'open_popup_button_selector', [".pdp_bi8"])
            
            # éªŒè¯ browser_service çŠ¶æ€
            if not self.browser_service:
                self.logger.error("âŒ browser_service ä¸º Noneï¼Œæ— æ³•ç»§ç»­æŠ“å–")
                return ScrapingResult(
                    success=False,
                    data={},
                    error_message="æµè§ˆå™¨æœåŠ¡æœªåˆå§‹åŒ–",
                    execution_time=time.time() - start_time
                )
            
            # æ ¹æ®æ˜¯å¦æœ‰ static_soup å†³å®šä¼ å‚æ–¹å¼
            if static_soup:
                result = wait_for_content_smart(open_popup_selectors,
                                                browser_service=self.browser_service,
                                                soup=static_soup)
            else:
                result = wait_for_content_smart(open_popup_selectors,
                                                browser_service=self.browser_service)

            # å¼¹å‡ºç«å“å®¹å™¨å¹¶è·å–å†…å®¹
            popup_result = self._present_competitor_popup(expand_pop_layer)

            if not popup_result.get('success'):
                # æ£€æŸ¥æ˜¯å¦æ˜¯å› ä¸ºæ²¡æœ‰è·Ÿå–ä¿¡æ¯
                if popup_result.get('error') == 'no_competitors':
                    self.logger.info("â„¹ï¸  è¯¥å•†å“æ²¡æœ‰è·Ÿå–ä¿¡æ¯ï¼Œè¿”å›ç©ºç»“æœ")
                    return ScrapingResult(
                        success=True,
                        data={
                            'competitors': [],
                            'total_count': 0,
                            'scraped_at': time.time(),
                            'target_url': target_url,
                            'has_competitors': False
                        },
                        execution_time=time.time() - start_time
                    )
                
                return ScrapingResult(
                    success=False,
                    data={},
                    error_message=popup_result.get('error', 'å¼¹å‡ºç«å“å®¹å™¨å¤±è´¥'),
                    execution_time=time.time() - start_time
                )

            # æå–ç«å“ä¿¡æ¯
            competitors_info = self.extract_competitors_from_content(
                popup_result.get('popup_container'), max_competitors)

            # æ„å»ºå®é™…çš„æŠ“å–ç»“æœ
            competitors_data = {
                'competitors': competitors_info,
                'total_count': len(competitors_info),
                'scraped_at': time.time(),
                'target_url': target_url,
                'expanded': popup_result.get('expanded', False)
            }

            # ğŸ”§ æ–°åŠŸèƒ½ï¼šæå–ç¬¬ä¸€ä¸ªç«å“çš„å•†å“IDï¼ˆå¦‚æœcontextä¸­å¯ç”¨ï¼‰
            extract_first_product = kwargs.get('extract_first_product', False)
            if extract_first_product and len(competitors_info) > 0:
                self.logger.info("ğŸ¯ å¼€å§‹æå–ç¬¬ä¸€ä¸ªç«å“çš„å•†å“ID...")
                product_result = self._get_first_competitor_product(
                    popup_result.get('popup_container'),
                    ranking=1
                )
                
                if product_result and product_result.get('success'):
                    competitors_data['first_competitor_product_id'] = product_result.get('product_id')
                    competitors_data['first_competitor_product_url'] = product_result.get('product_url')
                    competitors_data['extraction_method'] = product_result.get('method')
                    self.logger.info(f"âœ… æˆåŠŸæå–ç¬¬ä¸€ä¸ªç«å“å•†å“ID: {product_result.get('product_id')}")
                else:
                    competitors_data['first_competitor_product_id'] = None
                    competitors_data['first_competitor_product_url'] = None
                    self.logger.warning("âš ï¸ æœªèƒ½æå–ç¬¬ä¸€ä¸ªç«å“çš„å•†å“ID")
            elif extract_first_product:
                self.logger.info("â„¹ï¸  æ— ç«å“ä¿¡æ¯ï¼Œè·³è¿‡å•†å“IDæå–")
                competitors_data['first_competitor_product_id'] = None

            return ScrapingResult(
                success=True,
                data=competitors_data,
                execution_time=time.time() - start_time
            )

        except TimeoutError as e:
            self.logger.error(f"ç»¼åˆè·Ÿå–æŠ“å–è¶…æ—¶: {e}")
            return ScrapingResult(
                success=False,
                data={},
                error_message=f"æŠ“å–è¶…æ—¶: {str(e)}",
                execution_time=time.time() - start_time
            )
        except Exception as e:
            self.logger.error(f"ç»¼åˆè·Ÿå–æŠ“å–å¤±è´¥: {e.__class__.__name__}: {e}")
            return ScrapingResult(
                success=False,
                data={},
                error_message=str(e),
                execution_time=time.time() - start_time
            )
