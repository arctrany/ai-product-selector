"""
Seerfarå¹³å°æŠ“å–å™¨

è´Ÿè´£ä»Seerfarå¹³å°æŠ“å–OZONåº—é“ºçš„é”€å”®æ•°æ®å’Œå•†å“ä¿¡æ¯ã€‚
åŸºäºç°ä»£åŒ–çš„Playwrightæµè§ˆå™¨æœåŠ¡ã€‚
"""

import time
import re
from typing import Dict, Any, List, Optional, Callable

from .base_scraper import BaseScraper
from .global_browser_singleton import get_global_browser_service
from .scraper_utils import ScraperUtils
from ..models import ScrapingResult
from common.config import GoodStoreSelectorConfig
from common.config.seerfar_selectors import get_seerfar_selector, SEERFAR_SELECTORS


class SeerfarScraper(BaseScraper):
    """Seerfarå¹³å°æŠ“å–å™¨"""

    def __init__(self, config: Optional[GoodStoreSelectorConfig] = None):
        """åˆå§‹åŒ–SeerfaræŠ“å–å™¨"""
        super().__init__()
        from common.config import get_config
        import logging

        self.config = config or get_config()
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        self.base_url = self.config.scraping.seerfar_base_url
        self.store_detail_path = self.config.scraping.seerfar_store_detail_path

        # ä½¿ç”¨å…¨å±€æµè§ˆå™¨æœåŠ¡
        self.browser_service = get_global_browser_service()

    def scrape_store_sales_data(self, store_id: str, store_filter_func=None) -> ScrapingResult:
        """
        æŠ“å–åº—é“ºé”€å”®æ•°æ®

        Args:
            store_id: åº—é“ºID
            store_filter_func: åº—é“ºè¿‡æ»¤å‡½æ•°ï¼Œç”¨äºç­›é€‰åº—é“ºï¼ˆæ£€æŸ¥é”€å”®é¢å’Œè®¢å•é‡ï¼‰

        Returns:
            ScrapingResult: æŠ“å–ç»“æœï¼ŒåŒ…å«é”€å”®æ•°æ®
        """
        # æ„å»ºåº—é“ºè¯¦æƒ…é¡µURL
        url = f"{self.base_url}{self.store_detail_path}?storeId={store_id}&platform=OZON"

        # dryrunæ¨¡å¼ä¸‹è®°å½•å…¥å‚ï¼Œä½†ä»æ‰§è¡ŒçœŸå®çš„æŠ“å–æµç¨‹
        if self.config.dryrun:
            self.logger.info(f"ğŸ§ª è¯•è¿è¡Œæ¨¡å¼ - Seerfaråº—é“ºé”€å”®æ•°æ®æŠ“å–å…¥å‚: åº—é“ºID={store_id}, URL={url}")
            self.logger.info("ğŸ§ª è¯•è¿è¡Œæ¨¡å¼ - æ‰§è¡ŒçœŸå®çš„é”€å”®æ•°æ®æŠ“å–æµç¨‹ï¼ˆç»“æœä¸ä¼šä¿å­˜åˆ°æ–‡ä»¶ï¼‰")

        # ä½¿ç”¨ç»§æ‰¿çš„æŠ“å–æ–¹æ³•
        result = self.scrape_page_data(url, self._extract_sales_data)

        # å¦‚æœæä¾›äº†è¿‡æ»¤å‡½æ•°ï¼Œåˆ™åº”ç”¨è¿‡æ»¤
        # æ³¨æ„ï¼šéœ€è¦å°†å­—æ®µåè½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼
        if result.success and store_filter_func and result.data:
            filter_data = {
                'store_sales_30days': result.data.get('sold_30days', 0),
                'store_orders_30days': result.data.get('sold_count_30days', 0)
            }
            if not store_filter_func(filter_data):
                self.logger.info(f"åº—é“º{store_id}ä¸ç¬¦åˆç­›é€‰æ¡ä»¶")
                return ScrapingResult(
                    success=False,
                    data={},
                    error_message="åº—é“ºä¸ç¬¦åˆç­›é€‰æ¡ä»¶",
                    execution_time=result.execution_time
                )

        return result



    def scrape(
        self,
        store_id: str,
        include_products: bool = True,
        max_products: Optional[int] = None,
        product_filter_func: Optional[Callable[[Dict[str, Any]], bool]] = None,
        store_filter_func: Optional[Callable[[Dict[str, Any]], bool]] = None,
        **kwargs
    ) -> ScrapingResult:
        """
        ç»Ÿä¸€çš„åº—é“ºæŠ“å–æ¥å£ï¼ˆæ•´åˆé”€å”®æ•°æ®å’Œå•†å“æŠ“å–ï¼‰

        Args:
            store_id: åº—é“ºID
            include_products: æ˜¯å¦åŒ…å«å•†å“ä¿¡æ¯ï¼Œé»˜è®¤ True
            max_products: æœ€å¤§æŠ“å–å•†å“æ•°é‡ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„å€¼
            product_filter_func: å•†å“è¿‡æ»¤å‡½æ•°ï¼Œæ¥å—å•†å“æ•°æ®å­—å…¸ï¼Œè¿”å›å¸ƒå°”å€¼
            store_filter_func: åº—é“ºè¿‡æ»¤å‡½æ•°ï¼Œæ¥å—é”€å”®æ•°æ®å­—å…¸ï¼Œè¿”å›å¸ƒå°”å€¼
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            ScrapingResult: æŠ“å–ç»“æœï¼ŒåŒ…å«é”€å”®æ•°æ®å’Œå•†å“åˆ—è¡¨

        ä½¿ç”¨åœºæ™¯ï¼š
            1. åªè·å–é”€å”®æ•°æ®ï¼šscrape(store_id, include_products=False)
            2. è·å–å®Œæ•´ä¿¡æ¯ï¼šscrape(store_id, include_products=True)
            3. å¸¦è¿‡æ»¤çš„æŠ“å–ï¼šscrape(store_id, store_filter_func=..., product_filter_func=...)
        """
        start_time = time.time()

        try:
            # 1. æŠ“å–é”€å”®æ•°æ®
            sales_result = self.scrape_store_sales_data(store_id)
            if not sales_result.success:
                return sales_result

            result_data = {
                'store_id': store_id,
                'sales_data': sales_result.data
            }

            # 2. åº”ç”¨åº—é“ºè¿‡æ»¤ï¼ˆå¦‚æœæä¾›ï¼‰
            if store_filter_func:
                # è½¬æ¢å­—æ®µåä»¥åŒ¹é…è¿‡æ»¤å‡½æ•°æœŸæœ›çš„æ ¼å¼
                filter_data = {
                    'store_sales_30days': sales_result.data.get('sold_30days', 0),
                    'store_orders_30days': sales_result.data.get('sold_count_30days', 0)
                }
                if not store_filter_func(filter_data):
                    self.logger.info(f"åº—é“º{store_id}æœªé€šè¿‡åº—é“ºè¿‡æ»¤æ¡ä»¶ï¼Œè·³è¿‡å•†å“æŠ“å–")
                    return ScrapingResult(
                        success=False,
                        data=result_data,
                        error_message="åº—é“ºæœªé€šè¿‡è¿‡æ»¤æ¡ä»¶",
                        execution_time=time.time() - start_time
                    )

            # 3. å¦‚æœéœ€è¦ï¼ŒæŠ“å–å•†å“ä¿¡æ¯
            if include_products:
                # ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤å€¼æˆ–ä¼ å…¥çš„å€¼
                max_products = max_products or self.config.store_filter.max_products_to_check

                # æ„å»ºåº—é“ºè¯¦æƒ…é¡µURL
                url = f"{self.base_url}{self.store_detail_path}?storeId={store_id}&platform=OZON"

                # dryrunæ¨¡å¼ä¸‹è®°å½•å…¥å‚
                if self.config.dryrun:
                    self.logger.info(f"ğŸ§ª è¯•è¿è¡Œæ¨¡å¼ - Seerfaråº—é“ºå•†å“æŠ“å–å…¥å‚: åº—é“ºID={store_id}, "
                                     f"æœ€å¤§å•†å“æ•°={max_products}, URL={url}")
                    self.logger.info("ğŸ§ª è¯•è¿è¡Œæ¨¡å¼ - æ‰§è¡ŒçœŸå®çš„å•†å“æŠ“å–æµç¨‹ï¼ˆç»“æœä¸ä¼šä¿å­˜åˆ°æ–‡ä»¶ï¼‰")

                # åˆ›å»ºæå–å‡½æ•°
                def extract_products(browser_service):
                    products = self._extract_products_list(
                        max_products,
                        product_filter_func
                    )
                    return {'products': products, 'total_count': len(products)}

                # ä½¿ç”¨ç»§æ‰¿çš„æŠ“å–æ–¹æ³•
                products_result = self.scrape_page_data(url, extract_products)

                if products_result.success:
                    result_data['products'] = products_result.data['products']
                else:
                    self.logger.warning(f"æŠ“å–åº—é“º{store_id}å•†å“ä¿¡æ¯å¤±è´¥: {products_result.error_message}")
                    result_data['products'] = []

            return ScrapingResult(
                success=True,
                data=result_data,
                execution_time=time.time() - start_time
            )

        except Exception as e:
            self.logger.error(f"ç»¼åˆæŠ“å–åº—é“º{store_id}ä¿¡æ¯å¤±è´¥: {e}")
            return ScrapingResult(
                success=False,
                data={},
                error_message=str(e),
                execution_time=time.time() - start_time
            )

    def _extract_sales_data(self, browser_service) -> Dict[str, Any]:
        """
        åŒæ­¥æå–é”€å”®æ•°æ® - ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é€‰æ‹©å™¨

        Args:
            browser_service: æµè§ˆå™¨æœåŠ¡å®ä¾‹

        Returns:
            Dict[str, Any]: é”€å”®æ•°æ®
        """
        self.logger.debug("ğŸš€ _extract_sales_data æ–¹æ³•è¢«è°ƒç”¨")
        sales_data = {}

        try:
            # ç›´æ¥è®¿é—® page å¯¹è±¡
            page = browser_service.page
            self.logger.debug(f"ğŸ“„ è·å–åˆ°é¡µé¢å¯¹è±¡: {page}")

            # éªŒè¯ page å¯¹è±¡
            self.logger.debug("ğŸ” å¼€å§‹é¡µé¢éªŒè¯...")
            page_valid = self._validate_page()
            self.logger.debug(f"ğŸ“‹ é¡µé¢éªŒè¯ç»“æœ: {page_valid}")

            if not page_valid:
                self.logger.warning("âŒ é¡µé¢éªŒè¯å¤±è´¥ï¼Œæ— æ³•æå–é”€å”®æ•°æ®")
                return {
                    'sold_30days': 0,
                    'sold_count_30days': 0,
                    'daily_avg_sold': 0
                }

            self.logger.debug("å¼€å§‹æå–é”€å”®æ•°æ®...")

            # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é€‰æ‹©å™¨æå–é”€å”®é¢
            self.logger.debug("æå–é”€å”®é¢...")
            self._extract_sales_amount(page, sales_data)

            # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é€‰æ‹©å™¨æå–é”€é‡
            self.logger.debug("æå–é”€é‡...")
            self._extract_sales_volume(page, sales_data)

            # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é€‰æ‹©å™¨æå–æ—¥å‡é”€é‡
            self.logger.debug("æå–æ—¥å‡é”€é‡...")
            self._extract_daily_avg_sales(page, sales_data)

            # âœ… ä¿®å¤ï¼šå³ä½¿æ²¡æœ‰æå–åˆ°æ•°æ®ï¼Œä¹Ÿè¦è¿”å›ä¸€ä¸ªæœ‰æ•ˆçš„ç»“æœ
            # è¿™æ ·å¯ä»¥é¿å…scrape_store_sales_dataè¿”å›success=False
            if not sales_data:
                self.logger.warning("æœªæå–åˆ°ä»»ä½•é”€å”®æ•°æ®ï¼Œä½†è¿”å›ç©ºæ•°æ®ç»“æ„ä»¥ç»§ç»­åç»­æµç¨‹")
                # è¿”å›é»˜è®¤çš„ç©ºæ•°æ®ç»“æ„ï¼Œè€Œä¸æ˜¯ç©ºå­—å…¸
                sales_data = {
                    'sold_30days': 0,
                    'sold_count_30days': 0,
                    'daily_avg_sold': 0
                }

            # åˆå¹¶æ—¥å¿—è¾“å‡ºåº—é“ºæ•°æ®æ‘˜è¦
            sales_amount = sales_data.get('sold_30days', 0)
            sales_volume = sales_data.get('sold_count_30days', 0)
            daily_avg = sales_data.get('daily_avg_sold', 0)
            self.logger.info(
                f"ğŸ“Š åº—é“ºæ•°æ®æå–å®Œæˆ - é”€å”®é¢: {sales_amount:.0f}â‚½, é”€é‡: {sales_volume}, æ—¥å‡: {daily_avg}")

            self.logger.debug(f"æå–çš„é”€å”®æ•°æ®: {sales_data}")
            return sales_data

        except Exception as e:
            self.logger.error(f"æå–é”€å”®æ•°æ®å¤±è´¥: {e}", exc_info=True)
            # âœ… ä¿®å¤ï¼šå³ä½¿å‘ç”Ÿå¼‚å¸¸ï¼Œä¹Ÿè¿”å›é»˜è®¤æ•°æ®ç»“æ„ï¼Œé¿å…æ•´ä¸ªæµç¨‹å¤±è´¥
            return {
                'sold_30days': 0,
                'sold_count_30days': 0,
                'daily_avg_sold': 0
            }

    def _extract_sales_amount(self, page, sales_data: Dict[str, Any]):
        """æå–é”€å”®é¢ - ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é€‰æ‹©å™¨"""
        try:
            # ä»é…ç½®æ–‡ä»¶è·å–é”€å”®é¢é€‰æ‹©å™¨
            sales_amount_selector = get_seerfar_selector('store_sales_data', 'sales_amount')
            self.logger.debug(f"é”€å”®é¢é€‰æ‹©å™¨: {sales_amount_selector}")
            if not sales_amount_selector:
                self.logger.error("âŒ æœªèƒ½æ‰¾åˆ°é”€å”®é¢é€‰æ‹©å™¨é…ç½®")
                return

            # ğŸ”§ ä½¿ç”¨åŒæ­¥æ–¹æ³•è·å–æ–‡æœ¬å†…å®¹
            self.logger.debug(f"å°è¯•è·å–é”€å”®é¢å…ƒç´ æ–‡æœ¬ï¼Œé€‰æ‹©å™¨: {sales_amount_selector}")
            text = self.browser_service.text_content_sync(sales_amount_selector, timeout=5000)
            self.logger.debug(f"é”€å”®é¢å…ƒç´ æ–‡æœ¬å†…å®¹: '{text}'")
            if text and text.strip():
                # æå–æ•°å­—å¹¶è½¬æ¢ä¸ºé”€å”®é¢
                stripped_text = text.strip()
                self.logger.debug(f"å¤„ç†é”€å”®é¢æ–‡æœ¬: '{stripped_text}'")
                number = ScraperUtils.extract_number_from_text(stripped_text)
                self.logger.debug(f"æå–åˆ°çš„æ•°å­—: {number}")
                if number:
                    sales_data['sold_30days'] = number
                    self.logger.debug(f"é”€å”®é¢æå–æˆåŠŸ: {number}")
                    return
                else:
                    self.logger.warning(f"æ— æ³•ä»æ–‡æœ¬ä¸­æå–æ•°å­—: '{stripped_text}'")

            self.logger.warning("âš ï¸ æœªèƒ½æå–åˆ°é”€å”®é¢æ•°æ®")

        except Exception as e:
            self.logger.error(f"âŒ é”€å”®é¢æå–å¤±è´¥: {str(e)}", exc_info=True)

    def _extract_sales_volume(self, page, sales_data: Dict[str, Any]):
        """æå–é”€é‡ - ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é€‰æ‹©å™¨"""
        try:
            # ä»é…ç½®æ–‡ä»¶è·å–é”€é‡é€‰æ‹©å™¨
            sales_volume_selector = get_seerfar_selector('store_sales_data', 'sales_volume')
            self.logger.debug(f"é”€é‡é€‰æ‹©å™¨: {sales_volume_selector}")
            if not sales_volume_selector:
                self.logger.error("âŒ æœªèƒ½æ‰¾åˆ°é”€é‡é€‰æ‹©å™¨é…ç½®")
                return

            # ğŸ”§ ä½¿ç”¨åŒæ­¥æ–¹æ³•è·å–æ–‡æœ¬å†…å®¹
            self.logger.debug(f"å°è¯•è·å–é”€é‡å…ƒç´ æ–‡æœ¬ï¼Œé€‰æ‹©å™¨: {sales_volume_selector}")
            text = self.browser_service.text_content_sync(sales_volume_selector, timeout=5000)
            self.logger.debug(f"é”€é‡å…ƒç´ æ–‡æœ¬å†…å®¹: '{text}'")
            if text and text.strip():
                # æå–æ•°å­—å¹¶è½¬æ¢ä¸ºé”€é‡
                stripped_text = text.strip()
                self.logger.debug(f"å¤„ç†é”€é‡æ–‡æœ¬: '{stripped_text}'")
                number = ScraperUtils.extract_number_from_text(stripped_text)
                self.logger.debug(f"æå–åˆ°çš„æ•°å­—: {number}")
                if number:
                    sales_data['sold_count_30days'] = int(number)
                    self.logger.debug(f"é”€é‡æå–æˆåŠŸ: {number}")
                    return
                else:
                    self.logger.warning(f"æ— æ³•ä»æ–‡æœ¬ä¸­æå–æ•°å­—: '{stripped_text}'")

            self.logger.warning("âš ï¸ æœªèƒ½æå–åˆ°é”€é‡æ•°æ®")

        except Exception as e:
            self.logger.error(f"âŒ é”€é‡æå–å¤±è´¥: {str(e)}", exc_info=True)

    def _extract_daily_avg_sales(self, page, sales_data: Dict[str, Any]):
        """æå–æ—¥å‡é”€é‡ - ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é€‰æ‹©å™¨"""
        try:
            # ä»é…ç½®æ–‡ä»¶è·å–æ—¥å‡é”€é‡é€‰æ‹©å™¨
            daily_avg_selector = get_seerfar_selector('store_sales_data', 'daily_avg_sales')
            self.logger.debug(f"æ—¥å‡é”€é‡é€‰æ‹©å™¨: {daily_avg_selector}")
            if not daily_avg_selector:
                self.logger.error("âŒ æœªèƒ½æ‰¾åˆ°æ—¥å‡é”€é‡é€‰æ‹©å™¨é…ç½®")
                return

            # ğŸ”§ ä½¿ç”¨åŒæ­¥æ–¹æ³•è·å–æ–‡æœ¬å†…å®¹
            self.logger.debug(f"å°è¯•è·å–æ—¥å‡é”€é‡å…ƒç´ æ–‡æœ¬ï¼Œé€‰æ‹©å™¨: {daily_avg_selector}")
            text = self.browser_service.text_content_sync(daily_avg_selector, timeout=5000)
            self.logger.debug(f"æ—¥å‡é”€é‡å…ƒç´ æ–‡æœ¬å†…å®¹: '{text}'")
            if text and text.strip():
                # æå–æ•°å­—å¹¶è½¬æ¢ä¸ºæ—¥å‡é”€é‡
                stripped_text = text.strip()
                self.logger.debug(f"å¤„ç†æ—¥å‡é”€é‡æ–‡æœ¬: '{stripped_text}'")
                number = ScraperUtils.extract_number_from_text(stripped_text)
                self.logger.debug(f"æå–åˆ°çš„æ•°å­—: {number}")
                if number:
                    sales_data['daily_avg_sold'] = number
                    self.logger.debug(f"æ—¥å‡é”€é‡æå–æˆåŠŸ: {number}")
                    return
                else:
                    self.logger.warning(f"æ— æ³•ä»æ–‡æœ¬ä¸­æå–æ•°å­—: '{stripped_text}'")

            self.logger.warning("âš ï¸ æœªèƒ½æå–åˆ°æ—¥å‡é”€é‡æ•°æ®")

        except Exception as e:
            self.logger.error(f"âŒ æ—¥å‡é”€é‡æå–å¤±è´¥: {str(e)}", exc_info=True)

    def _extract_category_data(self, page, sales_data: Dict[str, Any]):
        """æå–ç±»ç›®æ•°æ® - ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é€‰æ‹©å™¨"""
        try:
            # ä»é…ç½®æ–‡ä»¶è·å–ç±»ç›®æ•°æ®é€‰æ‹©å™¨
            category_xpath = get_seerfar_selector('store_sales_data', 'category_data')
            if not category_xpath:
                self.logger.debug("æœªé…ç½®ç±»ç›®æ•°æ®é€‰æ‹©å™¨ï¼Œè·³è¿‡ç±»ç›®æ•°æ®æå–")
                return

            # ğŸ”§ ä½¿ç”¨åŒæ­¥æ–¹æ³•è·å–æ–‡æœ¬å†…å®¹
            text = self.browser_service.text_content_sync(f'xpath={category_xpath}', timeout=5000)
            if text and text.strip():
                sales_data['category_info'] = text.strip()
                return

            self.logger.warning("âš ï¸ æœªèƒ½æå–åˆ°ç±»ç›®æ•°æ®")

        except Exception as e:
            self.logger.error(f"âŒ ç±»ç›®æ•°æ®æå–å¤±è´¥: {str(e)}")

    def _extract_sales_data_generic(self, page) -> Dict[str, Any]:
        """
        åŒæ­¥é€šç”¨æ–¹æ³•æå–é”€å”®æ•°æ®
        
        Args:
            page: Playwrighté¡µé¢å¯¹è±¡
            
        Returns:
            Dict[str, Any]: é”€å”®æ•°æ®
        """
        sales_data = {}

        try:
            # ğŸ”§ ä½¿ç”¨ evaluate_sync ç›´æ¥åœ¨é¡µé¢ä¸Šæå–æ–‡æœ¬å†…å®¹
            script = """
                () => {
                    const xpath = "//*[contains(text(), 'â‚½') or contains(text(), 'ä¸‡') or contains(text(), 'åƒ')]";
                    const elements = document.evaluate(xpath, document, null, XPathResult.ORDERED_NODE_SNAPSHOT_TYPE, null);
                    const texts = [];
                    for (let i = 0; i < Math.min(elements.snapshotLength, 10); i++) {
                        const element = elements.snapshotItem(i);
                        if (element && element.textContent) {
                            texts.push(element.textContent.trim());
                        }
                    }
                    return texts;
                }
            """
            text_list = self.browser_service.evaluate_sync(script, timeout=5000)

            if text_list:
                for text in text_list:
                    try:
                        if not text:
                            continue

                        # åˆ¤æ–­æ˜¯å¦ä¸ºé”€å”®é¢
                        if any(keyword in text for keyword in ['é”€å”®é¢', 'è¥ä¸šé¢', 'æ”¶å…¥', 'â‚½']):
                            number = ScraperUtils.extract_number_from_text(text)
                            if number and number > 1000:  # é”€å”®é¢é€šå¸¸è¾ƒå¤§
                                sales_data['sold_30days'] = number

                        # åˆ¤æ–­æ˜¯å¦ä¸ºé”€é‡
                        elif any(keyword in text for keyword in ['é”€é‡', 'è®¢å•', 'ä»¶æ•°']):
                            number = ScraperUtils.extract_number_from_text(text)
                            if number and 10 <= number <= 10000:  # é”€é‡é€šå¸¸åœ¨åˆç†èŒƒå›´å†…
                                sales_data['sold_count_30days'] = int(number)
                    except Exception as e:
                        self.logger.debug(f"å¤„ç†å…ƒç´ æ–‡æœ¬å¤±è´¥: {e}")
                        continue

            # å¦‚æœæ‰¾åˆ°é”€å”®é¢å’Œé”€é‡ï¼Œè®¡ç®—æ—¥å‡é”€é‡
            if 'sold_30days' in sales_data and 'sold_count_30days' in sales_data:
                sales_data['daily_avg_sold'] = sales_data['sold_count_30days'] / 30

            return sales_data

        except Exception as e:
            self.logger.error(f"é€šç”¨æ–¹æ³•æå–é”€å”®æ•°æ®å¤±è´¥: {e}")
            return {}

    def _extract_all_products_data_js(self, product_rows_selector: str) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨ JavaScript evaluate ä¸€æ¬¡æ€§æå–æ‰€æœ‰å•†å“è¡Œæ•°æ®

        è¿™ä¸ªæ–¹æ³•é¿å…äº† ElementHandle çš„äº‹ä»¶å¾ªç¯é—®é¢˜ï¼Œé€šè¿‡åœ¨æµè§ˆå™¨ç«¯ç›´æ¥æå–æ•°æ®

        Args:
            product_rows_selector: å•†å“è¡Œé€‰æ‹©å™¨

        Returns:
            List[Dict[str, Any]]: å•†å“è¡Œæ•°æ®åˆ—è¡¨
        """
        try:
            # ğŸ”§ ä»é…ç½®æ–‡ä»¶è·å–é€‰æ‹©å™¨å’Œåˆ—ç´¢å¼•
            category_cn_selector = SEERFAR_SELECTORS.product_list.get('category_cn_selector', 'span.category-title')
            category_ru_selector = SEERFAR_SELECTORS.product_list.get('category_ru_selector', 'span.text-muted')
            category_column_idx = SEERFAR_SELECTORS.column_indexes['category']
            sales_volume_column_idx = SEERFAR_SELECTORS.column_indexes['sales_volume']

            # JavaScript ä»£ç ï¼šåœ¨æµè§ˆå™¨ç«¯æå–æ‰€æœ‰å•†å“è¡Œæ•°æ®
            # æ³¨æ„ï¼šæ­£åˆ™è¡¨è¾¾å¼ç›´æ¥ç¡¬ç¼–ç åœ¨JSä¸­ï¼Œé¿å…f-stringèŠ±æ‹¬å·å†²çª
            js_code = f"""
            () => {{
                const rows = document.querySelectorAll('{product_rows_selector}');
                return Array.from(rows).map((row, index) => {{
                    const tds = row.querySelectorAll('td');
                    
                    // æå–ç±»ç›®ï¼ˆç¬¬{category_column_idx}åˆ—ï¼‰
                    const categoryTd = tds[{category_column_idx}];
                    const categoryCn = categoryTd?.querySelector('{category_cn_selector}')?.textContent?.trim() || null;
                    const categoryRu = categoryTd?.querySelector('{category_ru_selector}')?.textContent?.trim() || null;
                    
                    // æå–é”€é‡ï¼ˆç¬¬{sales_volume_column_idx}åˆ—ï¼‰
                    const salesTd = tds[{sales_volume_column_idx}];
                    const salesText = salesTd?.textContent?.trim() || '';
                    const salesMatch = salesText.match(/^(\\d+)/);
                    const salesVolume = salesMatch ? parseInt(salesMatch[1]) : null;
                    
                    // æå–é‡é‡ï¼ˆå€’æ•°ç¬¬äºŒåˆ—ï¼‰
                    const weightTd = tds[tds.length - 2];
                    const weightText = weightTd?.textContent?.trim() || '';
                    const weightMatch = weightText.match(/([\\d.]+)\\s*(g|kg)/i);
                    let weight = null;
                    if (weightMatch) {{
                        const value = parseFloat(weightMatch[1]);
                        const unit = weightMatch[2].toLowerCase();
                        weight = unit === 'kg' ? value * 1000 : value;
                    }}
                    
                    // æå–ä¸Šæ¶æ—¶é—´ï¼ˆæœ€åä¸€åˆ—ï¼‰
                    const listingTd = tds[tds.length - 1];
                    const listingHtml = listingTd?.innerHTML || '';
                    const dateMatch = listingHtml.match(/(\\d{{4}}-\\d{{2}}-\\d{{2}})/);
                    const durationMatch = listingHtml.match(/<span[^>]*>([^<]+)<\\/span>/);
                    const listingDate = dateMatch ? dateMatch[1] : null;
                    const shelfDuration = durationMatch ? durationMatch[1].trim() : null;
                    
                    // æå– OZON URLï¼ˆç¬¬2åˆ—çš„ onclick äº‹ä»¶ï¼‰
                    const td2 = tds[2];
                    const clickableElement = td2?.querySelector('[onclick*="window.open"]') || 
                                           td2?.querySelector('a[onclick*="window.open"]');
                    const onclickAttr = clickableElement?.getAttribute('onclick') || '';
                    const urlMatch = onclickAttr.match(/window\\.open\\('([^']+)'\\)/);
                    const ozonUrl = urlMatch ? urlMatch[1] : null;
                    
                    // è·å– data-index ç”¨äºå»é‡
                    const dataIndex = row.getAttribute('data-index');
                    
                    return {{
                        index: index,
                        dataIndex: dataIndex,
                        categoryCn: categoryCn,
                        categoryRu: categoryRu,
                        salesVolume: salesVolume,
                        weight: weight,
                        listingDate: listingDate,
                        shelfDuration: shelfDuration,
                        ozonUrl: ozonUrl
                    }};
                }});
            }}
            """

            # ä½¿ç”¨åŒæ­¥çš„ evaluate æ–¹æ³•
            products_data = self.browser_service.evaluate_sync(js_code)

            # å»é‡ï¼šåŸºäº dataIndex
            if products_data:
                seen_indices = set()
                unique_products = []
                for product in products_data:
                    data_index = product.get('dataIndex')
                    if data_index and data_index not in seen_indices:
                        seen_indices.add(data_index)
                        unique_products.append(product)
                    elif not data_index:
                        # æ²¡æœ‰ data-index çš„ä¹Ÿä¿ç•™
                        unique_products.append(product)

                self.logger.info(f"ğŸ“‹ JavaScript æå–åˆ° {len(unique_products)} ä¸ªå•†å“è¡Œï¼ˆå»é‡åï¼‰")
                return unique_products

            return []

        except Exception as e:
            self.logger.error(f"âŒ JavaScript æå–å•†å“æ•°æ®å¤±è´¥: {e}")
            return []

    def _extract_products_list(self, max_products: int,
                              product_filter_func: Optional[Callable[[Dict[str, Any]], bool]] = None) -> List[Dict[str, Any]]:
        """
        æå–å•†å“åˆ—è¡¨ - åŒæ­¥å®ç°

        é‡æ„çš„åŒæ­¥ç‰ˆæœ¬ï¼Œæ”¯æŒå‰ç½®è¿‡æ»¤ï¼Œæ¶ˆé™¤å¼‚æ­¥å¤æ‚æ€§ã€‚
        ä½¿ç”¨JavaScriptä¸€æ¬¡æ€§æå–æ‰€æœ‰å•†å“æ•°æ®ï¼Œé¿å…é€ä¸ªå…ƒç´ æŸ¥è¯¢çš„æ€§èƒ½é—®é¢˜ã€‚

        Args:
            max_products: æœ€å¤§å•†å“æ•°é‡
            product_filter_func: å•†å“è¿‡æ»¤å‡½æ•°ï¼Œç”¨äºå‰ç½®è¿‡æ»¤

        Returns:
            List[Dict[str, Any]]: å•†å“åˆ—è¡¨
        """
        products = []
        filtered_count = 0

        try:
            self.logger.info(f"å¼€å§‹æå–å•†å“åˆ—è¡¨ï¼ˆåŒæ­¥å®ç°ï¼Œæœ€å¤š {max_products} ä¸ªï¼‰")

            # ä»é…ç½®æ–‡ä»¶è·å–å•†å“åˆ—è¡¨é€‰æ‹©å™¨
            product_rows_selector = get_seerfar_selector('product_list', 'product_rows')
            product_rows_alt_selector = get_seerfar_selector('product_list', 'product_rows_alt')

            if not product_rows_selector or not product_rows_alt_selector:
                self.logger.error("âŒ æœªèƒ½æ‰¾åˆ°å•†å“åˆ—è¡¨é€‰æ‹©å™¨é…ç½®")
                return []

            # ä½¿ç”¨JavaScriptä¸€æ¬¡æ€§æå–æ‰€æœ‰å•†å“æ•°æ®
            products_data = self._extract_all_products_data_js(product_rows_selector)

            if not products_data:
                # å°è¯•å¤‡ç”¨é€‰æ‹©å™¨
                products_data = self._extract_all_products_data_js(product_rows_alt_selector)

            if not products_data:
                self.logger.warning("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•å•†å“è¡Œ")
                return []

            total_rows = len(products_data)
            self.logger.info(f"ğŸ“‹ æ‰¾åˆ° {total_rows} ä¸ªå•†å“è¡Œï¼Œå¼€å§‹å¤„ç†ï¼ˆæœ€å¤š {max_products} ä¸ªï¼‰")

            # éå†æå–çš„å•†å“æ•°æ®
            for i in range(min(total_rows, max_products)):
                try:
                    product_data_js = products_data[i]

                    # æ„å»ºåŸºç¡€å•†å“æ•°æ®ç”¨äºå‰ç½®è¿‡æ»¤
                    basic_product_data = {
                        'product_category_cn': product_data_js.get('categoryCn'),
                        'product_category_ru': product_data_js.get('categoryRu'),
                        'product_listing_date': product_data_js.get('listingDate'),
                        'product_shelf_duration': product_data_js.get('shelfDuration'),
                        'product_sales_volume': product_data_js.get('salesVolume'),
                        'product_weight': product_data_js.get('weight')
                    }

                    # åº”ç”¨å‰ç½®è¿‡æ»¤
                    if product_filter_func:
                        if not product_filter_func(basic_product_data):
                            filtered_count += 1
                            self.logger.debug(f"â­ï¸  å•†å“ #{i+1} æœªé€šè¿‡å‰ç½®è¿‡æ»¤ï¼Œè·³è¿‡ OZON è¯¦æƒ…é¡µå¤„ç†")
                            continue

                    # æ„å»ºå®Œæ•´å•†å“æ•°æ®
                    product_data = {
                        'category_cn': product_data_js.get('categoryCn'),
                        'category_ru': product_data_js.get('categoryRu'),
                        'listing_date': product_data_js.get('listingDate'),
                        'shelf_duration': product_data_js.get('shelfDuration'),
                        'sales_volume': product_data_js.get('salesVolume'),
                        'weight': product_data_js.get('weight')
                    }

                    # è·å– OZON URL
                    ozon_url = product_data_js.get('ozonUrl')
                    ozon_data_success = False
                    if ozon_url:
                        self.logger.info(f"ğŸ“ æå–åˆ° OZON URL: {ozon_url}")

                        # æŠ“å– OZON è¯¦æƒ…é¡µæ•°æ® - åŒæ­¥å®ç°
                        ozon_data = self._fetch_ozon_details(ozon_url)
                        if ozon_data:
                            product_data.update(ozon_data)
                            ozon_data_success = True
                        else:
                            self.logger.warning(f"âš ï¸ å•†å“ #{i+1} OZON æ•°æ®è·å–å¤±è´¥")

                    if product_data:
                        products.append(product_data)
                        if ozon_data_success:
                            self.logger.info(f"âœ… å•†å“ #{i+1} æå–æˆåŠŸï¼ˆå« OZON æ•°æ®ï¼‰")
                        else:
                            self.logger.warning(f"âš ï¸ å•†å“ #{i+1} æå–éƒ¨åˆ†æˆåŠŸï¼ˆä»…åŸºç¡€æ•°æ®ï¼ŒOZON æ•°æ®ç¼ºå¤±ï¼‰")

                except Exception as e:
                    self.logger.warning(f"âš ï¸  æå–ç¬¬ {i + 1} ä¸ªå•†å“ä¿¡æ¯å¤±è´¥: {e}")
                    continue

            if products:
                self.logger.info(f"ğŸ‰ æˆåŠŸæå– {len(products)} ä¸ªæœ‰æ•ˆå•†å“ä¿¡æ¯ï¼ˆå‰ç½®è¿‡æ»¤è·³è¿‡ {filtered_count} ä¸ªï¼‰")
            else:
                self.logger.warning("âš ï¸  æœªæå–åˆ°æœ‰æ•ˆçš„å•†å“ä¿¡æ¯")
            return products

        except Exception as e:
            self.logger.error(f"âŒ æå–å•†å“åˆ—è¡¨å¤±è´¥: {e}")
            return []

    def close(self):
        """
        å…³é—­ SeerfarScraperï¼Œæ¸…ç†èµ„æº - åŒæ­¥å®ç°
        """
        try:
            super().close()  # è°ƒç”¨åŸºç±»çš„åŒæ­¥å…³é—­æ–¹æ³•
            self.logger.info("ğŸ”’ SeerfarScraper å·²å…³é—­")
        except Exception as e:
            self.logger.warning(f"å…³é—­ SeerfarScraper æ—¶å‡ºé”™: {e}")

    def _extract_basic_product_data(self, row_element) -> Dict[str, Any]:
        """
        æå– Seerfar è¡¨æ ¼ä¸­çš„åŸºç¡€å•†å“æ•°æ®

        Args:
            row_element: è¡Œå…ƒç´ 

        Returns:
            Dict[str, Any]: åŸºç¡€å•†å“æ•°æ®ï¼ˆç±»ç›®ã€ä¸Šæ¶æ—¶é—´ã€é”€é‡ã€é‡é‡ï¼‰
        """
        product_data = {}

        # 1. æå–ç±»ç›®ä¿¡æ¯
        category_data = self._extract_category(row_element)
        product_data.update(category_data)

        # 2. æå–ä¸Šæ¶æ—¶é—´
        listing_date_data = self._extract_listing_date(row_element)
        product_data.update(listing_date_data)

        # 3. æå–é”€é‡
        sales_volume = self._extract_product_sales_volume(row_element)
        if sales_volume is not None:
            product_data['sales_volume'] = sales_volume

        # 4. æå–é‡é‡
        weight = self._extract_weight(row_element)
        if weight is not None:
            product_data['weight'] = weight

        return product_data

    def _get_ozon_url_from_row(self, row_element) -> Optional[str]:
        """
        ä»è¡Œå…ƒç´ ä¸­æå– OZON URL

        Args:
            row_element: è¡Œå…ƒç´ 

        Returns:
            Optional[str]: OZON URLï¼Œå¦‚æœæå–å¤±è´¥è¿”å› None
        """
        try:
            # éªŒè¯é¡µé¢å¯¹è±¡
            if not self._validate_page():
                return None

            # ä»é…ç½®æ–‡ä»¶è·å–é€‰æ‹©å™¨
            third_column_selector = get_seerfar_selector('product_list', 'third_column')
            clickable_element_selector = get_seerfar_selector('product_list', 'clickable_element')
            clickable_element_alt_selector = get_seerfar_selector('product_list', 'clickable_element_alt')

            if not third_column_selector or not clickable_element_selector or not clickable_element_alt_selector:
                self.logger.error("âŒ æœªèƒ½æ‰¾åˆ°å•†å“è¡Œå…ƒç´ é€‰æ‹©å™¨é…ç½®")
                return None

            # ä½¿ç”¨JavaScriptä¸€æ¬¡æ€§è·å–OZON URLï¼Œé¿å…å¤æ‚çš„å…ƒç´ æŸ¥æ‰¾
            js_script = f"""
            // æŸ¥æ‰¾åŒ…å«onclickçš„å¯ç‚¹å‡»å…ƒç´ 
            const rowElements = document.querySelectorAll('tr[data-index]');
            let targetRow = null;
            
            // æ‰¾åˆ°å¯¹åº”çš„è¡Œï¼ˆé€šè¿‡data-indexæˆ–ä½ç½®ï¼‰
            for (let row of rowElements) {{
                const cells = row.querySelectorAll('td');
                if (cells.length >= 3) {{
                    const thirdCell = cells[2]; // ç¬¬ä¸‰åˆ—
                    const clickableElements = thirdCell.querySelectorAll('*[onclick*="window.open"]');
                    if (clickableElements.length > 0) {{
                        const onclick = clickableElements[0].getAttribute('onclick');
                        if (onclick && onclick.includes('window.open')) {{
                            const urlMatch = onclick.match(/window\\.open\\('([^']+)'\\)/);
                            if (urlMatch) {{
                                return urlMatch[1]; // è¿”å›URL
                            }}
                        }}
                    }}
                }}
            }}
            return null;
            """

            ozon_url = self.browser_service.evaluate_sync(js_script)
            if not ozon_url:
                self.logger.warning("âš ï¸ æœªæ‰¾åˆ°OZON URL")
                return None

            self.logger.info(f"ğŸ”— æå–åˆ°OZON URL: {ozon_url}")

            # ç›´æ¥è¿”å›URLï¼Œä¸éœ€è¦è¿›ä¸€æ­¥è§£æ
            return ozon_url

        except Exception as e:
            self.logger.error(f"æå– OZON URL å¤±è´¥: {e}")
            return None

    def _extract_data_with_selector(self, category: str, key: str, data_key: str,
                                   validation_func: Callable = None,
                                   default_selector: str = None) -> Optional[Any]:
        """
        é€šç”¨æ•°æ®æå–æ–¹æ³• - ç»Ÿä¸€å¤„ç†é€‰æ‹©å™¨è·å–ã€å…ƒç´ ç­‰å¾…ã€æ•°æ®éªŒè¯

        Args:
            category: é…ç½®åˆ†ç±»
            key: é…ç½®é”®å
            data_key: æ•°æ®é”®åï¼ˆç”¨äºæ—¥å¿—ï¼‰
            validation_func: éªŒè¯å‡½æ•°
            default_selector: é»˜è®¤é€‰æ‹©å™¨

        Returns:
            æå–çš„æ•°æ®æˆ–None
        """
        try:
            # ä»é…ç½®æ–‡ä»¶è·å–é€‰æ‹©å™¨
            selector = get_seerfar_selector(category, key)
            if not selector and default_selector:
                selector = default_selector
                self.logger.warning(f"ä½¿ç”¨é™çº§{data_key}é€‰æ‹©å™¨")

            if not selector:
                self.logger.error(f"âŒ æœªèƒ½æ‰¾åˆ°{data_key}é€‰æ‹©å™¨é…ç½®")
                return None

            # ç­‰å¾…å…ƒç´ åŠ è½½
            if not self.wait_for_element(selector, timeout=15.0):
                self.logger.warning(f"âš ï¸ {data_key}å…ƒç´ ç­‰å¾…è¶…æ—¶ï¼Œå°è¯•ç›´æ¥æå–")

            # æå–æ–‡æœ¬å†…å®¹
            text = self.get_text_content(selector, timeout=5.0)
            if text and text.strip():
                # æå–æ•°å­—
                value = ScraperUtils.extract_number_from_text(text.strip())
                if value and (validation_func is None or validation_func(value)):
                    self.logger.debug(f"âœ… {data_key}æå–æˆåŠŸ: {value}")
                    return value
                else:
                    self.logger.warning(f"âš ï¸ {data_key}æ•°å€¼å¼‚å¸¸: {value}")
            else:
                self.logger.warning(f"âš ï¸ æœªèƒ½è·å–{data_key}æ–‡æœ¬")

        except Exception as e:
            self.logger.error(f"âŒ {data_key}æå–å¤±è´¥: {e}")

        return None

    def _extract_data_with_js(self, js_template: str, data_key: str = "æ•°æ®", **kwargs) -> Any:
        """
        é€šç”¨JavaScriptæ•°æ®æå–æ–¹æ³• - ç»Ÿä¸€JavaScriptæ‰§è¡Œå’Œé”™è¯¯å¤„ç†

        Args:
            js_template: JavaScriptæ¨¡æ¿
            data_key: æ•°æ®é”®åï¼ˆç”¨äºæ—¥å¿—ï¼‰
            **kwargs: æ¨¡æ¿å‚æ•°

        Returns:
            JavaScriptæ‰§è¡Œç»“æœ
        """
        try:
            # æ ¼å¼åŒ–JavaScriptä»£ç 
            js_script = js_template.format(**kwargs)

            # æ‰§è¡ŒJavaScript
            result = self.browser_service.evaluate_sync(js_script)

            if result is not None:
                self.logger.debug(f"âœ… {data_key}JavaScriptæå–æˆåŠŸ")
                return result
            else:
                self.logger.warning(f"âš ï¸ {data_key}JavaScriptæ‰§è¡Œæœªè¿”å›æœ‰æ•ˆæ•°æ®")
                return None

        except Exception as e:
            self.logger.error(f"âŒ {data_key}JavaScriptæ‰§è¡Œå¤±è´¥: {e}")
            return None

    def _extract_sales_amount(self, page, sales_data: Dict[str, Any]):
        """æå–é”€å”®é¢ - ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é€‰æ‹©å™¨"""
        try:
            # ä»é…ç½®æ–‡ä»¶è·å–é”€å”®é¢é€‰æ‹©å™¨
            sales_amount_selector = get_seerfar_selector('store_sales_data', 'sales_amount')
            self.logger.debug(f"é”€å”®é¢é€‰æ‹©å™¨: {sales_amount_selector}")
            if not sales_amount_selector:
                self.logger.error("âŒ æœªèƒ½æ‰¾åˆ°é”€å”®é¢é€‰æ‹©å™¨é…ç½®")
                # å°è¯•ä½¿ç”¨é™çº§é€‰æ‹©å™¨
                sales_amount_selector = '.store-total-revenue'
                self.logger.warning(f"ä½¿ç”¨é™çº§é”€å”®é¢é€‰æ‹©å™¨: {sales_amount_selector}")

            # ç­‰å¾…å…ƒç´ åŠ è½½ï¼ˆæœ€å¤š15ç§’ï¼‰
            if not self.wait_for_element(sales_amount_selector, timeout=15.0):
                self.logger.warning("âš ï¸ é”€å”®é¢å…ƒç´ ç­‰å¾…è¶…æ—¶ï¼Œå°è¯•ç›´æ¥æå–")

            # æå–é”€å”®é¢æ–‡æœ¬
            sales_text = self.get_text_content(sales_amount_selector, timeout=5.0)
            self.logger.debug(f"é”€å”®é¢å…ƒç´ æ–‡æœ¬å†…å®¹: '{sales_text}'")
            if sales_text and sales_text.strip():
                # ä½¿ç”¨å·¥å…·ç±»æå–æ•°å­—
                sales_amount = ScraperUtils.extract_number_from_text(sales_text.strip())
                self.logger.debug(f"æå–åˆ°çš„é”€å”®é¢æ•°å­—: {sales_amount}")
                if sales_amount is not None:  # ä¸å†ä½¿ç”¨è¿‡äºä¸¥æ ¼çš„éªŒè¯
                    sales_data['sold_30days'] = sales_amount
                    self.logger.debug(f"âœ… é”€å”®é¢æå–æˆåŠŸ: {sales_amount}")
                    return
                else:
                    self.logger.warning(f"âš ï¸ æ— æ³•ä»æ–‡æœ¬ä¸­æå–é”€å”®é¢æ•°å­—: '{sales_text.strip()}'")
            else:
                self.logger.warning("âš ï¸ æœªèƒ½è·å–é”€å”®é¢æ–‡æœ¬")

        except Exception as e:
            self.logger.error(f"âŒ é”€å”®é¢æå–å¤±è´¥: {e}", exc_info=True)

    def _extract_sales_volume(self, page, sales_data: Dict[str, Any]):
        """æå–é”€é‡ - ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é€‰æ‹©å™¨"""
        try:
            # ä»é…ç½®æ–‡ä»¶è·å–é”€é‡é€‰æ‹©å™¨
            sales_volume_selector = get_seerfar_selector('store_sales_data', 'sales_volume')
            self.logger.debug(f"é”€é‡é€‰æ‹©å™¨: {sales_volume_selector}")
            if not sales_volume_selector:
                self.logger.error("âŒ æœªèƒ½æ‰¾åˆ°é”€é‡é€‰æ‹©å™¨é…ç½®")
                # å°è¯•ä½¿ç”¨é™çº§é€‰æ‹©å™¨
                sales_volume_selector = '.store-total-sales'
                self.logger.warning(f"ä½¿ç”¨é™çº§é”€é‡é€‰æ‹©å™¨: {sales_volume_selector}")

            # ç­‰å¾…å…ƒç´ åŠ è½½ï¼ˆæœ€å¤š15ç§’ï¼‰
            if not self.wait_for_element(sales_volume_selector, timeout=15.0):
                self.logger.warning("âš ï¸ é”€é‡å…ƒç´ ç­‰å¾…è¶…æ—¶ï¼Œå°è¯•ç›´æ¥æå–")

            # æå–é”€é‡æ–‡æœ¬
            volume_text = self.get_text_content(sales_volume_selector, timeout=5.0)
            self.logger.debug(f"é”€é‡å…ƒç´ æ–‡æœ¬å†…å®¹: '{volume_text}'")
            if volume_text and volume_text.strip():
                # ä½¿ç”¨å·¥å…·ç±»æå–æ•°å­—
                sales_volume = ScraperUtils.extract_number_from_text(volume_text.strip())
                self.logger.debug(f"æå–åˆ°çš„é”€é‡æ•°å­—: {sales_volume}")
                if sales_volume is not None:  # ä¸å†ä½¿ç”¨è¿‡äºä¸¥æ ¼çš„éªŒè¯
                    sales_data['sold_count_30days'] = int(sales_volume)
                    self.logger.debug(f"âœ… é”€é‡æå–æˆåŠŸ: {sales_volume}")
                    return
                else:
                    self.logger.warning(f"âš ï¸ æ— æ³•ä»æ–‡æœ¬ä¸­æå–é”€é‡æ•°å­—: '{volume_text.strip()}'")
            else:
                self.logger.warning("âš ï¸ æœªèƒ½è·å–é”€é‡æ–‡æœ¬")

        except Exception as e:
            self.logger.error(f"âŒ é”€é‡æå–å¤±è´¥: {e}", exc_info=True)

    def _extract_daily_avg_sales(self, page, sales_data: Dict[str, Any]):
        """æå–æ—¥å‡é”€é‡ - ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é€‰æ‹©å™¨æˆ–æ ¹æ®å·²æœ‰æ•°æ®è®¡ç®—"""
        try:
            # ä»é…ç½®æ–‡ä»¶è·å–æ—¥å‡é”€é‡é€‰æ‹©å™¨
            daily_avg_selector = get_seerfar_selector('store_sales_data', 'daily_avg_sales')
            self.logger.debug(f"æ—¥å‡é”€é‡é€‰æ‹©å™¨: {daily_avg_selector}")
            if not daily_avg_selector:
                self.logger.error("âŒ æœªèƒ½æ‰¾åˆ°æ—¥å‡é”€é‡é€‰æ‹©å™¨é…ç½®")
                # å°è¯•ä½¿ç”¨é™çº§é€‰æ‹©å™¨
                daily_avg_selector = '.store-daily-sales'
                self.logger.warning(f"ä½¿ç”¨é™çº§æ—¥å‡é”€é‡é€‰æ‹©å™¨: {daily_avg_selector}")

            # ç­‰å¾…å…ƒç´ åŠ è½½ï¼ˆæœ€å¤š15ç§’ï¼‰
            if not self.wait_for_element(daily_avg_selector, timeout=15.0):
                self.logger.warning("âš ï¸ æ—¥å‡é”€é‡å…ƒç´ ç­‰å¾…è¶…æ—¶ï¼Œå°è¯•ç›´æ¥æå–")

            # æå–æ—¥å‡é”€é‡æ–‡æœ¬
            daily_avg_text = self.get_text_content(daily_avg_selector, timeout=5.0)
            self.logger.debug(f"æ—¥å‡é”€é‡å…ƒç´ æ–‡æœ¬å†…å®¹: '{daily_avg_text}'")
            if daily_avg_text and daily_avg_text.strip():
                # ä½¿ç”¨å·¥å…·ç±»æå–æ•°å­—
                daily_avg = ScraperUtils.extract_number_from_text(daily_avg_text.strip())
                self.logger.debug(f"æå–åˆ°çš„æ—¥å‡é”€é‡æ•°å­—: {daily_avg}")
                if daily_avg is not None:  # ä¸å†ä½¿ç”¨è¿‡äºä¸¥æ ¼çš„éªŒè¯
                    sales_data['daily_avg_sold'] = daily_avg
                    self.logger.debug(f"âœ… æ—¥å‡é”€é‡æå–æˆåŠŸ: {daily_avg}")
                    return
                else:
                    self.logger.warning(f"âš ï¸ æ— æ³•ä»æ–‡æœ¬ä¸­æå–æ—¥å‡é”€é‡æ•°å­—: '{daily_avg_text.strip()}'")
            else:
                self.logger.warning("âš ï¸ æœªèƒ½è·å–æ—¥å‡é”€é‡æ–‡æœ¬")

        except Exception as e:
            self.logger.error(f"âŒ æ—¥å‡é”€é‡æå–å¤±è´¥: {e}", exc_info=True)

        # å¦‚æœç›´æ¥æå–å¤±è´¥ï¼Œå°è¯•æ ¹æ®å·²æœ‰æ•°æ®è®¡ç®—
        if 'sold_count_30days' in sales_data:
            try:
                daily_avg = sales_data['sold_count_30days'] / 30
                sales_data['daily_avg_sold'] = daily_avg
                self.logger.debug(f"âœ… æ—¥å‡é”€é‡è®¡ç®—æˆåŠŸ: {daily_avg}")
            except Exception as e:
                self.logger.error(f"âŒ æ—¥å‡é”€é‡è®¡ç®—å¤±è´¥: {e}")
        else:
            self.logger.warning("âš ï¸ æ— æ³•è·å–æˆ–è®¡ç®—æ—¥å‡é”€é‡")

    def _resolve_ozon_url(self, ozon_url: str) -> str:
        """
        è§£æ OZON URLï¼Œå¤„ç†å¯èƒ½çš„é‡å®šå‘ - åŒæ­¥å®ç°

        Args:
            ozon_url: åŸå§‹ OZON URL

        Returns:
            str: æœ€ç»ˆçš„ OZON URL
        """
        try:
            # å®Œæ•´å®ç°ï¼šä½¿ç”¨HTTPè¯·æ±‚è§£æURLé‡å®šå‘ï¼Œé¿å…å½±å“å½“å‰æµè§ˆå™¨çŠ¶æ€
            import requests
            from urllib.parse import urlparse

            self.logger.debug(f"å¼€å§‹è§£æURLé‡å®šå‘: {ozon_url}")

            # è®¾ç½®è¯·æ±‚å¤´ï¼Œæ¨¡æ‹ŸçœŸå®æµè§ˆå™¨
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
            }

            # å‘é€HEADè¯·æ±‚æ£€æŸ¥é‡å®šå‘ï¼Œè¶…æ—¶3ç§’
            response = requests.head(
                ozon_url,
                headers=headers,
                allow_redirects=True,
                timeout=3
            )

            # è·å–æœ€ç»ˆURL
            final_url = response.url

            if final_url != ozon_url:
                self.logger.info(f"URLé‡å®šå‘è§£æ: {ozon_url} -> {final_url}")
            else:
                self.logger.debug(f"URLæ— é‡å®šå‘: {ozon_url}")

            return final_url

        except requests.exceptions.Timeout:
            self.logger.warning(f"URLé‡å®šå‘æ£€æŸ¥è¶…æ—¶ï¼Œä½¿ç”¨åŸå§‹URL: {ozon_url}")
            return ozon_url
        except requests.exceptions.RequestException as e:
            self.logger.warning(f"URLé‡å®šå‘æ£€æŸ¥å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹URL: {e}")
            return ozon_url
        except Exception as e:
            self.logger.warning(f"URLå¤„ç†å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹URL: {e}")
            return ozon_url

    def _fetch_ozon_details(self, ozon_url: str) -> Optional[Dict[str, Any]]:
        """
        æŠ“å– OZON è¯¦æƒ…é¡µæ•°æ® - åŒæ­¥å®ç°

        Args:
            ozon_url: OZON å•†å“è¯¦æƒ…é¡µ URL

        Returns:
            Optional[Dict[str, Any]]: OZON è¯¦æƒ…é¡µæ•°æ®ï¼ŒåŒ…å«ä»·æ ¼ã€è·Ÿå–åº—é“ºã€ERP æ•°æ®
        """
        self.logger.info("ğŸ“Š è°ƒç”¨ OzonScraper å¤„ç† OZON å•†å“è¯¦æƒ…é¡µï¼ˆåŒæ­¥å®ç°ï¼‰...")
        try:
            from .ozon_scraper import OzonScraper

            # åˆ›å»º OzonScraper å®ä¾‹å¹¶ä½¿ç”¨å…¬å…±æ¥å£ - åŒæ­¥è°ƒç”¨
            ozon_scraper = OzonScraper(self.config)
            ozon_result = ozon_scraper.scrape(ozon_url, include_competitors=True)

            # å¤„ç†æŠ“å–ç»“æœ
            if ozon_result.success:
                ozon_data = {}

                # æå–ä»·æ ¼æ•°æ®
                if 'price_data' in ozon_result.data:
                    ozon_data.update(ozon_result.data['price_data'])
                    self.logger.debug(f"âœ… ä»·æ ¼æ•°æ®å·²æå–: {len(ozon_result.data['price_data'])}é¡¹")

                # æå–è·Ÿå–åº—é“ºæ•°æ®
                if 'competitors' in ozon_result.data:
                    ozon_data['competitors'] = ozon_result.data['competitors']
                    self.logger.debug(f"âœ… è·Ÿå–åº—é“ºæ•°æ®å·²æå–: {len(ozon_result.data['competitors'])}ä¸ª")

                # æå– ERP æ•°æ®
                if 'erp_data' in ozon_result.data:
                    ozon_data['erp_data'] = ozon_result.data['erp_data']
                    self.logger.debug("âœ… ERP æ•°æ®å·²æå–")

                self.logger.info(f"âœ… OZON æ•°æ®æå–å®Œæˆ: æ‰§è¡Œæ—¶é—´={ozon_result.execution_time:.2f}ç§’")
                return ozon_data
            else:
                self.logger.warning(f"âš ï¸ OZON æ•°æ®æå–å¤±è´¥: {ozon_result.error_message}")
                return None

        except Exception as scrape_error:
            self.logger.error(f"âŒ è°ƒç”¨ OzonScraper å¤±è´¥: {scrape_error}")
            return None

    def _validate_page(self) -> bool:
        """
        éªŒè¯å½“å‰é¡µé¢æ˜¯å¦æœ‰æ•ˆ - åŒæ­¥å®ç°

        Returns:
            bool: é¡µé¢æ˜¯å¦æœ‰æ•ˆ
        """
        if not self.browser_service:
            self.logger.error("âŒ browser_service æœªåˆå§‹åŒ–")
            return False

        # æ£€æŸ¥é¡µé¢æ˜¯å¦å¯ç”¨ï¼ˆé€šè¿‡æ£€æŸ¥pageå¯¹è±¡å’ŒåŸºæœ¬å±æ€§ï¼‰
        try:
            # æ–¹å¼1ï¼šæ£€æŸ¥pageå¯¹è±¡æ˜¯å¦å­˜åœ¨
            page = getattr(self.browser_service, 'page', None)
            if page is None:
                self.logger.warning("é¡µé¢å¯¹è±¡ä¸å­˜åœ¨")
                return False

            # æ–¹å¼2ï¼šå°è¯•è·å–é¡µé¢URLä½œä¸ºéªŒè¯
            if hasattr(self.browser_service, 'get_page_url_sync'):
                url = self.browser_service.get_page_url_sync()
                if url is not None:
                    self.logger.debug(f"é¡µé¢URLéªŒè¯æˆåŠŸ: {url}")
                    return True
                else:
                    # URLä¸ºNoneå¯èƒ½è¡¨ç¤ºé¡µé¢æœªå®Œå…¨åŠ è½½ï¼Œä½†æˆ‘ä»¬ä»è®¤ä¸ºé¡µé¢å­˜åœ¨
                    self.logger.debug("é¡µé¢URLä¸ºNoneï¼Œä½†pageå¯¹è±¡å­˜åœ¨ï¼Œå‡è®¾é¡µé¢æœ‰æ•ˆ")
                    return True
            else:
                # é™çº§æ–¹æ¡ˆï¼šæ£€æŸ¥pageå¯¹è±¡çš„åŸºæœ¬å±æ€§
                self.logger.warning("æµè§ˆå™¨æœåŠ¡æ²¡æœ‰åŒæ­¥è·å–é¡µé¢URLæ–¹æ³•ï¼Œä½¿ç”¨é™çº§éªŒè¯")
                # å¦‚æœèƒ½è·å–åˆ°pageå¯¹è±¡ï¼Œå‡è®¾é¡µé¢æœ‰æ•ˆ
                return True

        except Exception as e:
            self.logger.error(f"âŒ é¡µé¢éªŒè¯å¤±è´¥: {e}")
            return False

    def _deduplicate_rows(self, rows: list) -> list:
        """
        å»é‡å•†å“è¡Œï¼Œé¿å… CSS å’Œ XPath é€‰æ‹©å™¨åŒ¹é…åˆ°ç›¸åŒå…ƒç´ 

        ä½¿ç”¨ data-index å±æ€§è¿›è¡Œå»é‡ã€‚å¦‚æœå…ƒç´ æ²¡æœ‰ data-index å±æ€§ï¼Œ
        åˆ™ä¿ç•™è¯¥å…ƒç´ ã€‚

        Args:
            rows: å•†å“è¡Œå…ƒç´ åˆ—è¡¨

        Returns:
            list: å»é‡åçš„å•†å“è¡Œåˆ—è¡¨
        """
        seen_indices = set()
        unique_rows = []

        for row in rows:
            # ç®€åŒ–å»é‡é€»è¾‘ï¼šç›´æ¥æ·»åŠ æ‰€æœ‰è¡Œï¼Œå»é‡å·²åœ¨JavaScriptå±‚å¤„ç†
            unique_rows.append(row)

        return unique_rows

    def _extract_category(self, row_element) -> Dict[str, Optional[str]]:
        """ä»è¡Œå…ƒç´ ä¸­æå–ç±»ç›®ä¿¡æ¯ - ä½¿ç”¨é€šç”¨JavaScriptæ–¹æ³•"""
        result = {'category_cn': None, 'category_ru': None}

        # JavaScriptæ¨¡æ¿ - æå–ç±»ç›®ä¿¡æ¯
        js_template = """
        const categoryIndex = {category_index};
        const rows = document.querySelectorAll('tr[data-index]');
        
        for (let row of rows) {{
            const cells = row.querySelectorAll('td');
            if (cells.length > categoryIndex) {{
                const categoryCell = cells[categoryIndex];
                
                // æå–ä¸­æ–‡ç±»ç›®
                const categoryCnEl = categoryCell.querySelector('span.category-title, .category-title');
                const categoryCn = categoryCnEl ? categoryCnEl.textContent.trim() : null;
                
                // æå–ä¿„æ–‡ç±»ç›®  
                const categoryRuEl = categoryCell.querySelector('span.text-muted, .text-muted');
                const categoryRu = categoryRuEl ? categoryRuEl.textContent.trim() : null;
                
                if (categoryCn || categoryRu) {{
                    return {{
                        category_cn: categoryCn,
                        category_ru: categoryRu
                    }};
                }}
            }}
        }}
        return null;
        """

        category_data = self._extract_data_with_js(
            js_template,
            "ç±»ç›®ä¿¡æ¯",
            category_index=SEERFAR_SELECTORS.column_indexes['category']
        )

        if category_data:
            result.update(category_data)
            self.logger.debug(f"âœ… ç±»ç›®æå–æˆåŠŸ: {result}")
        else:
            self.logger.warning("âš ï¸ æœªèƒ½æå–åˆ°ç±»ç›®ä¿¡æ¯")

        return result

    def _extract_listing_date(self, row_element) -> Dict[str, Optional[str]]:
        """
        ä»è¡Œå…ƒç´ ä¸­æå–ä¸Šæ¶æ—¶é—´ä¿¡æ¯

        Args:
            row_element: Playwright è¡Œå…ƒç´ 

        Returns:
            Dict[str, Optional[str]]: åŒ…å« listing_date å’Œ shelf_duration çš„å­—å…¸
        """
        result = {'listing_date': None, 'shelf_duration': None}

        try:
            # ä½¿ç”¨JavaScriptç›´æ¥æå–ä¸Šæ¶æ—¶é—´ä¿¡æ¯
            js_script = """
            const rows = document.querySelectorAll('tr[data-index]');
            
            for (let row of rows) {
                const cells = row.querySelectorAll('td');
                if (cells.length > 0) {
                    const lastCell = cells[cells.length - 1]; // æœ€åä¸€ä¸ªtd
                    const innerHtml = lastCell.innerHTML;
                    
                    // æå–æ—¥æœŸï¼ˆåŒ¹é… YYYY-MM-DD æ ¼å¼ï¼‰
                    const dateMatch = innerHtml.match(/(\\d{4}-\\d{2}-\\d{2})/);
                    const date = dateMatch ? dateMatch[1] : null;
                    
                    // æå–è´§æ¶æ—¶é•¿ï¼ˆåŒ¹é…æ•°å­—+å¤©/æœˆç­‰ï¼‰
                    const durationMatch = innerHtml.match(/>\\s*([^<>]*(?:å¤©|æœˆ|å¹´|day|month|year)[^<>]*)/i);
                    let duration = durationMatch ? durationMatch[1].trim() : null;
                    
                    if (duration === '') duration = null;
                    
                    if (date || duration) {
                        return {
                            listing_date: date,
                            shelf_duration: duration
                        };
                    }
                }
            }
            return null;
            """

            date_data = self.browser_service.evaluate_sync(js_script)
            if date_data:
                if date_data.get('listing_date'):
                    result['listing_date'] = date_data['listing_date']
                if date_data.get('shelf_duration'):
                    result['shelf_duration'] = date_data['shelf_duration']

                self.logger.debug(f"âœ… ä¸Šæ¶æ—¶é—´æå–æˆåŠŸ: {result}")
            else:
                self.logger.warning("âš ï¸ æœªèƒ½æå–åˆ°ä¸Šæ¶æ—¶é—´ä¿¡æ¯")
                return result

            # ä¸‹é¢çš„æ­£åˆ™å¤„ç†é€»è¾‘å·²ç»åœ¨JavaScriptä¸­å®Œæˆï¼Œåˆ é™¤
            return result



        except Exception as e:
            self.logger.error(f"âŒ ä¸Šæ¶æ—¶é—´æå–å¤±è´¥: {e}")

        return result

    def _extract_product_sales_volume(self, row_element) -> Optional[int]:
        """
        ä»è¡Œå…ƒç´ ä¸­æå–å•†å“é”€é‡ä¿¡æ¯

        Args:
            row_element: Playwright è¡Œå…ƒç´ 

        Returns:
            Optional[int]: é”€é‡æ•°å€¼ï¼Œå¦‚æœæå–å¤±è´¥è¿”å› None
        """
        # ğŸ”§ ä»é…ç½®æ–‡ä»¶è·å–åˆ—ç´¢å¼•
        sales_volume_column_index = SEERFAR_SELECTORS.column_indexes['sales_volume']

        try:
            # ä½¿ç”¨JavaScriptç›´æ¥æå–é”€é‡ä¿¡æ¯
            js_script = f"""
            const salesIndex = {sales_volume_column_index};
            const rows = document.querySelectorAll('tr[data-index]');
            
            for (let row of rows) {{
                const cells = row.querySelectorAll('td');
                if (cells.length > salesIndex) {{
                    const salesCell = cells[salesIndex];
                    const salesText = salesCell.textContent || '';
                    
                    if (salesText.trim()) {{
                        // æå–ç¬¬ä¸€è¡Œçš„æ•°å­—ï¼ˆå¿½ç•¥å¢é•¿ç‡ï¼‰
                        const lines = salesText.trim().split('\\n');
                        if (lines.length > 0) {{
                            const firstLine = lines[0].trim();
                            // æå–çº¯æ•°å­—
                            const salesMatch = firstLine.match(/\\d+/);
                            if (salesMatch) {{
                                return parseInt(salesMatch[0], 10);
                            }}
                        }}
                    }}
                }}
            }}
            return null;
            """

            sales_volume = self.browser_service.evaluate_sync(js_script)
            if sales_volume is not None:
                self.logger.debug(f"âœ… é”€é‡æå–æˆåŠŸ: {sales_volume}")
                return sales_volume
            else:
                self.logger.warning("âš ï¸ æœªèƒ½æå–åˆ°é”€é‡ä¿¡æ¯")
                return None



        except Exception as e:
            self.logger.error(f"âŒ é”€é‡æå–å¤±è´¥: {e}")
            return None

    def _extract_weight(self, row_element) -> Optional[float]:
        """
        ä»è¡Œå…ƒç´ ä¸­æå–å•†å“é‡é‡ä¿¡æ¯

        Args:
            row_element: Playwright è¡Œå…ƒç´ 

        Returns:
            Optional[float]: é‡é‡æ•°å€¼ï¼ˆå…‹ï¼‰ï¼Œå¦‚æœæå–å¤±è´¥è¿”å› None
        """
        try:
            # ä½¿ç”¨JavaScriptç›´æ¥æå–é‡é‡ä¿¡æ¯
            js_script = """
            const rows = document.querySelectorAll('tr[data-index]');
            
            for (let row of rows) {
                const cells = row.querySelectorAll('td');
                if (cells.length >= 2) {
                    const weightCell = cells[cells.length - 2]; // å€’æ•°ç¬¬äºŒä¸ªtd
                    const weightText = weightCell.textContent || '';
                    
                    if (weightText.trim()) {
                        // æå–æ•°å­—å’Œå•ä½ï¼Œæ”¯æŒkgå’Œg
                        const weightMatch = weightText.match(/(\\d+(?:\\.\\d+)?)\\s*(kg|g)/i);
                        if (weightMatch) {
                            const value = parseFloat(weightMatch[1]);
                            const unit = weightMatch[2].toLowerCase();
                            
                            // ç»Ÿä¸€è½¬æ¢ä¸ºå…‹
                            const weightGrams = unit === 'kg' ? value * 1000 : value;
                            return weightGrams;
                        }
                    }
                }
            }
            return null;
            """

            weight_grams = self.browser_service.evaluate_sync(js_script)
            if weight_grams is not None:
                self.logger.debug(f"âœ… é‡é‡æå–æˆåŠŸ: {weight_grams}g")
                return weight_grams
            else:
                self.logger.warning("âš ï¸ æœªèƒ½æå–åˆ°é‡é‡ä¿¡æ¯")
                return None



        except Exception as e:
            self.logger.error(f"âŒ é‡é‡æå–å¤±è´¥: {e}")
            return None

