"""
Seerfarå¹³å°æŠ“å–å™¨

è´Ÿè´£ä»Seerfarå¹³å°æŠ“å–OZONåº—é“ºçš„é”€å”®æ•°æ®å’Œå•†å“ä¿¡æ¯ã€‚
åŸºäºç°ä»£åŒ–çš„Playwrightæµè§ˆå™¨æœåŠ¡ã€‚
"""

import time
from typing import Dict, Any, List, Optional, Callable

from .base_scraper import BaseScraper
from .global_browser_singleton import get_global_browser_service
from common.models.scraping_result import ScrapingResult
from common.utils.wait_utils import WaitUtils
from common.utils.scraping_utils import ScrapingUtils
from common.utils.sales_data_utils import extract_sales_data_generic
from common.config.seerfar_selectors import SeerfarSelectors, get_seerfar_selector, SEERFAR_SELECTORS
# æ¥å£å¯¼å…¥å·²ç§»é™¤ï¼Œç›´æ¥ç»§æ‰¿BaseScraper


class SeerfarScraper(BaseScraper):
    """
    Seerfarå¹³å°æŠ“å–å™¨

    å®ç°IStoreScraperæ¥å£ï¼Œæä¾›æ ‡å‡†åŒ–çš„åº—é“ºæ•°æ®æŠ“å–åŠŸèƒ½
    """

    def __init__(self, selectors_config: Optional[SeerfarSelectors] = None):
        """åˆå§‹åŒ–SeerfaræŠ“å–å™¨"""
        super().__init__()
        import logging
        from common.config.base_config import get_config

        # ğŸ”§ é‡æ„ï¼šä½¿ç”¨æ–°çš„é…ç½®ç³»ç»Ÿ
        self.selectors_config = selectors_config or SEERFAR_SELECTORS
        # ğŸ”§ ä¿®å¤ï¼šconfigåº”è¯¥æ˜¯ç³»ç»Ÿé…ç½®å¯¹è±¡ï¼ŒåŒ…å«dryrunç­‰ç³»ç»Ÿçº§å‚æ•°
        self.config = get_config()  # è·å–åŒ…å«dryrunå±æ€§çš„ç³»ç»Ÿé…ç½®
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")

        # ğŸ”§ é‡æ„ï¼šä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„URLé…ç½®ï¼ˆç¬¦åˆæ¶æ„åˆ†ç¦»åŸåˆ™ï¼‰
        self.base_url = self.config.browser.seerfar_base_url
        self.store_detail_path = self.config.browser.seerfar_store_detail_path

        # ä½¿ç”¨å…¨å±€æµè§ˆå™¨æœåŠ¡
        self.browser_service = get_global_browser_service()
        
        # ğŸ”§ é‡æ„ï¼šåˆå§‹åŒ–ç»Ÿä¸€å·¥å…·ç±»
        self.wait_utils = WaitUtils(self.browser_service, self.logger)
        self.scraping_utils = ScrapingUtils(self.logger)

    def scrape_store_sales_data(self,
                               store_id: str,
                               period_days: int = 30,
                               options: Optional[Dict[str, Any]] = None) -> ScrapingResult:
        """
        æŠ“å–åº—é“ºé”€å”®æ•°æ®ï¼ˆæ ‡å‡†æ¥å£å®ç°ï¼‰

        Args:
            store_id: åº—é“ºID
            period_days: ç»Ÿè®¡å¤©æ•°ï¼Œé»˜è®¤30å¤©
            options: æŠ“å–é€‰é¡¹ï¼Œå¯åŒ…å«store_filter_funcç­‰é…ç½®

        Returns:
            ScrapingResult: æŠ“å–ç»“æœï¼ŒåŒ…å«é”€å”®æ•°æ®

        Raises:
            NavigationException: é¡µé¢å¯¼èˆªå¤±è´¥
            DataExtractionException: æ•°æ®æå–å¤±è´¥
        """
        # æ„å»ºåº—é“ºè¯¦æƒ…é¡µURL
        url = f"{self.base_url}{self.store_detail_path}?storeId={store_id}&platform=OZON"

        # dryrunæ¨¡å¼ä¸‹è®°å½•å…¥å‚ï¼Œä½†ä»æ‰§è¡ŒçœŸå®çš„æŠ“å–æµç¨‹
        if self.config.dryrun:
            self.logger.info(f"ğŸ§ª è¯•è¿è¡Œæ¨¡å¼ - Seerfaråº—é“ºé”€å”®æ•°æ®æŠ“å–å…¥å‚: åº—é“ºID={store_id}, URL={url}")
            self.logger.info("ğŸ§ª è¯•è¿è¡Œæ¨¡å¼ - æ‰§è¡ŒçœŸå®çš„é”€å”®æ•°æ®æŠ“å–æµç¨‹ï¼ˆç»“æœä¸ä¼šä¿å­˜åˆ°æ–‡ä»¶ï¼‰")

        # ä½¿ç”¨ç»§æ‰¿çš„æŠ“å–æ–¹æ³•
        result = self.scrape_page_data(url, self._extract_sales_data)

        # ä»é€‰é¡¹ä¸­è·å–è¿‡æ»¤å‡½æ•°å¹¶åº”ç”¨è¿‡æ»¤
        # æ³¨æ„ï¼šéœ€è¦å°†å­—æ®µåè½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼
        store_filter_func = options.get('store_filter_func') if options else None
        if result.success and store_filter_func and result.data:
            filter_data = {
                'store_sales_30days': result.data.get('sold_30days', 0),
                'store_orders_30days': result.data.get('sold_count_30days', 0)
            }
            if not store_filter_func(filter_data):
                self.logger.info(f"åº—é“º{store_id}ä¸ç¬¦åˆç­›é€‰æ¡ä»¶")
                return ScrapingResult(
                    success=False,
                    data={},
                    error_message="åº—é“ºä¸ç¬¦åˆç­›é€‰æ¡ä»¶",
                    execution_time=result.execution_time
                )

        return result



    def scrape(
        self,
        store_id: str,
        include_products: bool = True,
        max_products: Optional[int] = None,
        product_filter_func: Optional[Callable[[Dict[str, Any]], bool]] = None,
        store_filter_func: Optional[Callable[[Dict[str, Any]], bool]] = None,
        **kwargs
    ) -> ScrapingResult:
        """
        ç»Ÿä¸€çš„åº—é“ºæŠ“å–æ¥å£ï¼ˆæ•´åˆé”€å”®æ•°æ®å’Œå•†å“æŠ“å–ï¼‰

        Args:
            store_id: åº—é“ºID
            include_products: æ˜¯å¦åŒ…å«å•†å“ä¿¡æ¯ï¼Œé»˜è®¤ True
            max_products: æœ€å¤§æŠ“å–å•†å“æ•°é‡ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„å€¼
            product_filter_func: å•†å“è¿‡æ»¤å‡½æ•°ï¼Œæ¥å—å•†å“æ•°æ®å­—å…¸ï¼Œè¿”å›å¸ƒå°”å€¼
            store_filter_func: åº—é“ºè¿‡æ»¤å‡½æ•°ï¼Œæ¥å—é”€å”®æ•°æ®å­—å…¸ï¼Œè¿”å›å¸ƒå°”å€¼
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            ScrapingResult: æŠ“å–ç»“æœï¼ŒåŒ…å«é”€å”®æ•°æ®å’Œå•†å“åˆ—è¡¨

        ä½¿ç”¨åœºæ™¯ï¼š
            1. åªè·å–é”€å”®æ•°æ®ï¼šscrape(store_id, include_products=False)
            2. è·å–å®Œæ•´ä¿¡æ¯ï¼šscrape(store_id, include_products=True)
            3. å¸¦è¿‡æ»¤çš„æŠ“å–ï¼šscrape(store_id, store_filter_func=..., product_filter_func=...)
        """
        start_time = time.time()

        try:
            # 1. æŠ“å–é”€å”®æ•°æ®
            sales_result = self.scrape_store_sales_data(store_id)
            if not sales_result.success:
                return sales_result

            result_data = {
                'store_id': store_id,
                'sales_data': sales_result.data
            }

            # 2. åº”ç”¨åº—é“ºè¿‡æ»¤ï¼ˆFilterManagerå·²ç»Ÿä¸€å¤„ç†å­—æ®µåå…¼å®¹æ€§ï¼‰
            if store_filter_func:
                if not store_filter_func(sales_result.data):
                    self.logger.info(f"åº—é“º{store_id}æœªé€šè¿‡åº—é“ºè¿‡æ»¤æ¡ä»¶ï¼Œè·³è¿‡å•†å“æŠ“å–")
                    return ScrapingResult(
                        success=False,
                        data=result_data,
                        error_message="åº—é“ºæœªé€šè¿‡è¿‡æ»¤æ¡ä»¶",
                        execution_time=time.time() - start_time
                    )

            # 3. å¦‚æœéœ€è¦ï¼ŒæŠ“å–å•†å“ä¿¡æ¯
            if include_products:
                # ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤å€¼æˆ–ä¼ å…¥çš„å€¼
                max_products = max_products or self.config.store_filter.max_products_to_check

                # æ„å»ºåº—é“ºè¯¦æƒ…é¡µURL
                url = f"{self.base_url}{self.store_detail_path}?storeId={store_id}&platform=OZON"

                # dryrunæ¨¡å¼ä¸‹è®°å½•å…¥å‚
                if self.config.dryrun:
                    self.logger.info(f"ğŸ§ª è¯•è¿è¡Œæ¨¡å¼ - Seerfaråº—é“ºå•†å“æŠ“å–å…¥å‚: åº—é“ºID={store_id}, "
                                     f"æœ€å¤§å•†å“æ•°={max_products}, URL={url}")
                    self.logger.info("ğŸ§ª è¯•è¿è¡Œæ¨¡å¼ - æ‰§è¡ŒçœŸå®çš„å•†å“æŠ“å–æµç¨‹ï¼ˆç»“æœä¸ä¼šä¿å­˜åˆ°æ–‡ä»¶ï¼‰")

                # åˆ›å»ºæå–å‡½æ•°
                def extract_products(browser_service):
                    products = self._extract_products_list(
                        max_products,
                        product_filter_func
                    )
                    return {'products': products, 'total_count': len(products)}

                # ä½¿ç”¨ç»§æ‰¿çš„æŠ“å–æ–¹æ³•
                products_result = self.scrape_page_data(url, extract_products)

                if products_result.success:
                    result_data['products'] = products_result.data['products']
                else:
                    self.logger.warning(f"æŠ“å–åº—é“º{store_id}å•†å“ä¿¡æ¯å¤±è´¥: {products_result.error_message}")
                    result_data['products'] = []

            return ScrapingResult(
                success=True,
                data=result_data,
                execution_time=time.time() - start_time
            )

        except Exception as e:
            self.logger.error(f"ç»¼åˆæŠ“å–åº—é“º{store_id}ä¿¡æ¯å¤±è´¥: {e}")
            return ScrapingResult(
                success=False,
                data={},
                error_message=str(e),
                execution_time=time.time() - start_time
            )

    def _extract_sales_data(self, browser_service) -> Dict[str, Any]:
        """åŒæ­¥æå–é”€å”®æ•°æ® - ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é€‰æ‹©å™¨"""
        sales_data = {}
        try:
            page = browser_service.page
            if not self._validate_page():
                return {'sold_30days': 0, 'sold_count_30days': 0, 'daily_avg_sold': 0}

            self._extract_all_sales_data(page, sales_data)

            if not sales_data:
                sales_data = {'sold_30days': 0, 'sold_count_30days': 0, 'daily_avg_sold': 0}

            return sales_data
        except Exception as e:
            self.logger.error(f"æå–é”€å”®æ•°æ®å¤±è´¥: {e}")
            return {'sold_30days': 0, 'sold_count_30days': 0, 'daily_avg_sold': 0}

    def _extract_category_data(self, page, sales_data: Dict[str, Any]):
        """æå–ç±»ç›®æ•°æ® - ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é€‰æ‹©å™¨"""





    def _extract_all_products_data_js(self, product_rows_selector: str) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨ JavaScript evaluate ä¸€æ¬¡æ€§æå–æ‰€æœ‰å•†å“è¡Œæ•°æ® - ä½¿ç”¨ä¸“é—¨çš„seerfaræå–è„šæœ¬
        """
        try:
            # ğŸ”§ è°ƒè¯•ï¼šå¢åŠ è¯¦ç»†çš„è°ƒè¯•æ—¥å¿—
            self.logger.info(f"ğŸ”§ DEBUG: å¼€å§‹JavaScriptæå–ï¼Œé€‰æ‹©å™¨: {product_rows_selector}")

            # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨ä¸“é—¨çš„seerfaræå–è„šæœ¬ï¼ŒåŒ…å«å®Œæ•´çš„OZON URLæå–é€»è¾‘
            js_script = SEERFAR_SELECTORS.js_scripts['extract_products']
            self.logger.info(f"ğŸ”§ DEBUG: JavaScriptè„šæœ¬é•¿åº¦: {len(js_script)}")

            # ğŸ”§ éªŒè¯æµè§ˆå™¨æœåŠ¡çŠ¶æ€
            if not self.browser_service:
                self.logger.error("âŒ CRITICAL: browser_service ä¸º None")
                return []

            self.logger.info(f"ğŸ”§ DEBUG: æµè§ˆå™¨æœåŠ¡ç±»å‹: {type(self.browser_service)}")

            # ğŸ”§ æ¶æ„é‡æ„ï¼šé€šè¿‡scraping_utilsç»Ÿä¸€æ‰§è¡ŒJavaScript
            # ğŸ”§ ä¿®å¤ï¼šæ”¯æŒå‚æ•°ä¼ é€’ï¼Œå°†é€‰æ‹©å™¨ä½œä¸ºå‚æ•°ä¼ é€’ç»™JavaScriptè„šæœ¬
            self.logger.info("ğŸ”§ DEBUG: è°ƒç”¨ extract_data_with_js...")

            products_data = self.scraping_utils.extract_data_with_js(
                self.browser_service,
                js_script,
                "å•†å“åˆ—è¡¨æ•°æ®",
                product_rows_selector  # ä¼ é€’é€‰æ‹©å™¨å‚æ•°
            )

            self.logger.info(f"ğŸ”§ DEBUG: extract_data_with_js è¿”å›ç»“æœç±»å‹: {type(products_data)}")
            self.logger.info(f"ğŸ”§ DEBUG: extract_data_with_js è¿”å›ç»“æœ: {products_data}")

            if products_data:
                self.logger.info(f"ğŸ“‹ JavaScript æå–åˆ° {len(products_data)} ä¸ªå•†å“è¡Œ")
                return products_data
            else:
                self.logger.warning("âš ï¸ CRITICAL: products_data ä¸ºç©ºæˆ– None")
                return []

        except Exception as e:
            self.logger.error(f"âŒ CRITICAL: JavaScript æå–å•†å“æ•°æ®å¤±è´¥: {e}")
            import traceback
            self.logger.error(f"âŒ å¼‚å¸¸è¯¦æƒ…:\n{traceback.format_exc()}")
            return []

    def _extract_products_list(self, max_products: int,
                              product_filter_func: Optional[Callable[[Dict[str, Any]], bool]] = None) -> List[Dict[str, Any]]:
        """
        æå–å•†å“åˆ—è¡¨ - åŒæ­¥å®ç°

        é‡æ„çš„åŒæ­¥ç‰ˆæœ¬ï¼Œæ”¯æŒå‰ç½®è¿‡æ»¤ï¼Œæ¶ˆé™¤å¼‚æ­¥å¤æ‚æ€§ã€‚
        ä½¿ç”¨JavaScriptä¸€æ¬¡æ€§æå–æ‰€æœ‰å•†å“æ•°æ®ï¼Œé¿å…é€ä¸ªå…ƒç´ æŸ¥è¯¢çš„æ€§èƒ½é—®é¢˜ã€‚

        Args:
            max_products: æœ€å¤§å•†å“æ•°é‡
            product_filter_func: å•†å“è¿‡æ»¤å‡½æ•°ï¼Œç”¨äºå‰ç½®è¿‡æ»¤

        Returns:
            List[Dict[str, Any]]: å•†å“åˆ—è¡¨
        """
        products = []
        filtered_count = 0

        try:
            self.logger.info(f"å¼€å§‹æå–å•†å“åˆ—è¡¨ï¼ˆåŒæ­¥å®ç°ï¼Œæœ€å¤š {max_products} ä¸ªï¼‰")

            # ä»é…ç½®æ–‡ä»¶è·å–å•†å“åˆ—è¡¨é€‰æ‹©å™¨
            product_rows_selector = get_seerfar_selector('product_list', 'product_rows')
            product_rows_alt_selector = get_seerfar_selector('product_list', 'product_rows_alt')

            if not product_rows_selector or not product_rows_alt_selector:
                self.logger.error("âŒ æœªèƒ½æ‰¾åˆ°å•†å“åˆ—è¡¨é€‰æ‹©å™¨é…ç½®")
                return []

            # ä½¿ç”¨JavaScriptä¸€æ¬¡æ€§æå–æ‰€æœ‰å•†å“æ•°æ®
            products_data = self._extract_all_products_data_js(product_rows_selector)

            if not products_data:
                self.logger.warning("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•å•†å“è¡Œ")
                return []

            total_rows = len(products_data)
            self.logger.info(f"ğŸ“‹ æ‰¾åˆ° {total_rows} ä¸ªå•†å“è¡Œï¼Œå¼€å§‹å¤„ç†ï¼ˆæœ€å¤š {max_products} ä¸ªï¼‰")

            # éå†æå–çš„å•†å“æ•°æ®
            for i in range(min(total_rows, max_products)):
                try:
                    product_data_js = products_data[i]

                    # æ„å»ºåŸºç¡€å•†å“æ•°æ®ç”¨äºå‰ç½®è¿‡æ»¤
                    basic_product_data = {
                        'product_category_cn': product_data_js.get('categoryCn'),
                        'product_category_ru': product_data_js.get('categoryRu'),
                        'product_listing_date': product_data_js.get('listingDate'),
                        'product_shelf_duration': product_data_js.get('shelfDuration'),
                        'product_sales_volume': product_data_js.get('salesVolume'),
                        'product_weight': product_data_js.get('weight')
                    }

                    # åº”ç”¨å‰ç½®è¿‡æ»¤
                    if product_filter_func:
                        if not product_filter_func(basic_product_data):
                            filtered_count += 1
                            self.logger.debug(f"â­ï¸  å•†å“ #{i+1} æœªé€šè¿‡å‰ç½®è¿‡æ»¤ï¼Œè·³è¿‡ OZON è¯¦æƒ…é¡µå¤„ç†")
                            continue

                    # æ„å»ºå®Œæ•´å•†å“æ•°æ®
                    product_data = {
                        'category_cn': product_data_js.get('categoryCn'),
                        'category_ru': product_data_js.get('categoryRu'),
                        'listing_date': product_data_js.get('listingDate'),
                        'shelf_duration': product_data_js.get('shelfDuration'),
                        'sales_volume': product_data_js.get('salesVolume'),
                        'weight': product_data_js.get('weight')
                    }

                    # è·å– OZON URL
                    ozon_url = product_data_js.get('ozonUrl')
                    ozon_data_success = False
                    if ozon_url:
                        self.logger.info(f"ğŸ“ æå–åˆ° OZON URL: {ozon_url}")

                        # æŠ“å– OZON è¯¦æƒ…é¡µæ•°æ® - åŒæ­¥å®ç°
                        ozon_data = self._fetch_ozon_details(ozon_url)
                        if ozon_data:
                            product_data.update(ozon_data)
                            ozon_data_success = True
                        else:
                            self.logger.warning(f"âš ï¸ å•†å“ #{i+1} OZON æ•°æ®è·å–å¤±è´¥")

                    if product_data:
                        products.append(product_data)
                        if ozon_data_success:
                            self.logger.info(f"âœ… å•†å“ #{i+1} æå–æˆåŠŸï¼ˆå« OZON æ•°æ®ï¼‰")
                        else:
                            self.logger.warning(f"âš ï¸ å•†å“ #{i+1} æå–éƒ¨åˆ†æˆåŠŸï¼ˆä»…åŸºç¡€æ•°æ®ï¼ŒOZON æ•°æ®ç¼ºå¤±ï¼‰")

                except Exception as e:
                    self.logger.warning(f"âš ï¸  æå–ç¬¬ {i + 1} ä¸ªå•†å“ä¿¡æ¯å¤±è´¥: {e}")
                    continue

            if products:
                self.logger.info(f"ğŸ‰ æˆåŠŸæå– {len(products)} ä¸ªæœ‰æ•ˆå•†å“ä¿¡æ¯ï¼ˆå‰ç½®è¿‡æ»¤è·³è¿‡ {filtered_count} ä¸ªï¼‰")
            else:
                self.logger.warning("âš ï¸  æœªæå–åˆ°æœ‰æ•ˆçš„å•†å“ä¿¡æ¯")
            return products

        except Exception as e:
            self.logger.error(f"âŒ æå–å•†å“åˆ—è¡¨å¤±è´¥: {e}")
            return []

    def close(self):
        """
        å…³é—­ SeerfarScraperï¼Œæ¸…ç†èµ„æº - åŒæ­¥å®ç°
        """
        try:
            super().close()  # è°ƒç”¨åŸºç±»çš„åŒæ­¥å…³é—­æ–¹æ³•
            self.logger.info("ğŸ”’ SeerfarScraper å·²å…³é—­")
        except Exception as e:
            self.logger.warning(f"å…³é—­ SeerfarScraper æ—¶å‡ºé”™: {e}")

    def _extract_basic_product_data(self, row_element) -> Dict[str, Any]:
        """
        æå– Seerfar è¡¨æ ¼ä¸­çš„åŸºç¡€å•†å“æ•°æ®

        Args:
            row_element: è¡Œå…ƒç´ 

        Returns:
            Dict[str, Any]: åŸºç¡€å•†å“æ•°æ®ï¼ˆç±»ç›®ã€ä¸Šæ¶æ—¶é—´ã€é”€é‡ã€é‡é‡ï¼‰
        """
        product_data = {}

        # 1. æå–ç±»ç›®ä¿¡æ¯
        category_data = self._extract_category(row_element)
        product_data.update(category_data)

        # 2. æå–ä¸Šæ¶æ—¶é—´
        listing_date_data = self._extract_listing_date(row_element)
        product_data.update(listing_date_data)

        # 3. æå–é”€é‡
        sales_volume = self._extract_product_sales_volume(row_element)
        if sales_volume is not None:
            product_data['sales_volume'] = sales_volume

        # 4. æå–é‡é‡
        weight = self._extract_weight(row_element)
        if weight is not None:
            product_data['weight'] = weight

        return product_data

    def _get_ozon_url_from_row(self, row_element) -> Optional[str]:
        """
        ä»è¡Œå…ƒç´ ä¸­æå– OZON URL

        Args:
            row_element: è¡Œå…ƒç´ 

        Returns:
            Optional[str]: OZON URLï¼Œå¦‚æœæå–å¤±è´¥è¿”å› None
        """
        try:
            # éªŒè¯é¡µé¢å¯¹è±¡
            if not self._validate_page():
                return None

            # ä»é…ç½®æ–‡ä»¶è·å–é€‰æ‹©å™¨
            third_column_selector = get_seerfar_selector('product_list', 'third_column')
            clickable_element_selector = get_seerfar_selector('product_list', 'clickable_element')
            clickable_element_alt_selector = get_seerfar_selector('product_list', 'clickable_element_alt')

            if not third_column_selector or not clickable_element_selector or not clickable_element_alt_selector:
                self.logger.error("âŒ æœªèƒ½æ‰¾åˆ°å•†å“è¡Œå…ƒç´ é€‰æ‹©å™¨é…ç½®")
                return None

            # ğŸ”§ æ¶æ„é‡æ„ï¼šé€šè¿‡scraping_utilsç»Ÿä¸€æ‰§è¡ŒJavaScript
            js_script = SEERFAR_SELECTORS.js_scripts['extract_ozon_url']
            ozon_url = self.scraping_utils.extract_data_with_js(
                self.browser_service, js_script, "OZON URL"
            )
            if not ozon_url:
                self.logger.warning("âš ï¸ æœªæ‰¾åˆ°OZON URL")
                return None

            self.logger.info(f"ğŸ”— æå–åˆ°OZON URL: {ozon_url}")

            # ç›´æ¥è¿”å›URLï¼Œä¸éœ€è¦è¿›ä¸€æ­¥è§£æ
            return ozon_url

        except Exception as e:
            self.logger.error(f"æå– OZON URL å¤±è´¥: {e}")
            return None



    def _extract_all_sales_data(self, page, sales_data: Dict[str, Any]):
        """ä¸€æ¬¡æ€§æå–æ‰€æœ‰é”€å”®æ•°æ® - åˆå¹¶é‡å¤çš„æå–é€»è¾‘"""
        extraction_configs = [
            ('store_sales_data', 'sales_amount', 'é”€å”®é¢', 'sold_30days', '.store-total-revenue', False),
            ('store_sales_data', 'sales_volume', 'é”€é‡', 'sold_count_30days', '.store-total-sales', True),
            ('store_sales_data', 'daily_avg_sales', 'æ—¥å‡é”€é‡', 'daily_avg_sold', '.store-daily-sales', False)
        ]

        for config_key, selector_key, desc, result_key, default_selector, is_int in extraction_configs:
            result = extract_sales_data_generic(
                self.browser_service, self.wait_utils, get_seerfar_selector,
                config_key, selector_key, desc, result_key,
                default_selector=default_selector, is_int=is_int,
                logger=self.logger
            )
            if result:
                sales_data.update(result)

        # æ—¥å‡é”€é‡è®¡ç®—åå¤‡æ–¹æ¡ˆ
        if 'daily_avg_sold' not in sales_data and 'sold_count_30days' in sales_data:
            try:
                sales_data['daily_avg_sold'] = sales_data['sold_count_30days'] / 30
                self.logger.debug(f"âœ… æ—¥å‡é”€é‡è®¡ç®—æˆåŠŸ: {sales_data['daily_avg_sold']}")
            except Exception as e:
                self.logger.error(f"âŒ æ—¥å‡é”€é‡è®¡ç®—å¤±è´¥: {e}")



    def _fetch_ozon_details(self, ozon_url: str) -> Optional[Dict[str, Any]]:
        """
        æŠ“å– OZON è¯¦æƒ…é¡µæ•°æ® - åŒæ­¥å®ç°

        Args:
            ozon_url: OZON å•†å“è¯¦æƒ…é¡µ URL

        Returns:
            Optional[Dict[str, Any]]: OZON è¯¦æƒ…é¡µæ•°æ®ï¼ŒåŒ…å«ä»·æ ¼ã€è·Ÿå–åº—é“ºã€ERP æ•°æ®
        """
        self.logger.info("ğŸ“Š è°ƒç”¨ OzonScraper å¤„ç† OZON å•†å“è¯¦æƒ…é¡µï¼ˆåŒæ­¥å®ç°ï¼‰...")
        try:
            from .ozon_scraper import OzonScraper

            # åˆ›å»º OzonScraper å®ä¾‹å¹¶ä½¿ç”¨å…¬å…±æ¥å£ - åŒæ­¥è°ƒç”¨
            ozon_scraper = OzonScraper(self.config)
            ozon_result = ozon_scraper.scrape(ozon_url, include_competitors=True)

            # å¤„ç†æŠ“å–ç»“æœ
            if ozon_result.success:
                ozon_data = {}

                # æå–ä»·æ ¼æ•°æ®
                if 'price_data' in ozon_result.data:
                    ozon_data.update(ozon_result.data['price_data'])
                    self.logger.debug(f"âœ… ä»·æ ¼æ•°æ®å·²æå–: {len(ozon_result.data['price_data'])}é¡¹")

                # æå–è·Ÿå–åº—é“ºæ•°æ®
                if 'competitors' in ozon_result.data:
                    ozon_data['competitors'] = ozon_result.data['competitors']
                    self.logger.debug(f"âœ… è·Ÿå–åº—é“ºæ•°æ®å·²æå–: {len(ozon_result.data['competitors'])}ä¸ª")

                # æå– ERP æ•°æ®
                if 'erp_data' in ozon_result.data:
                    ozon_data['erp_data'] = ozon_result.data['erp_data']
                    self.logger.debug("âœ… ERP æ•°æ®å·²æå–")

                self.logger.info(f"âœ… OZON æ•°æ®æå–å®Œæˆ: æ‰§è¡Œæ—¶é—´={ozon_result.execution_time:.2f}ç§’")
                return ozon_data
            else:
                self.logger.warning(f"âš ï¸ OZON æ•°æ®æå–å¤±è´¥: {ozon_result.error_message}")
                return None

        except Exception as scrape_error:
            # ğŸ”§ å¢å¼ºé”™è¯¯è°ƒè¯•ï¼šè·å–å®Œæ•´çš„å¼‚å¸¸ä¿¡æ¯
            import traceback
            self.logger.error(f"âŒ è°ƒç”¨ OzonScraper å¤±è´¥: {scrape_error}")
            self.logger.error(f"âŒ å¼‚å¸¸ç±»å‹: {type(scrape_error)}")
            self.logger.error(f"âŒ å®Œæ•´å¼‚å¸¸è¯¦æƒ…:\n{traceback.format_exc()}")
            return None

    def _validate_page(self) -> bool:
        """
        éªŒè¯å½“å‰é¡µé¢æ˜¯å¦æœ‰æ•ˆ - åŒæ­¥å®ç°

        Returns:
            bool: é¡µé¢æ˜¯å¦æœ‰æ•ˆ
        """
        if not self.browser_service:
            self.logger.error("âŒ browser_service æœªåˆå§‹åŒ–")
            return False

        # æ£€æŸ¥é¡µé¢æ˜¯å¦å¯ç”¨ï¼ˆé€šè¿‡æ£€æŸ¥pageå¯¹è±¡å’ŒåŸºæœ¬å±æ€§ï¼‰
        try:
            # æ–¹å¼1ï¼šæ£€æŸ¥pageå¯¹è±¡æ˜¯å¦å­˜åœ¨
            page = getattr(self.browser_service, 'page', None)
            if page is None:
                self.logger.warning("é¡µé¢å¯¹è±¡ä¸å­˜åœ¨")
                return False

            # æ–¹å¼2ï¼šå°è¯•è·å–é¡µé¢URLä½œä¸ºéªŒè¯
            if hasattr(self.browser_service, 'get_page_url_sync'):
                url = self.browser_service.get_page_url_sync()
                if url is not None:
                    self.logger.debug(f"é¡µé¢URLéªŒè¯æˆåŠŸ: {url}")
                    return True
                else:
                    # URLä¸ºNoneå¯èƒ½è¡¨ç¤ºé¡µé¢æœªå®Œå…¨åŠ è½½ï¼Œä½†æˆ‘ä»¬ä»è®¤ä¸ºé¡µé¢å­˜åœ¨
                    self.logger.debug("é¡µé¢URLä¸ºNoneï¼Œä½†pageå¯¹è±¡å­˜åœ¨ï¼Œå‡è®¾é¡µé¢æœ‰æ•ˆ")
                    return True
            else:
                # é™çº§æ–¹æ¡ˆï¼šæ£€æŸ¥pageå¯¹è±¡çš„åŸºæœ¬å±æ€§
                self.logger.warning("æµè§ˆå™¨æœåŠ¡æ²¡æœ‰åŒæ­¥è·å–é¡µé¢URLæ–¹æ³•ï¼Œä½¿ç”¨é™çº§éªŒè¯")
                # å¦‚æœèƒ½è·å–åˆ°pageå¯¹è±¡ï¼Œå‡è®¾é¡µé¢æœ‰æ•ˆ
                return True

        except Exception as e:
            self.logger.error(f"âŒ é¡µé¢éªŒè¯å¤±è´¥: {e}")
            return False



    def _extract_category(self, row_element) -> Dict[str, Optional[str]]:
        """ä»è¡Œå…ƒç´ ä¸­æå–ç±»ç›®ä¿¡æ¯"""
        # ğŸ”§ é‡æ„ï¼šä½¿ç”¨å¤–éƒ¨åŒ–çš„JavaScriptè„šæœ¬
        js_script = SEERFAR_SELECTORS.js_scripts['extract_category']
        category_index = SEERFAR_SELECTORS.column_indexes['category']

        try:
            # ğŸ”§ æ¶æ„é‡æ„ï¼šé€šè¿‡scraping_utilsç»Ÿä¸€æ‰§è¡ŒJavaScript
            result = self.scraping_utils.extract_data_with_js(
                self.browser_service,
                f"({js_script})({category_index})",
                "å•†å“ç±»ç›®"
            )
            return result or {'category_cn': None, 'category_ru': None}
        except Exception as e:
            self.logger.error(f"âŒ ç±»ç›®æå–å¤±è´¥: {e}")
            return {'category_cn': None, 'category_ru': None}

    def _extract_listing_date(self, row_element) -> Dict[str, Optional[str]]:
        """
        ä»è¡Œå…ƒç´ ä¸­æå–ä¸Šæ¶æ—¶é—´ä¿¡æ¯

        Args:
            row_element: Playwright è¡Œå…ƒç´ 

        Returns:
            Dict[str, Optional[str]]: åŒ…å« listing_date å’Œ shelf_duration çš„å­—å…¸
        """
        result = {'listing_date': None, 'shelf_duration': None}

        try:
            # ğŸ”§ æ¶æ„é‡æ„ï¼šé€šè¿‡scraping_utilsç»Ÿä¸€æ‰§è¡ŒJavaScript
            js_script = SEERFAR_SELECTORS.js_scripts['extract_listing_date']
            date_data = self.scraping_utils.extract_data_with_js(
                self.browser_service, js_script, "ä¸Šæ¶æ—¶é—´"
            )
            if date_data:
                if date_data.get('listing_date'):
                    result['listing_date'] = date_data['listing_date']
                if date_data.get('shelf_duration'):
                    result['shelf_duration'] = date_data['shelf_duration']

                self.logger.debug(f"âœ… ä¸Šæ¶æ—¶é—´æå–æˆåŠŸ: {result}")
            else:
                self.logger.warning("âš ï¸ æœªèƒ½æå–åˆ°ä¸Šæ¶æ—¶é—´ä¿¡æ¯")
                return result

            # ä¸‹é¢çš„æ­£åˆ™å¤„ç†é€»è¾‘å·²ç»åœ¨JavaScriptä¸­å®Œæˆï¼Œåˆ é™¤
            return result



        except Exception as e:
            self.logger.error(f"âŒ ä¸Šæ¶æ—¶é—´æå–å¤±è´¥: {e}")

        return result

    def _extract_product_sales_volume(self, row_element) -> Optional[int]:
        """
        ä»è¡Œå…ƒç´ ä¸­æå–å•†å“é”€é‡ä¿¡æ¯

        Args:
            row_element: Playwright è¡Œå…ƒç´ 

        Returns:
            Optional[int]: é”€é‡æ•°å€¼ï¼Œå¦‚æœæå–å¤±è´¥è¿”å› None
        """
        # ğŸ”§ ä»é…ç½®æ–‡ä»¶è·å–åˆ—ç´¢å¼•
        sales_volume_column_index = SEERFAR_SELECTORS.column_indexes['sales_volume']

        try:
            # ğŸ”§ æ¶æ„é‡æ„ï¼šé€šè¿‡scraping_utilsç»Ÿä¸€æ‰§è¡ŒJavaScript
            js_script = SEERFAR_SELECTORS.js_scripts['extract_sales_volume']
            # è°ƒç”¨å¤–éƒ¨è„šæœ¬ï¼Œä¼ å…¥é”€é‡åˆ—ç´¢å¼•ä½œä¸ºå‚æ•°
            sales_volume = self.scraping_utils.extract_data_with_js(
                self.browser_service,
                f"({js_script})({sales_volume_column_index})",
                "å•†å“é”€é‡"
            )
            if sales_volume is not None:
                self.logger.debug(f"âœ… é”€é‡æå–æˆåŠŸ: {sales_volume}")
                return sales_volume
            else:
                self.logger.warning("âš ï¸ æœªèƒ½æå–åˆ°é”€é‡ä¿¡æ¯")
                return None



        except Exception as e:
            self.logger.error(f"âŒ é”€é‡æå–å¤±è´¥: {e}")
            return None

    def _extract_weight(self, row_element) -> Optional[float]:
        """
        ä»è¡Œå…ƒç´ ä¸­æå–å•†å“é‡é‡ä¿¡æ¯

        Args:
            row_element: Playwright è¡Œå…ƒç´ 

        Returns:
            Optional[float]: é‡é‡æ•°å€¼ï¼ˆå…‹ï¼‰ï¼Œå¦‚æœæå–å¤±è´¥è¿”å› None
        """
        try:
            # ğŸ”§ æ¶æ„é‡æ„ï¼šé€šè¿‡scraping_utilsç»Ÿä¸€æ‰§è¡ŒJavaScript
            js_script = SEERFAR_SELECTORS.js_scripts['extract_weight']
            weight_grams = self.scraping_utils.extract_data_with_js(
                self.browser_service, js_script, "å•†å“é‡é‡"
            )
            if weight_grams is not None:
                self.logger.debug(f"âœ… é‡é‡æå–æˆåŠŸ: {weight_grams}g")
                return weight_grams
            else:
                self.logger.warning("âš ï¸ æœªèƒ½æå–åˆ°é‡é‡ä¿¡æ¯")
                return None



        except Exception as e:
            self.logger.error(f"âŒ é‡é‡æå–å¤±è´¥: {e}")
            return None

    def scrape_store_info(self,
                         store_id: str,
                         include_products: bool = True,
                         max_products: Optional[int] = None,
                         options: Optional[Dict[str, Any]] = None) -> ScrapingResult:
        """
        æŠ“å–åº—é“ºåŸºæœ¬ä¿¡æ¯ï¼ˆæ ‡å‡†æ¥å£å®ç°ï¼‰

        Args:
            store_id: åº—é“ºID
            include_products: æ˜¯å¦åŒ…å«å•†å“ä¿¡æ¯
            max_products: æœ€å¤§å•†å“æ•°é‡
            options: æŠ“å–é€‰é¡¹

        Returns:
            ScrapingResult: åº—é“ºä¿¡æ¯æŠ“å–ç»“æœ

        Raises:
            NavigationException: é¡µé¢å¯¼èˆªå¤±è´¥
            DataExtractionException: æ•°æ®æå–å¤±è´¥
        """
        try:
            # ç®€åŒ–å®ç°ï¼šç›´æ¥è°ƒç”¨æ ¸å¿ƒscrapeæ–¹æ³•ï¼Œé¿å…å¤æ‚ä¾èµ–
            return self.scrape(
                store_id=store_id,
                include_products=include_products,
                max_products=max_products,
                product_filter_func=options.get('product_filter_func') if options else None,
                store_filter_func=options.get('store_filter_func') if options else None
            )

        except Exception as e:
            return ScrapingResult(
                success=False,
                data={},
                error_message=f"åº—é“ºä¿¡æ¯æŠ“å–å¤±è´¥: {str(e)}"
            )

    # ========== æŠ½è±¡æ–¹æ³•å®ç° ==========

    def extract_data(self,
                    selectors: Optional[Dict[str, str]] = None,
                    options: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        ä»å½“å‰é¡µé¢æå–æ•°æ®ï¼ˆæŠ½è±¡æ–¹æ³•å®ç°ï¼‰

        Args:
            selectors: é€‰æ‹©å™¨æ˜ å°„
            options: æå–é€‰é¡¹

        Returns:
            Dict[str, Any]: æå–çš„æ•°æ®
        """
        try:
            # è·å–é¡µé¢å†…å®¹
            page_content = self.get_page_content()
            if not page_content:
                return {}

            # ä½¿ç”¨é»˜è®¤çš„é”€å”®æ•°æ®æå–é€»è¾‘
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(page_content, 'html.parser')

            extracted_data = {}

            # æå–é”€å”®é¢ã€é”€é‡ç­‰å…³é”®æŒ‡æ ‡
            sales_data = {}
            self._extract_sales_amount(None, sales_data)
            self._extract_sales_volume(None, sales_data)
            self._extract_daily_avg_sales(None, sales_data)

            extracted_data.update(sales_data)

            return extracted_data

        except Exception as e:
            self.logger.error(f"æ•°æ®æå–å¤±è´¥: {e}")
            return {}

    def validate_data(self, data: Dict[str, Any],
                     filters: Optional[List[Callable]] = None) -> bool:
        """
        éªŒè¯æå–çš„æ•°æ®ï¼ˆæŠ½è±¡æ–¹æ³•å®ç°ï¼‰

        Args:
            data: å¾…éªŒè¯çš„æ•°æ®
            filters: éªŒè¯è¿‡æ»¤å™¨åˆ—è¡¨

        Returns:
            bool: æ•°æ®æ˜¯å¦æœ‰æ•ˆ
        """
        try:
            # åŸºæœ¬éªŒè¯ï¼šæ•°æ®ä¸ä¸ºç©º
            if not data:
                return False

            # éªŒè¯å…³é”®å­—æ®µ
            required_fields = ['sold_30days', 'sold_count_30days']
            for field in required_fields:
                if field in data:
                    value = data[field]
                    if value is not None and value >= 0:
                        continue
                    else:
                        self.logger.warning(f"å­—æ®µ {field} å€¼æ— æ•ˆ: {value}")
                        return False

            # åº”ç”¨è‡ªå®šä¹‰è¿‡æ»¤å™¨
            if filters:
                for filter_func in filters:
                    if not filter_func(data):
                        return False

            return True

        except Exception as e:
            self.logger.error(f"æ•°æ®éªŒè¯å¤±è´¥: {e}")
            return False

    def get_health_status(self) -> Dict[str, Any]:
        """
        è·å–Scraperå¥åº·çŠ¶æ€ï¼ˆæŠ½è±¡æ–¹æ³•å®ç°ï¼‰

        Returns:
            Dict[str, Any]: å¥åº·çŠ¶æ€ä¿¡æ¯
        """
        try:
            status = {
                'scraper_name': 'SeerfarScraper',
                'status': 'healthy',
                'browser_service_available': self.browser_service is not None,
                'last_operation_time': getattr(self, '_last_operation_time', None),
                'total_operations': getattr(self, '_operation_count', 0)
            }

            # æ£€æŸ¥æµè§ˆå™¨æœåŠ¡çŠ¶æ€
            if self.browser_service:
                try:
                    # ğŸ”§ æ¶æ„é‡æ„ï¼šé€šè¿‡scraping_utilsç»Ÿä¸€æ‰§è¡ŒJavaScript
                    js_script = SEERFAR_SELECTORS.js_scripts['get_page_url']
                    page_url = self.scraping_utils.extract_data_with_js(
                        self.browser_service, js_script, "é¡µé¢URL"
                    )
                    status['browser_responsive'] = page_url is not None
                    status['current_url'] = page_url
                except:
                    status['browser_responsive'] = False
                    status['status'] = 'degraded'
            else:
                status['status'] = 'unavailable'
                status['browser_responsive'] = False

            return status

        except Exception as e:
            return {
                'scraper_name': 'SeerfarScraper',
                'status': 'error',
                'error': str(e)
            }