"""
Seerfarå¹³å°æŠ“å–å™¨

è´Ÿè´£ä»Seerfarå¹³å°æŠ“å–OZONåº—é“ºçš„é”€å”®æ•°æ®å’Œå•†å“ä¿¡æ¯ã€‚
åŸºäºç°ä»£åŒ–çš„Playwrightæµè§ˆå™¨æœåŠ¡ã€‚
"""

import time
import re
from typing import Dict, Any, List, Optional, Callable

from .base_scraper import BaseScraper
from .global_browser_singleton import get_global_browser_service
from .scraper_utils import ScraperUtils
from ..models import ScrapingResult
from common.config import GoodStoreSelectorConfig
from common.config.seerfar_selectors import get_seerfar_selector


class SeerfarScraper(BaseScraper):
    """Seerfarå¹³å°æŠ“å–å™¨"""

    def __init__(self, config: Optional[GoodStoreSelectorConfig] = None):
        """åˆå§‹åŒ–SeerfaræŠ“å–å™¨"""
        super().__init__()
        from common.config import get_config
        import logging

        self.config = config or get_config()
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        self.base_url = self.config.scraping.seerfar_base_url
        self.store_detail_path = self.config.scraping.seerfar_store_detail_path

        # ä½¿ç”¨å…¨å±€æµè§ˆå™¨æœåŠ¡
        self.browser_service = get_global_browser_service()

    def scrape_store_sales_data(self, store_id: str, store_filter_func=None) -> ScrapingResult:
        """
        æŠ“å–åº—é“ºé”€å”®æ•°æ®

        Args:
            store_id: åº—é“ºID
            store_filter_func: åº—é“ºè¿‡æ»¤å‡½æ•°ï¼Œç”¨äºç­›é€‰åº—é“ºï¼ˆæ£€æŸ¥é”€å”®é¢å’Œè®¢å•é‡ï¼‰

        Returns:
            ScrapingResult: æŠ“å–ç»“æœï¼ŒåŒ…å«é”€å”®æ•°æ®
        """
        # æ„å»ºåº—é“ºè¯¦æƒ…é¡µURL
        url = f"{self.base_url}{self.store_detail_path}?storeId={store_id}&platform=OZON"

        # dryrunæ¨¡å¼ä¸‹è®°å½•å…¥å‚ï¼Œä½†ä»æ‰§è¡ŒçœŸå®çš„æŠ“å–æµç¨‹
        if self.config.dryrun:
            self.logger.info(f"ğŸ§ª è¯•è¿è¡Œæ¨¡å¼ - Seerfaråº—é“ºé”€å”®æ•°æ®æŠ“å–å…¥å‚: åº—é“ºID={store_id}, URL={url}")
            self.logger.info("ğŸ§ª è¯•è¿è¡Œæ¨¡å¼ - æ‰§è¡ŒçœŸå®çš„é”€å”®æ•°æ®æŠ“å–æµç¨‹ï¼ˆç»“æœä¸ä¼šä¿å­˜åˆ°æ–‡ä»¶ï¼‰")

        # ä½¿ç”¨ç»§æ‰¿çš„æŠ“å–æ–¹æ³•
        result = self.scrape_page_data(url, self._extract_sales_data_async)

        # å¦‚æœæä¾›äº†è¿‡æ»¤å‡½æ•°ï¼Œåˆ™åº”ç”¨è¿‡æ»¤
        # æ³¨æ„ï¼šéœ€è¦å°†å­—æ®µåè½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼
        if result.success and store_filter_func and result.data:
            filter_data = {
                'store_sales_30days': result.data.get('sold_30days', 0),
                'store_orders_30days': result.data.get('sold_count_30days', 0)
            }
            if not store_filter_func(filter_data):
                self.logger.info(f"åº—é“º{store_id}ä¸ç¬¦åˆç­›é€‰æ¡ä»¶")
                return ScrapingResult(
                    success=False,
                    data={},
                    error_message="åº—é“ºä¸ç¬¦åˆç­›é€‰æ¡ä»¶",
                    execution_time=result.execution_time
                )

        return result



    def scrape(
        self,
        store_id: str,
        include_products: bool = True,
        max_products: Optional[int] = None,
        product_filter_func: Optional[Callable[[Dict[str, Any]], bool]] = None,
        store_filter_func: Optional[Callable[[Dict[str, Any]], bool]] = None,
        **kwargs
    ) -> ScrapingResult:
        """
        ç»Ÿä¸€çš„åº—é“ºæŠ“å–æ¥å£ï¼ˆæ•´åˆé”€å”®æ•°æ®å’Œå•†å“æŠ“å–ï¼‰

        Args:
            store_id: åº—é“ºID
            include_products: æ˜¯å¦åŒ…å«å•†å“ä¿¡æ¯ï¼Œé»˜è®¤ True
            max_products: æœ€å¤§æŠ“å–å•†å“æ•°é‡ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„å€¼
            product_filter_func: å•†å“è¿‡æ»¤å‡½æ•°ï¼Œæ¥å—å•†å“æ•°æ®å­—å…¸ï¼Œè¿”å›å¸ƒå°”å€¼
            store_filter_func: åº—é“ºè¿‡æ»¤å‡½æ•°ï¼Œæ¥å—é”€å”®æ•°æ®å­—å…¸ï¼Œè¿”å›å¸ƒå°”å€¼
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            ScrapingResult: æŠ“å–ç»“æœï¼ŒåŒ…å«é”€å”®æ•°æ®å’Œå•†å“åˆ—è¡¨

        ä½¿ç”¨åœºæ™¯ï¼š
            1. åªè·å–é”€å”®æ•°æ®ï¼šscrape(store_id, include_products=False)
            2. è·å–å®Œæ•´ä¿¡æ¯ï¼šscrape(store_id, include_products=True)
            3. å¸¦è¿‡æ»¤çš„æŠ“å–ï¼šscrape(store_id, store_filter_func=..., product_filter_func=...)
        """
        start_time = time.time()

        try:
            # 1. æŠ“å–é”€å”®æ•°æ®
            sales_result = self.scrape_store_sales_data(store_id)
            if not sales_result.success:
                return sales_result

            result_data = {
                'store_id': store_id,
                'sales_data': sales_result.data
            }

            # 2. åº”ç”¨åº—é“ºè¿‡æ»¤ï¼ˆå¦‚æœæä¾›ï¼‰
            if store_filter_func:
                # è½¬æ¢å­—æ®µåä»¥åŒ¹é…è¿‡æ»¤å‡½æ•°æœŸæœ›çš„æ ¼å¼
                filter_data = {
                    'store_sales_30days': sales_result.data.get('sold_30days', 0),
                    'store_orders_30days': sales_result.data.get('sold_count_30days', 0)
                }
                if not store_filter_func(filter_data):
                    self.logger.info(f"åº—é“º{store_id}æœªé€šè¿‡åº—é“ºè¿‡æ»¤æ¡ä»¶ï¼Œè·³è¿‡å•†å“æŠ“å–")
                    return ScrapingResult(
                        success=False,
                        data=result_data,
                        error_message="åº—é“ºæœªé€šè¿‡è¿‡æ»¤æ¡ä»¶",
                        execution_time=time.time() - start_time
                    )

            # 3. å¦‚æœéœ€è¦ï¼ŒæŠ“å–å•†å“ä¿¡æ¯
            if include_products:
                # ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤å€¼æˆ–ä¼ å…¥çš„å€¼
                max_products = max_products or self.config.store_filter.max_products_to_check

                # æ„å»ºåº—é“ºè¯¦æƒ…é¡µURL
                url = f"{self.base_url}{self.store_detail_path}?storeId={store_id}&platform=OZON"

                # dryrunæ¨¡å¼ä¸‹è®°å½•å…¥å‚
                if self.config.dryrun:
                    self.logger.info(f"ğŸ§ª è¯•è¿è¡Œæ¨¡å¼ - Seerfaråº—é“ºå•†å“æŠ“å–å…¥å‚: åº—é“ºID={store_id}, "
                                     f"æœ€å¤§å•†å“æ•°={max_products}, URL={url}")
                    self.logger.info("ğŸ§ª è¯•è¿è¡Œæ¨¡å¼ - æ‰§è¡ŒçœŸå®çš„å•†å“æŠ“å–æµç¨‹ï¼ˆç»“æœä¸ä¼šä¿å­˜åˆ°æ–‡ä»¶ï¼‰")

                # åˆ›å»ºæå–å‡½æ•°
                async def extract_products(browser_service):
                    products = await self._extract_products_list_async(
                        browser_service,
                        max_products,
                        product_filter_func
                    )
                    return {'products': products, 'total_count': len(products)}

                # ä½¿ç”¨ç»§æ‰¿çš„æŠ“å–æ–¹æ³•
                products_result = self.scrape_page_data(url, extract_products)

                if products_result.success:
                    result_data['products'] = products_result.data['products']
                else:
                    self.logger.warning(f"æŠ“å–åº—é“º{store_id}å•†å“ä¿¡æ¯å¤±è´¥: {products_result.error_message}")
                    result_data['products'] = []

            return ScrapingResult(
                success=True,
                data=result_data,
                execution_time=time.time() - start_time
            )

        except Exception as e:
            self.logger.error(f"ç»¼åˆæŠ“å–åº—é“º{store_id}ä¿¡æ¯å¤±è´¥: {e}")
            return ScrapingResult(
                success=False,
                data={},
                error_message=str(e),
                execution_time=time.time() - start_time
            )

    async def _extract_sales_data_async(self, browser_service) -> Dict[str, Any]:
        """
        å¼‚æ­¥æå–é”€å”®æ•°æ® - ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é€‰æ‹©å™¨

        Args:
            browser_service: æµè§ˆå™¨æœåŠ¡å®ä¾‹

        Returns:
            Dict[str, Any]: é”€å”®æ•°æ®
        """
        sales_data = {}

        try:
            # ç›´æ¥è®¿é—® page å¯¹è±¡
            page = browser_service.page

            # éªŒè¯ page å¯¹è±¡
            if not self._validate_page(page):
                return {}

            # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é€‰æ‹©å™¨æå–é”€å”®é¢
            await self._extract_sales_amount(page, sales_data)

            # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é€‰æ‹©å™¨æå–é”€é‡
            await self._extract_sales_volume(page, sales_data)

            # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é€‰æ‹©å™¨æå–æ—¥å‡é”€é‡
            await self._extract_daily_avg_sales(page, sales_data)

            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å…·ä½“å…ƒç´ ï¼Œå°è¯•é€šç”¨æ–¹æ³•
            if not sales_data:
                sales_data = await self._extract_sales_data_generic_async(page)

            # åˆå¹¶æ—¥å¿—è¾“å‡ºåº—é“ºæ•°æ®æ‘˜è¦
            if sales_data:
                sales_amount = sales_data.get('sold_30days', 0)
                sales_volume = sales_data.get('sold_count_30days', 0)
                daily_avg = sales_data.get('daily_avg_sold', 0)
                self.logger.info(
                    f"ğŸ“Š åº—é“ºæ•°æ®æå–å®Œæˆ - é”€å”®é¢: {sales_amount:.0f}â‚½, é”€é‡: {sales_volume}, æ—¥å‡: {daily_avg}")

            self.logger.debug(f"æå–çš„é”€å”®æ•°æ®: {sales_data}")
            return sales_data

        except Exception as e:
            self.logger.error(f"æå–é”€å”®æ•°æ®å¤±è´¥: {e}")
            return {}

    async def _extract_sales_amount(self, page, sales_data: Dict[str, Any]):
        """æå–é”€å”®é¢ - ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é€‰æ‹©å™¨"""
        try:
            # ä»é…ç½®æ–‡ä»¶è·å–é”€å”®é¢é€‰æ‹©å™¨
            sales_amount_selector = get_seerfar_selector('store_sales_data', 'sales_amount')
            if not sales_amount_selector:
                self.logger.error("âŒ æœªèƒ½æ‰¾åˆ°é”€å”®é¢é€‰æ‹©å™¨é…ç½®")
                return

            # ç­‰å¾…å…ƒç´ å‡ºç°
            try:
                await page.wait_for_selector(sales_amount_selector, timeout=5000)
            except:
                self.logger.debug("é”€å”®é¢å…ƒç´ ç­‰å¾…è¶…æ—¶ï¼Œç»§ç»­å°è¯•æå–")

            element = await page.query_selector(sales_amount_selector)
            if element:
                text = await element.text_content()
                if text and text.strip():
                    # æå–æ•°å­—å¹¶è½¬æ¢ä¸ºé”€å”®é¢
                    number = ScraperUtils.extract_number_from_text(text.strip())
                    if number:
                        sales_data['sold_30days'] = number
                        return

            self.logger.warning("âš ï¸ æœªèƒ½æå–åˆ°é”€å”®é¢æ•°æ®")

        except Exception as e:
            self.logger.error(f"âŒ é”€å”®é¢æå–å¤±è´¥: {str(e)}")

    async def _extract_sales_volume(self, page, sales_data: Dict[str, Any]):
        """æå–é”€é‡ - ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é€‰æ‹©å™¨"""
        try:
            # ä»é…ç½®æ–‡ä»¶è·å–é”€é‡é€‰æ‹©å™¨
            sales_volume_selector = get_seerfar_selector('store_sales_data', 'sales_volume')
            if not sales_volume_selector:
                self.logger.error("âŒ æœªèƒ½æ‰¾åˆ°é”€é‡é€‰æ‹©å™¨é…ç½®")
                return

            # ç­‰å¾…å…ƒç´ å‡ºç°
            try:
                await page.wait_for_selector(sales_volume_selector, timeout=5000)
            except:
                self.logger.debug("é”€é‡å…ƒç´ ç­‰å¾…è¶…æ—¶ï¼Œç»§ç»­å°è¯•æå–")

            element = await page.query_selector(sales_volume_selector)
            if element:
                text = await element.text_content()
                if text and text.strip():
                    # æå–æ•°å­—å¹¶è½¬æ¢ä¸ºé”€é‡
                    number = ScraperUtils.extract_number_from_text(text.strip())
                    if number:
                        sales_data['sold_count_30days'] = int(number)
                        return

            self.logger.warning("âš ï¸ æœªèƒ½æå–åˆ°é”€é‡æ•°æ®")

        except Exception as e:
            self.logger.error(f"âŒ é”€é‡æå–å¤±è´¥: {str(e)}")

    async def _extract_daily_avg_sales(self, page, sales_data: Dict[str, Any]):
        """æå–æ—¥å‡é”€é‡ - ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é€‰æ‹©å™¨"""
        try:
            # ä»é…ç½®æ–‡ä»¶è·å–æ—¥å‡é”€é‡é€‰æ‹©å™¨
            daily_avg_selector = get_seerfar_selector('store_sales_data', 'daily_avg_sales')
            if not daily_avg_selector:
                self.logger.error("âŒ æœªèƒ½æ‰¾åˆ°æ—¥å‡é”€é‡é€‰æ‹©å™¨é…ç½®")
                return

            # ç­‰å¾…å…ƒç´ å‡ºç°
            try:
                await page.wait_for_selector(daily_avg_selector, timeout=5000)
            except:
                self.logger.debug("æ—¥å‡é”€é‡å…ƒç´ ç­‰å¾…è¶…æ—¶ï¼Œç»§ç»­å°è¯•æå–")

            element = await page.query_selector(daily_avg_selector)
            if element:
                text = await element.text_content()
                if text and text.strip():
                    # æå–æ•°å­—å¹¶è½¬æ¢ä¸ºæ—¥å‡é”€é‡
                    number = ScraperUtils.extract_number_from_text(text.strip())
                    if number:
                        sales_data['daily_avg_sold'] = number
                        return

            self.logger.warning("âš ï¸ æœªèƒ½æå–åˆ°æ—¥å‡é”€é‡æ•°æ®")

        except Exception as e:
            self.logger.error(f"âŒ æ—¥å‡é”€é‡æå–å¤±è´¥: {str(e)}")

    async def _extract_category_data(self, page, sales_data: Dict[str, Any]):
        """æå–ç±»ç›®æ•°æ® - ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é€‰æ‹©å™¨"""
        try:
            # ä»é…ç½®æ–‡ä»¶è·å–ç±»ç›®æ•°æ®é€‰æ‹©å™¨
            category_xpath = get_seerfar_selector('store_sales_data', 'category_data')
            if not category_xpath:
                self.logger.debug("æœªé…ç½®ç±»ç›®æ•°æ®é€‰æ‹©å™¨ï¼Œè·³è¿‡ç±»ç›®æ•°æ®æå–")
                return

            # ç­‰å¾…å…ƒç´ å‡ºç°
            try:
                await page.wait_for_selector(f'xpath={category_xpath}', timeout=5000)
            except:
                self.logger.debug("ç±»ç›®æ•°æ®å…ƒç´ ç­‰å¾…è¶…æ—¶ï¼Œç»§ç»­å°è¯•æå–")

            element = await page.query_selector(f'xpath={category_xpath}')
            if element:
                text = await element.text_content()
                if text and text.strip():
                    sales_data['category_info'] = text.strip()
                    return

            self.logger.warning("âš ï¸ æœªèƒ½æå–åˆ°ç±»ç›®æ•°æ®")

        except Exception as e:
            self.logger.error(f"âŒ ç±»ç›®æ•°æ®æå–å¤±è´¥: {str(e)}")

    async def _extract_sales_data_generic_async(self, page) -> Dict[str, Any]:
        """
        å¼‚æ­¥é€šç”¨æ–¹æ³•æå–é”€å”®æ•°æ®
        
        Args:
            page: Playwrighté¡µé¢å¯¹è±¡
            
        Returns:
            Dict[str, Any]: é”€å”®æ•°æ®
        """
        sales_data = {}

        try:
            # æŸ¥æ‰¾æ‰€æœ‰åŒ…å«æ•°å­—çš„å…ƒç´ 
            number_elements = await page.query_selector_all(
                "//*[contains(text(), 'â‚½') or contains(text(), 'ä¸‡') or contains(text(), 'åƒ')]")

            for element in number_elements[:10]:  # é™åˆ¶æ£€æŸ¥å‰10ä¸ªå…ƒç´ 
                try:
                    text = await element.text_content()
                    if not text:
                        continue

                    # åˆ¤æ–­æ˜¯å¦ä¸ºé”€å”®é¢
                    if any(keyword in text for keyword in ['é”€å”®é¢', 'è¥ä¸šé¢', 'æ”¶å…¥', 'â‚½']):
                        number = ScraperUtils.extract_number_from_text(text)
                        if number and number > 1000:  # é”€å”®é¢é€šå¸¸è¾ƒå¤§
                            sales_data['sold_30days'] = number

                    # åˆ¤æ–­æ˜¯å¦ä¸ºé”€é‡
                    elif any(keyword in text for keyword in ['é”€é‡', 'è®¢å•', 'ä»¶æ•°']):
                        number = ScraperUtils.extract_number_from_text(text)
                        if number and 10 <= number <= 10000:  # é”€é‡é€šå¸¸åœ¨åˆç†èŒƒå›´å†…
                            sales_data['sold_count_30days'] = int(number)
                except Exception as e:
                    self.logger.debug(f"å¤„ç†å…ƒç´ æ–‡æœ¬å¤±è´¥: {e}")
                    continue

            # å¦‚æœæ‰¾åˆ°é”€å”®é¢å’Œé”€é‡ï¼Œè®¡ç®—æ—¥å‡é”€é‡
            if 'sold_30days' in sales_data and 'sold_count_30days' in sales_data:
                sales_data['daily_avg_sold'] = sales_data['sold_count_30days'] / 30

            return sales_data

        except Exception as e:
            self.logger.error(f"é€šç”¨æ–¹æ³•æå–é”€å”®æ•°æ®å¤±è´¥: {e}")
            return {}

    async def _extract_products_list_async(self, browser_service, max_products: int,
                                           product_filter_func: Optional[Callable[[Dict[str, Any]], bool]] = None) -> List[Dict[str, Any]]:
        """
        å¼‚æ­¥æå–å•†å“åˆ—è¡¨ï¼Œæ”¯æŒå‰ç½®è¿‡æ»¤

        Args:
            browser_service: æµè§ˆå™¨æœåŠ¡å®ä¾‹
            max_products: æœ€å¤§å•†å“æ•°é‡
            product_filter_func: å•†å“è¿‡æ»¤å‡½æ•°ï¼Œç”¨äºå‰ç½®è¿‡æ»¤ï¼ˆåœ¨æå– OZON è¯¦æƒ…å‰ï¼‰

        Returns:
            List[Dict[str, Any]]: å•†å“åˆ—è¡¨
        """
        products = []
        filtered_count = 0

        try:
            # ç›´æ¥è®¿é—® page å¯¹è±¡
            page = browser_service.page

            # éªŒè¯ page å¯¹è±¡
            if not self._validate_page(page):
                return []

            # ä»é…ç½®æ–‡ä»¶è·å–å•†å“åˆ—è¡¨é€‰æ‹©å™¨
            product_rows_selector = get_seerfar_selector('product_list', 'product_rows')
            product_rows_alt_selector = get_seerfar_selector('product_list', 'product_rows_alt')

            if not product_rows_selector or not product_rows_alt_selector:
                self.logger.error("âŒ æœªèƒ½æ‰¾åˆ°å•†å“åˆ—è¡¨é€‰æ‹©å™¨é…ç½®")
                return []

            # æŸ¥æ‰¾å•†å“è¡¨æ ¼æˆ–åˆ—è¡¨
            product_rows = await page.query_selector_all(product_rows_selector)

            if not product_rows:
                # å°è¯•å…¶ä»–å¯èƒ½çš„é€‰æ‹©å™¨
                product_rows = await page.query_selector_all(product_rows_alt_selector)

            # ğŸ”§ ä¿®å¤ï¼šå»é‡ï¼Œé¿å… CSS å’Œ XPath é€‰æ‹©å™¨åŒ¹é…åˆ°ç›¸åŒå…ƒç´ 
            product_rows = await self._deduplicate_rows(product_rows)
            total_rows = len(product_rows)
            self.logger.info(f"ğŸ“‹ æ‰¾åˆ° {total_rows} ä¸ªå•†å“è¡Œï¼ˆå»é‡åï¼‰ï¼Œå¼€å§‹å¤„ç†ï¼ˆæœ€å¤š {max_products} ä¸ªï¼‰")

            # ğŸ”§ ä¿®å¤ï¼šæ¯æ¬¡å¾ªç¯é‡æ–°è·å–å•†å“è¡Œï¼Œé¿å…é¡µé¢å¯¼èˆªåå…ƒç´ å¤±æ•ˆ
            # ä½¿ç”¨ç´¢å¼•è€Œä¸æ˜¯ç›´æ¥éå†å…ƒç´ 
            for i in range(min(total_rows, max_products)):
                try:
                    # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ¯æ¬¡å¾ªç¯é‡æ–°è·å–å•†å“è¡Œåˆ—è¡¨
                    current_rows = await page.query_selector_all(product_rows_selector)
                    if not current_rows:
                        current_rows = await page.query_selector_all(product_rows_alt_selector)

                    # å»é‡
                    unique_current_rows = await self._deduplicate_rows(current_rows)

                    # æ£€æŸ¥ç´¢å¼•æ˜¯å¦æœ‰æ•ˆ
                    if i >= len(unique_current_rows):
                        self.logger.warning(f"âš ï¸  ç´¢å¼• {i} è¶…å‡ºèŒƒå›´ï¼Œè·³è¿‡")
                        break

                    row = unique_current_rows[i]

                    # å…ˆæå–åŸºç¡€å­—æ®µï¼ˆç±»ç›®ã€ä¸Šæ¶æ—¶é—´ã€é”€é‡ã€é‡é‡ï¼‰
                    category_data = await self._extract_category(row)
                    listing_date_data = await self._extract_listing_date(row)
                    sales_volume = await self._extract_product_sales_volume(row)
                    weight = await self._extract_weight(row)

                    # æ„å»ºåŸºç¡€å•†å“æ•°æ®ç”¨äºå‰ç½®è¿‡æ»¤ï¼ˆä½¿ç”¨ç»Ÿä¸€å­—æ®µåï¼‰
                    basic_product_data = {
                        'product_category_cn': category_data.get('category_cn'),
                        'product_category_ru': category_data.get('category_ru'),
                        'product_listing_date': listing_date_data.get('listing_date'),
                        'product_shelf_duration': listing_date_data.get('shelf_duration'),
                        'product_sales_volume': sales_volume,
                        'product_weight': weight
                    }

                    # åº”ç”¨å‰ç½®è¿‡æ»¤
                    if product_filter_func:
                        if not product_filter_func(basic_product_data):
                            filtered_count += 1
                            self.logger.debug(f"â­ï¸  å•†å“ #{i+1} æœªé€šè¿‡å‰ç½®è¿‡æ»¤ï¼Œè·³è¿‡ OZON è¯¦æƒ…é¡µå¤„ç†")
                            continue

                    # é€šè¿‡è¿‡æ»¤ï¼Œç»§ç»­æå–å®Œæ•´å•†å“ä¿¡æ¯ï¼ˆåŒ…æ‹¬ OZON è¯¦æƒ…é¡µï¼‰
                    product_data = await self._extract_product_from_row_async(row)
                    if product_data:
                        products.append(product_data)
                        self.logger.info(f"âœ… å•†å“ #{i+1} æå–æˆåŠŸ")

                except Exception as e:
                    self.logger.warning(f"âš ï¸  æå–ç¬¬ {i + 1} ä¸ªå•†å“ä¿¡æ¯å¤±è´¥: {e}")
                    continue

            if products:
                self.logger.info(f"ğŸ‰ æˆåŠŸæå– {len(products)} ä¸ªæœ‰æ•ˆå•†å“ä¿¡æ¯ï¼ˆå‰ç½®è¿‡æ»¤è·³è¿‡ {filtered_count} ä¸ªï¼‰")
            else:
                self.logger.warning("âš ï¸  æœªæå–åˆ°æœ‰æ•ˆçš„å•†å“ä¿¡æ¯")
            return products

        except Exception as e:
            self.logger.error(f"âŒ æå–å•†å“åˆ—è¡¨å¤±è´¥: {e}")
            return []

    async def _extract_product_from_row_async(self, row_element) -> Optional[Dict[str, Any]]:
        """
        å¼‚æ­¥ä»è¡Œå…ƒç´ ä¸­æå–å•†å“ä¿¡æ¯å¹¶ç‚¹å‡»è¿›å…¥OZONè¯¦æƒ…é¡µ

        ä¸»æ–¹æ³•ï¼šåè°ƒæ•´ä¸ªå•†å“ä¿¡æ¯æå–æµç¨‹

        Args:
            row_element: è¡Œå…ƒç´ 

        Returns:
            Dict[str, Any]: å®Œæ•´çš„å•†å“ä¿¡æ¯ï¼ˆåŒ…å«OZONè¯¦æƒ…é¡µæ•°æ®ï¼‰
        """
        try:
            # 1. æå– Seerfar è¡¨æ ¼åŸºç¡€æ•°æ®
            product_data = await self._extract_basic_product_data(row_element)

            # 2. è·å– OZON URL
            ozon_url = await self._get_ozon_url_from_row(row_element)
            if not ozon_url:
                return None

            # 3. æŠ“å– OZON è¯¦æƒ…é¡µæ•°æ®
            ozon_data = await self._fetch_ozon_details(ozon_url)
            if ozon_data:
                product_data.update(ozon_data)

            return product_data if product_data else None

        except Exception as e:
            self.logger.error(f"æå–å•†å“ä¿¡æ¯å¤±è´¥: {e}")
            return None

    async def _extract_basic_product_data(self, row_element) -> Dict[str, Any]:
        """
        æå– Seerfar è¡¨æ ¼ä¸­çš„åŸºç¡€å•†å“æ•°æ®

        Args:
            row_element: è¡Œå…ƒç´ 

        Returns:
            Dict[str, Any]: åŸºç¡€å•†å“æ•°æ®ï¼ˆç±»ç›®ã€ä¸Šæ¶æ—¶é—´ã€é”€é‡ã€é‡é‡ï¼‰
        """
        product_data = {}

        # 1. æå–ç±»ç›®ä¿¡æ¯
        category_data = await self._extract_category(row_element)
        product_data.update(category_data)

        # 2. æå–ä¸Šæ¶æ—¶é—´
        listing_date_data = await self._extract_listing_date(row_element)
        product_data.update(listing_date_data)

        # 3. æå–é”€é‡
        sales_volume = await self._extract_product_sales_volume(row_element)
        if sales_volume is not None:
            product_data['sales_volume'] = sales_volume

        # 4. æå–é‡é‡
        weight = await self._extract_weight(row_element)
        if weight is not None:
            product_data['weight'] = weight

        return product_data

    async def _get_ozon_url_from_row(self, row_element) -> Optional[str]:
        """
        ä»è¡Œå…ƒç´ ä¸­æå– OZON URL

        Args:
            row_element: è¡Œå…ƒç´ 

        Returns:
            Optional[str]: OZON URLï¼Œå¦‚æœæå–å¤±è´¥è¿”å› None
        """
        try:
            # éªŒè¯ page å¯¹è±¡
            page = self.browser_service.page
            if not self._validate_page(page):
                return None

            # ä»é…ç½®æ–‡ä»¶è·å–é€‰æ‹©å™¨
            third_column_selector = get_seerfar_selector('product_list', 'third_column')
            clickable_element_selector = get_seerfar_selector('product_list', 'clickable_element')
            clickable_element_alt_selector = get_seerfar_selector('product_list', 'clickable_element_alt')

            if not third_column_selector or not clickable_element_selector or not clickable_element_alt_selector:
                self.logger.error("âŒ æœªèƒ½æ‰¾åˆ°å•†å“è¡Œå…ƒç´ é€‰æ‹©å™¨é…ç½®")
                return None

            # æŸ¥æ‰¾ç¬¬ä¸‰åˆ—ä¸­æœ‰onclickäº‹ä»¶çš„å…ƒç´ 
            td3_element = await row_element.query_selector(third_column_selector)
            if not td3_element:
                self.logger.warning("âš ï¸ æœªæ‰¾åˆ°ç¬¬ä¸‰åˆ—ï¼Œè·³è¿‡æ­¤å•†å“")
                return None

            # æŸ¥æ‰¾å¯ç‚¹å‡»å…ƒç´ 
            clickable_element = await td3_element.query_selector(clickable_element_selector)
            if not clickable_element:
                clickable_element = await td3_element.query_selector(clickable_element_alt_selector)
                if not clickable_element:
                    self.logger.warning("âš ï¸ æœªæ‰¾åˆ°å¯ç‚¹å‡»çš„å•†å“å…ƒç´ ï¼Œè·³è¿‡æ­¤å•†å“")
                    return None

            # è®°å½•æ‰¾åˆ°çš„å…ƒç´ ç±»å‹
            element_tag = await clickable_element.evaluate("el => el.tagName")
            element_class = await clickable_element.evaluate("el => el.className")
            self.logger.info(f"ğŸ”— æ‰¾åˆ°å¯ç‚¹å‡»å…ƒç´ : {element_tag}.{element_class}")

            # æå– onclick ä¸­çš„ URL
            onclick_attr = await clickable_element.get_attribute("onclick")
            if onclick_attr and "window.open" in onclick_attr:
                import re
                url_match = re.search(r"window\.open\('([^']+)'\)", onclick_attr)
                if url_match:
                    ozon_url = url_match.group(1)
                    self.logger.info(f"æå–åˆ° OZON URL: {ozon_url}")

                    # è·å–æœ€ç»ˆ URLï¼ˆå¤„ç†é‡å®šå‘ï¼‰
                    final_url = await self._resolve_ozon_url(ozon_url, page)
                    return final_url

            self.logger.warning("æœªæ‰¾åˆ°æœ‰æ•ˆçš„onclickäº‹ä»¶")
            return None

        except Exception as e:
            self.logger.error(f"æå– OZON URL å¤±è´¥: {e}")
            return None

    async def _resolve_ozon_url(self, ozon_url: str, page) -> str:
        """
        è§£æ OZON URLï¼Œå¤„ç†å¯èƒ½çš„é‡å®šå‘

        Args:
            ozon_url: åŸå§‹ OZON URL
            page: å½“å‰é¡µé¢å¯¹è±¡

        Returns:
            str: æœ€ç»ˆçš„ OZON URL
        """
        new_page = None
        try:
            new_page = await page.context.new_page()
            await new_page.goto(ozon_url)
            await new_page.wait_for_load_state('domcontentloaded', timeout=5000)

            # è·å–æœ€ç»ˆURLï¼ˆå¯èƒ½æœ‰é‡å®šå‘ï¼‰
            final_ozon_url = new_page.url
            self.logger.info(f"è·å–åˆ°æœ€ç»ˆ OZON URL: {final_ozon_url}")
            return final_ozon_url

        finally:
            # ç«‹å³å…³é—­ä¸´æ—¶é¡µé¢ï¼Œé‡Šæ”¾æµè§ˆå™¨èµ„æº
            if new_page:
                try:
                    await new_page.close()
                    self.logger.debug("âœ… ä¸´æ—¶é¡µé¢å·²å…³é—­")
                except Exception as close_error:
                    self.logger.warning(f"å…³é—­ä¸´æ—¶é¡µé¢æ—¶å‡ºé”™: {close_error}")

    async def _fetch_ozon_details(self, ozon_url: str) -> Optional[Dict[str, Any]]:
        """
        æŠ“å– OZON è¯¦æƒ…é¡µæ•°æ®

        Args:
            ozon_url: OZON å•†å“è¯¦æƒ…é¡µ URL

        Returns:
            Optional[Dict[str, Any]]: OZON è¯¦æƒ…é¡µæ•°æ®ï¼ŒåŒ…å«ä»·æ ¼ã€è·Ÿå–åº—é“ºã€ERP æ•°æ®
        """
        self.logger.info("ğŸ“Š è°ƒç”¨ OzonScraper å¤„ç† OZON å•†å“è¯¦æƒ…é¡µ...")
        try:
            from .ozon_scraper import OzonScraper

            # åˆ›å»º OzonScraper å®ä¾‹å¹¶ä½¿ç”¨å…¬å…±æ¥å£
            ozon_scraper = OzonScraper(self.config)
            ozon_result = ozon_scraper.scrape(ozon_url, include_competitors=True)

            # å¤„ç†æŠ“å–ç»“æœ
            if ozon_result.success:
                ozon_data = {}

                # æå–ä»·æ ¼æ•°æ®
                if 'price_data' in ozon_result.data:
                    ozon_data.update(ozon_result.data['price_data'])
                    self.logger.debug(f"âœ… ä»·æ ¼æ•°æ®å·²æå–: {len(ozon_result.data['price_data'])}é¡¹")

                # æå–è·Ÿå–åº—é“ºæ•°æ®
                if 'competitors' in ozon_result.data:
                    ozon_data['competitors'] = ozon_result.data['competitors']
                    self.logger.debug(f"âœ… è·Ÿå–åº—é“ºæ•°æ®å·²æå–: {len(ozon_result.data['competitors'])}ä¸ª")

                # æå– ERP æ•°æ®
                if 'erp_data' in ozon_result.data:
                    ozon_data['erp_data'] = ozon_result.data['erp_data']
                    self.logger.debug("âœ… ERP æ•°æ®å·²æå–")

                self.logger.info(f"âœ… OZON æ•°æ®æå–å®Œæˆ: æ‰§è¡Œæ—¶é—´={ozon_result.execution_time:.2f}ç§’")
                return ozon_data
            else:
                self.logger.warning(f"âš ï¸ OZON æ•°æ®æå–å¤±è´¥: {ozon_result.error_message}")
                return None

        except Exception as scrape_error:
            self.logger.error(f"âŒ è°ƒç”¨ OzonScraper å¤±è´¥: {scrape_error}")
            return None

    def _validate_page(self, page) -> bool:
        """
        éªŒè¯ page å¯¹è±¡æ˜¯å¦æœ‰æ•ˆ

        Args:
            page: Playwright é¡µé¢å¯¹è±¡

        Returns:
            bool: é¡µé¢æ˜¯å¦æœ‰æ•ˆ
        """
        if page is None:
            self.logger.error("âŒ page å¯¹è±¡ä¸º Noneï¼Œæµè§ˆå™¨å¯èƒ½æœªæ­£ç¡®å¯åŠ¨")
            return False
        return True

    async def _deduplicate_rows(self, rows: list) -> list:
        """
        å»é‡å•†å“è¡Œï¼Œé¿å… CSS å’Œ XPath é€‰æ‹©å™¨åŒ¹é…åˆ°ç›¸åŒå…ƒç´ 

        ä½¿ç”¨ data-index å±æ€§è¿›è¡Œå»é‡ã€‚å¦‚æœå…ƒç´ æ²¡æœ‰ data-index å±æ€§ï¼Œ
        åˆ™ä¿ç•™è¯¥å…ƒç´ ã€‚

        Args:
            rows: å•†å“è¡Œå…ƒç´ åˆ—è¡¨

        Returns:
            list: å»é‡åçš„å•†å“è¡Œåˆ—è¡¨
        """
        seen_indices = set()
        unique_rows = []

        for row in rows:
            try:
                data_index = await row.get_attribute('data-index')
                if data_index and data_index not in seen_indices:
                    seen_indices.add(data_index)
                    unique_rows.append(row)
            except Exception:
                # å¦‚æœæ²¡æœ‰ data-index å±æ€§ï¼Œä¹ŸåŠ å…¥åˆ—è¡¨
                unique_rows.append(row)

        return unique_rows

    async def _extract_category(self, row_element) -> Dict[str, Optional[str]]:
        """
        ä»è¡Œå…ƒç´ ä¸­æå–ç±»ç›®ä¿¡æ¯

        Args:
            row_element: Playwright è¡Œå…ƒç´ 

        Returns:
            Dict[str, Optional[str]]: åŒ…å« category_cn å’Œ category_ru çš„å­—å…¸
        """
        result = {'category_cn': None, 'category_ru': None}

        # ç±»ç›®åˆ—ç´¢å¼•ï¼ˆä»0å¼€å§‹ï¼‰
        # ç¬¬0åˆ—ï¼šå¤é€‰æ¡†ï¼Œç¬¬1åˆ—ï¼šåºå·ï¼Œç¬¬2åˆ—ï¼šå•†å“ä¿¡æ¯ï¼Œç¬¬3åˆ—ï¼šç±»ç›®
        CATEGORY_COLUMN_INDEX = 3

        try:
            # æŸ¥æ‰¾ç¬¬ä¸‰ä¸ª td å…ƒç´ ï¼ˆç±»ç›®åˆ—ï¼‰
            td_elements = await row_element.query_selector_all('td')
            if len(td_elements) <= CATEGORY_COLUMN_INDEX:
                self.logger.warning("âš ï¸ æœªæ‰¾åˆ°è¶³å¤Ÿçš„ td å…ƒç´ æ¥æå–ç±»ç›®")
                return result

            category_td = td_elements[CATEGORY_COLUMN_INDEX]

            # æå–ä¸­æ–‡ç±»ç›®
            category_cn_element = await category_td.query_selector('span.category-title')
            if category_cn_element:
                category_cn_text = await category_cn_element.text_content()
                if category_cn_text:
                    result['category_cn'] = category_cn_text.strip()

            # æå–ä¿„æ–‡ç±»ç›®
            category_ru_element = await category_td.query_selector('span.text-muted')
            if category_ru_element:
                category_ru_text = await category_ru_element.text_content()
                if category_ru_text:
                    result['category_ru'] = category_ru_text.strip()

            if result['category_cn'] or result['category_ru']:
                self.logger.debug(f"âœ… ç±»ç›®æå–æˆåŠŸ: ä¸­æ–‡={result['category_cn']}, ä¿„æ–‡={result['category_ru']}")
            else:
                self.logger.warning("âš ï¸ æœªèƒ½æå–åˆ°ç±»ç›®ä¿¡æ¯")

        except Exception as e:
            self.logger.error(f"âŒ ç±»ç›®æå–å¤±è´¥: {e}")

        return result

    async def _extract_listing_date(self, row_element) -> Dict[str, Optional[str]]:
        """
        ä»è¡Œå…ƒç´ ä¸­æå–ä¸Šæ¶æ—¶é—´ä¿¡æ¯

        Args:
            row_element: Playwright è¡Œå…ƒç´ 

        Returns:
            Dict[str, Optional[str]]: åŒ…å« listing_date å’Œ shelf_duration çš„å­—å…¸
        """
        result = {'listing_date': None, 'shelf_duration': None}

        try:
            # æŸ¥æ‰¾æ‰€æœ‰ td å…ƒç´ ï¼Œä¸Šæ¶æ—¶é—´åœ¨æœ€åä¸€ä¸ª td
            td_elements = await row_element.query_selector_all('td')
            if not td_elements:
                self.logger.warning("âš ï¸ æœªæ‰¾åˆ° td å…ƒç´ æ¥æå–ä¸Šæ¶æ—¶é—´")
                return result

            # æœ€åä¸€ä¸ª td åŒ…å«ä¸Šæ¶æ—¶é—´
            listing_date_td = td_elements[-1]

            # è·å–å®Œæ•´çš„ HTML å†…å®¹ä»¥ä¾¿è§£æ
            inner_html = await listing_date_td.inner_html()

            # æå–æ—¥æœŸï¼ˆæ ¼å¼ï¼š2025-06-20ï¼‰
            date_match = re.search(r'(\d{4}-\d{2}-\d{2})', inner_html)
            if date_match:
                result['listing_date'] = date_match.group(1)

            # æå–è´§æ¶æ—¶é•¿ï¼ˆæ ¼å¼ï¼š4 ä¸ªæœˆ æˆ– < 1 ä¸ªæœˆï¼‰
            duration_match = re.search(r'<span[^>]*>([^<]+)</span>', inner_html)
            if duration_match:
                duration_text = duration_match.group(1).strip()
                if duration_text and duration_text != '':
                    result['shelf_duration'] = duration_text

            if result['listing_date'] or result['shelf_duration']:
                self.logger.debug(f"âœ… ä¸Šæ¶æ—¶é—´æå–æˆåŠŸ: æ—¥æœŸ={result['listing_date']}, æ—¶é•¿={result['shelf_duration']}")
            else:
                self.logger.warning("âš ï¸ æœªèƒ½æå–åˆ°ä¸Šæ¶æ—¶é—´ä¿¡æ¯")

        except Exception as e:
            self.logger.error(f"âŒ ä¸Šæ¶æ—¶é—´æå–å¤±è´¥: {e}")

        return result

    async def _extract_product_sales_volume(self, row_element) -> Optional[int]:
        """
        ä»è¡Œå…ƒç´ ä¸­æå–å•†å“é”€é‡ä¿¡æ¯

        Args:
            row_element: Playwright è¡Œå…ƒç´ 

        Returns:
            Optional[int]: é”€é‡æ•°å€¼ï¼Œå¦‚æœæå–å¤±è´¥è¿”å› None
        """
        # é”€é‡åˆ—ç´¢å¼•ï¼ˆä»0å¼€å§‹ï¼Œç¬¬5åˆ—ï¼‰
        SALES_VOLUME_COLUMN_INDEX = 4

        try:
            # æŸ¥æ‰¾æ‰€æœ‰ td å…ƒç´ 
            td_elements = await row_element.query_selector_all('td')
            if len(td_elements) <= SALES_VOLUME_COLUMN_INDEX:
                self.logger.warning("âš ï¸ æœªæ‰¾åˆ°è¶³å¤Ÿçš„ td å…ƒç´ æ¥æå–é”€é‡")
                return None

            # é”€é‡åœ¨ç¬¬5ä¸ª tdï¼ˆç´¢å¼•4ï¼‰
            sales_td = td_elements[SALES_VOLUME_COLUMN_INDEX]

            # è·å–æ–‡æœ¬å†…å®¹
            sales_text = await sales_td.text_content()
            if not sales_text:
                self.logger.warning("âš ï¸ é”€é‡ td å…ƒç´ ä¸ºç©º")
                return None

            # æå–æ•°å­—ï¼ˆåªæå–ç¬¬ä¸€è¡Œçš„æ•°å­—ï¼Œå¿½ç•¥å¢é•¿ç‡ï¼‰
            lines = sales_text.strip().split('\n')
            if lines:
                first_line = lines[0].strip()
                # æå–çº¯æ•°å­—
                sales_match = re.search(r'^(\d+)', first_line)
                if sales_match:
                    sales_volume = int(sales_match.group(1))
                    self.logger.debug(f"âœ… é”€é‡æå–æˆåŠŸ: {sales_volume}")
                    return sales_volume

            self.logger.warning(f"âš ï¸ æœªèƒ½ä»æ–‡æœ¬ä¸­æå–é”€é‡: {sales_text}")
            return None

        except Exception as e:
            self.logger.error(f"âŒ é”€é‡æå–å¤±è´¥: {e}")
            return None

    async def _extract_weight(self, row_element) -> Optional[float]:
        """
        ä»è¡Œå…ƒç´ ä¸­æå–å•†å“é‡é‡ä¿¡æ¯

        Args:
            row_element: Playwright è¡Œå…ƒç´ 

        Returns:
            Optional[float]: é‡é‡æ•°å€¼ï¼ˆå…‹ï¼‰ï¼Œå¦‚æœæå–å¤±è´¥è¿”å› None
        """
        try:
            # æŸ¥æ‰¾æ‰€æœ‰ td å…ƒç´ 
            td_elements = await row_element.query_selector_all('td')

            # é‡é‡åœ¨å€’æ•°ç¬¬äºŒåˆ—
            if len(td_elements) < 2:
                self.logger.warning("âš ï¸ æœªæ‰¾åˆ°è¶³å¤Ÿçš„ td å…ƒç´ æ¥æå–é‡é‡")
                return None

            # å€’æ•°ç¬¬äºŒä¸ª td
            weight_td = td_elements[-2]

            # è·å–æ–‡æœ¬å†…å®¹
            weight_text = await weight_td.text_content()
            if not weight_text:
                self.logger.warning("âš ï¸ é‡é‡ td å…ƒç´ ä¸ºç©º")
                return None

            # æå–æ•°å­—å’Œå•ä½ï¼ˆæ ¼å¼ï¼š161 g æˆ– 1.5 kgï¼‰
            weight_text = weight_text.strip()
            weight_match = re.search(r'([\d.]+)\s*(g|kg)', weight_text, re.IGNORECASE)

            if weight_match:
                value = float(weight_match.group(1))
                unit = weight_match.group(2).lower()

                # ç»Ÿä¸€è½¬æ¢ä¸ºå…‹
                if unit == 'kg':
                    weight_grams = value * 1000
                else:  # g
                    weight_grams = value

                self.logger.debug(f"âœ… é‡é‡æå–æˆåŠŸ: {weight_grams}g (åŸå§‹: {weight_text})")
                return weight_grams

            self.logger.warning(f"âš ï¸ æœªèƒ½ä»æ–‡æœ¬ä¸­æå–é‡é‡: {weight_text}")
            return None

        except Exception as e:
            self.logger.error(f"âŒ é‡é‡æå–å¤±è´¥: {e}")
            return None

