"""
Seerfarå¹³å°æŠ“å–å™¨

è´Ÿè´£ä»Seerfarå¹³å°æŠ“å–OZONåº—é“ºçš„é”€å”®æ•°æ®å’Œå•†å“ä¿¡æ¯ã€‚
åŸºäºç°ä»£åŒ–çš„Playwrightæµè§ˆå™¨æœåŠ¡ã€‚
"""

import time
import re
from typing import Dict, Any, List, Optional, Callable

from .xuanping_browser_service import XuanpingBrowserServiceSync
from ..models import ScrapingResult
from common.config import GoodStoreSelectorConfig
from common.config.seerfar_selectors import get_seerfar_selector


class SeerfarScraper:
    """Seerfarå¹³å°æŠ“å–å™¨"""

    def __init__(self, config: Optional[GoodStoreSelectorConfig] = None):
        """åˆå§‹åŒ–SeerfaræŠ“å–å™¨"""
        from common.config import get_config
        import logging

        self.config = config or get_config()
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        self.base_url = self.config.scraping.seerfar_base_url
        self.store_detail_path = self.config.scraping.seerfar_store_detail_path

        # åˆ›å»ºæµè§ˆå™¨æœåŠ¡
        self.browser_service = XuanpingBrowserServiceSync()

    def scrape_store_sales_data(self, store_id: str, filter_func=None) -> ScrapingResult:
        """
        æŠ“å–åº—é“ºé”€å”®æ•°æ®

        Args:
            store_id: åº—é“ºID
            filter_func: è¿‡æ»¤å‡½æ•°ï¼Œç”¨äºç­›é€‰åº—é“º

        Returns:
            ScrapingResult: æŠ“å–ç»“æœï¼ŒåŒ…å«é”€å”®æ•°æ®
        """
        # æ„å»ºåº—é“ºè¯¦æƒ…é¡µURL
        url = f"{self.base_url}{self.store_detail_path}?storeId={store_id}&platform=OZON"

        # dryrunæ¨¡å¼ä¸‹è®°å½•å…¥å‚ï¼Œä½†ä»æ‰§è¡ŒçœŸå®çš„æŠ“å–æµç¨‹
        if self.config.dryrun:
            self.logger.info(f"ğŸ§ª è¯•è¿è¡Œæ¨¡å¼ - Seerfaråº—é“ºé”€å”®æ•°æ®æŠ“å–å…¥å‚: åº—é“ºID={store_id}, URL={url}")
            self.logger.info("ğŸ§ª è¯•è¿è¡Œæ¨¡å¼ - æ‰§è¡ŒçœŸå®çš„é”€å”®æ•°æ®æŠ“å–æµç¨‹ï¼ˆç»“æœä¸ä¼šä¿å­˜åˆ°æ–‡ä»¶ï¼‰")

        # ä½¿ç”¨æµè§ˆå™¨æœåŠ¡æŠ“å–æ•°æ®
        result = self.browser_service.scrape_page_data(url, self._extract_sales_data_async)

        # å¦‚æœæä¾›äº†è¿‡æ»¤å‡½æ•°ï¼Œåˆ™åº”ç”¨è¿‡æ»¤
        if result.success and filter_func and result.data:
            if not filter_func(result.data):
                self.logger.info(f"åº—é“º{store_id}ä¸ç¬¦åˆç­›é€‰æ¡ä»¶")
                return ScrapingResult(
                    success=False,
                    data={},
                    error_message="åº—é“ºä¸ç¬¦åˆç­›é€‰æ¡ä»¶",
                    execution_time=result.execution_time
                )

        return result

    def scrape_store_products(self, store_id: str, max_products: Optional[int] = None, filter_func: Optional[Callable[[Dict[str, Any]], bool]] = None) -> ScrapingResult:
        """
        æŠ“å–åº—é“ºå•†å“åˆ—è¡¨

        Args:
            store_id: åº—é“ºID
            max_products: æœ€å¤§æŠ“å–å•†å“æ•°é‡
            filter_func: å•†å“è¿‡æ»¤å‡½æ•°ï¼Œæ¥å—å•†å“æ•°æ®å­—å…¸ï¼Œè¿”å›å¸ƒå°”å€¼

        Returns:
            ScrapingResult: æŠ“å–ç»“æœï¼ŒåŒ…å«å•†å“åˆ—è¡¨
        """
        max_products = max_products or self.config.store_filter.max_products_to_check

        # æ„å»ºåº—é“ºè¯¦æƒ…é¡µURL
        url = f"{self.base_url}{self.store_detail_path}?storeId={store_id}&platform=OZON"

        # dryrunæ¨¡å¼ä¸‹è®°å½•å…¥å‚ï¼Œä½†ä»æ‰§è¡ŒçœŸå®çš„æŠ“å–æµç¨‹
        if self.config.dryrun:
            self.logger.info(f"ğŸ§ª è¯•è¿è¡Œæ¨¡å¼ - Seerfaråº—é“ºå•†å“æŠ“å–å…¥å‚: åº—é“ºID={store_id}, "
                             f"æœ€å¤§å•†å“æ•°={max_products}, URL={url}")
            self.logger.info("ğŸ§ª è¯•è¿è¡Œæ¨¡å¼ - æ‰§è¡ŒçœŸå®çš„å•†å“æŠ“å–æµç¨‹ï¼ˆç»“æœä¸ä¼šä¿å­˜åˆ°æ–‡ä»¶ï¼‰")

        # åˆ›å»ºæå–å‡½æ•°
        async def extract_products(browser_service):
            products = await self._extract_products_list_async(browser_service, max_products)

            # å¦‚æœæä¾›äº†è¿‡æ»¤å‡½æ•°ï¼Œåˆ™åº”ç”¨è¿‡æ»¤
            if filter_func and products:
                filtered_products = [p for p in products if filter_func(p)]
                self.logger.info(f"å•†å“è¿‡æ»¤å®Œæˆï¼ŒåŸå§‹æ•°é‡: {len(products)}, è¿‡æ»¤åæ•°é‡: {len(filtered_products)}")
                products = filtered_products

            return {'products': products, 'total_count': len(products)}

        # ä½¿ç”¨æµè§ˆå™¨æœåŠ¡æŠ“å–æ•°æ®
        return self.browser_service.scrape_page_data(url, extract_products)

    def scrape(self, store_id: str, include_products: bool = True, **kwargs) -> ScrapingResult:
        """
        ç»¼åˆæŠ“å–åº—é“ºä¿¡æ¯
        
        Args:
            store_id: åº—é“ºID
            include_products: æ˜¯å¦åŒ…å«å•†å“ä¿¡æ¯
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            ScrapingResult: æŠ“å–ç»“æœ
        """
        start_time = time.time()

        try:
            # æŠ“å–é”€å”®æ•°æ®
            sales_result = self.scrape_store_sales_data(store_id)
            if not sales_result.success:
                return sales_result

            result_data = {
                'store_id': store_id,
                'sales_data': sales_result.data
            }

            # å¦‚æœéœ€è¦ï¼ŒæŠ“å–å•†å“ä¿¡æ¯
            if include_products:
                products_result = self.scrape_store_products(store_id)
                if products_result.success:
                    result_data['products'] = products_result.data['products']
                else:
                    self.logger.warning(f"æŠ“å–åº—é“º{store_id}å•†å“ä¿¡æ¯å¤±è´¥: {products_result.error_message}")
                    result_data['products'] = []

            return ScrapingResult(
                success=True,
                data=result_data,
                execution_time=time.time() - start_time
            )

        except Exception as e:
            self.logger.error(f"ç»¼åˆæŠ“å–åº—é“º{store_id}ä¿¡æ¯å¤±è´¥: {e}")
            return ScrapingResult(
                success=False,
                data={},
                error_message=str(e),
                execution_time=time.time() - start_time
            )

    async def _extract_sales_data_async(self, browser_service) -> Dict[str, Any]:
        """
        å¼‚æ­¥æå–é”€å”®æ•°æ® - ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é€‰æ‹©å™¨

        Args:
            browser_service: æµè§ˆå™¨æœåŠ¡å®ä¾‹

        Returns:
            Dict[str, Any]: é”€å”®æ•°æ®
        """
        sales_data = {}

        try:
            # ä½¿ç”¨Playwrightçš„é¡µé¢APIè¿›è¡Œå…ƒç´ æŸ¥æ‰¾
            page = browser_service.browser_driver.page

            # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é€‰æ‹©å™¨æå–é”€å”®é¢
            await self._extract_sales_amount(page, sales_data)

            # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é€‰æ‹©å™¨æå–é”€é‡
            await self._extract_sales_volume(page, sales_data)

            # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é€‰æ‹©å™¨æå–æ—¥å‡é”€é‡
            await self._extract_daily_avg_sales(page, sales_data)

            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å…·ä½“å…ƒç´ ï¼Œå°è¯•é€šç”¨æ–¹æ³•
            if not sales_data:
                sales_data = await self._extract_sales_data_generic_async(page)

            # åˆå¹¶æ—¥å¿—è¾“å‡ºåº—é“ºæ•°æ®æ‘˜è¦
            if sales_data:
                sales_amount = sales_data.get('sold_30days', 0)
                sales_volume = sales_data.get('sold_count_30days', 0)
                daily_avg = sales_data.get('daily_avg_sold', 0)
                self.logger.info(
                    f"ğŸ“Š åº—é“ºæ•°æ®æå–å®Œæˆ - é”€å”®é¢: {sales_amount:.0f}â‚½, é”€é‡: {sales_volume}, æ—¥å‡: {daily_avg}")

            self.logger.debug(f"æå–çš„é”€å”®æ•°æ®: {sales_data}")
            return sales_data

        except Exception as e:
            self.logger.error(f"æå–é”€å”®æ•°æ®å¤±è´¥: {e}")
            return {}

    async def _extract_sales_amount(self, page, sales_data: Dict[str, Any]):
        """æå–é”€å”®é¢ - ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é€‰æ‹©å™¨"""
        try:
            # ä»é…ç½®æ–‡ä»¶è·å–é”€å”®é¢é€‰æ‹©å™¨
            sales_amount_xpath = get_seerfar_selector('store_sales_data', 'sales_amount')
            if not sales_amount_xpath:
                self.logger.error("âŒ æœªèƒ½æ‰¾åˆ°é”€å”®é¢é€‰æ‹©å™¨é…ç½®")
                return

            # ç­‰å¾…å…ƒç´ å‡ºç°
            try:
                await page.wait_for_selector(f'xpath={sales_amount_xpath}', timeout=5000)
            except:
                self.logger.debug("é”€å”®é¢å…ƒç´ ç­‰å¾…è¶…æ—¶ï¼Œç»§ç»­å°è¯•æå–")

            element = await page.query_selector(f'xpath={sales_amount_xpath}')
            if element:
                text = await element.text_content()
                if text and text.strip():
                    # æå–æ•°å­—å¹¶è½¬æ¢ä¸ºé”€å”®é¢
                    number = self._extract_number_from_text(text.strip())
                    if number:
                        sales_data['sold_30days'] = number
                        return

            self.logger.warning("âš ï¸ æœªèƒ½æå–åˆ°é”€å”®é¢æ•°æ®")

        except Exception as e:
            self.logger.error(f"âŒ é”€å”®é¢æå–å¤±è´¥: {str(e)}")

    async def _extract_sales_volume(self, page, sales_data: Dict[str, Any]):
        """æå–é”€é‡ - ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é€‰æ‹©å™¨"""
        try:
            # ä»é…ç½®æ–‡ä»¶è·å–é”€é‡é€‰æ‹©å™¨
            sales_volume_xpath = get_seerfar_selector('store_sales_data', 'sales_volume')
            if not sales_volume_xpath:
                self.logger.error("âŒ æœªèƒ½æ‰¾åˆ°é”€é‡é€‰æ‹©å™¨é…ç½®")
                return

            # ç­‰å¾…å…ƒç´ å‡ºç°
            try:
                await page.wait_for_selector(f'xpath={sales_volume_xpath}', timeout=5000)
            except:
                self.logger.debug("é”€é‡å…ƒç´ ç­‰å¾…è¶…æ—¶ï¼Œç»§ç»­å°è¯•æå–")

            element = await page.query_selector(f'xpath={sales_volume_xpath}')
            if element:
                text = await element.text_content()
                if text and text.strip():
                    # æå–æ•°å­—å¹¶è½¬æ¢ä¸ºé”€é‡
                    number = self._extract_number_from_text(text.strip())
                    if number:
                        sales_data['sold_count_30days'] = int(number)
                        return

            self.logger.warning("âš ï¸ æœªèƒ½æå–åˆ°é”€é‡æ•°æ®")

        except Exception as e:
            self.logger.error(f"âŒ é”€é‡æå–å¤±è´¥: {str(e)}")

    async def _extract_daily_avg_sales(self, page, sales_data: Dict[str, Any]):
        """æå–æ—¥å‡é”€é‡ - ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é€‰æ‹©å™¨"""
        try:
            # ä»é…ç½®æ–‡ä»¶è·å–æ—¥å‡é”€é‡é€‰æ‹©å™¨
            daily_avg_xpath = get_seerfar_selector('store_sales_data', 'daily_avg_sales')
            if not daily_avg_xpath:
                self.logger.error("âŒ æœªèƒ½æ‰¾åˆ°æ—¥å‡é”€é‡é€‰æ‹©å™¨é…ç½®")
                return

            # ç­‰å¾…å…ƒç´ å‡ºç°
            try:
                await page.wait_for_selector(f'xpath={daily_avg_xpath}', timeout=5000)
            except:
                self.logger.debug("æ—¥å‡é”€é‡å…ƒç´ ç­‰å¾…è¶…æ—¶ï¼Œç»§ç»­å°è¯•æå–")

            element = await page.query_selector(f'xpath={daily_avg_xpath}')
            if element:
                text = await element.text_content()
                if text and text.strip():
                    # æå–æ•°å­—å¹¶è½¬æ¢ä¸ºæ—¥å‡é”€é‡
                    number = self._extract_number_from_text(text.strip())
                    if number:
                        sales_data['daily_avg_sold'] = number
                        return

            self.logger.warning("âš ï¸ æœªèƒ½æå–åˆ°æ—¥å‡é”€é‡æ•°æ®")

        except Exception as e:
            self.logger.error(f"âŒ æ—¥å‡é”€é‡æå–å¤±è´¥: {str(e)}")

    async def _extract_category_data(self, page, sales_data: Dict[str, Any]):
        """æå–ç±»ç›®æ•°æ® - ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é€‰æ‹©å™¨"""
        try:
            # ä»é…ç½®æ–‡ä»¶è·å–ç±»ç›®æ•°æ®é€‰æ‹©å™¨
            category_xpath = get_seerfar_selector('store_sales_data', 'category_data')
            if not category_xpath:
                self.logger.debug("æœªé…ç½®ç±»ç›®æ•°æ®é€‰æ‹©å™¨ï¼Œè·³è¿‡ç±»ç›®æ•°æ®æå–")
                return

            # ç­‰å¾…å…ƒç´ å‡ºç°
            try:
                await page.wait_for_selector(f'xpath={category_xpath}', timeout=5000)
            except:
                self.logger.debug("ç±»ç›®æ•°æ®å…ƒç´ ç­‰å¾…è¶…æ—¶ï¼Œç»§ç»­å°è¯•æå–")

            element = await page.query_selector(f'xpath={category_xpath}')
            if element:
                text = await element.text_content()
                if text and text.strip():
                    sales_data['category_info'] = text.strip()
                    return

            self.logger.warning("âš ï¸ æœªèƒ½æå–åˆ°ç±»ç›®æ•°æ®")

        except Exception as e:
            self.logger.error(f"âŒ ç±»ç›®æ•°æ®æå–å¤±è´¥: {str(e)}")

    async def _extract_sales_data_generic_async(self, page) -> Dict[str, Any]:
        """
        å¼‚æ­¥é€šç”¨æ–¹æ³•æå–é”€å”®æ•°æ®
        
        Args:
            page: Playwrighté¡µé¢å¯¹è±¡
            
        Returns:
            Dict[str, Any]: é”€å”®æ•°æ®
        """
        sales_data = {}

        try:
            # æŸ¥æ‰¾æ‰€æœ‰åŒ…å«æ•°å­—çš„å…ƒç´ 
            number_elements = await page.query_selector_all(
                "//*[contains(text(), 'â‚½') or contains(text(), 'ä¸‡') or contains(text(), 'åƒ')]")

            for element in number_elements[:10]:  # é™åˆ¶æ£€æŸ¥å‰10ä¸ªå…ƒç´ 
                try:
                    text = await element.text_content()
                    if not text:
                        continue

                    # åˆ¤æ–­æ˜¯å¦ä¸ºé”€å”®é¢
                    if any(keyword in text for keyword in ['é”€å”®é¢', 'è¥ä¸šé¢', 'æ”¶å…¥', 'â‚½']):
                        number = self._extract_number_from_text(text)
                        if number and number > 1000:  # é”€å”®é¢é€šå¸¸è¾ƒå¤§
                            sales_data['sold_30days'] = number

                    # åˆ¤æ–­æ˜¯å¦ä¸ºé”€é‡
                    elif any(keyword in text for keyword in ['é”€é‡', 'è®¢å•', 'ä»¶æ•°']):
                        number = self._extract_number_from_text(text)
                        if number and 10 <= number <= 10000:  # é”€é‡é€šå¸¸åœ¨åˆç†èŒƒå›´å†…
                            sales_data['sold_count_30days'] = int(number)
                except Exception as e:
                    self.logger.debug(f"å¤„ç†å…ƒç´ æ–‡æœ¬å¤±è´¥: {e}")
                    continue

            # å¦‚æœæ‰¾åˆ°é”€å”®é¢å’Œé”€é‡ï¼Œè®¡ç®—æ—¥å‡é”€é‡
            if 'sold_30days' in sales_data and 'sold_count_30days' in sales_data:
                sales_data['daily_avg_sold'] = sales_data['sold_count_30days'] / 30

            return sales_data

        except Exception as e:
            self.logger.error(f"é€šç”¨æ–¹æ³•æå–é”€å”®æ•°æ®å¤±è´¥: {e}")
            return {}

    async def _extract_products_list_async(self, browser_service, max_products: int) -> List[Dict[str, Any]]:
        """
        å¼‚æ­¥æå–å•†å“åˆ—è¡¨
        
        Args:
            browser_service: æµè§ˆå™¨æœåŠ¡å®ä¾‹
            max_products: æœ€å¤§å•†å“æ•°é‡
            
        Returns:
            List[Dict[str, Any]]: å•†å“åˆ—è¡¨
        """
        products = []

        try:
            page = browser_service.browser_driver.page

            # ä»é…ç½®æ–‡ä»¶è·å–å•†å“åˆ—è¡¨é€‰æ‹©å™¨
            product_rows_selector = get_seerfar_selector('product_list', 'product_rows')
            product_rows_alt_selector = get_seerfar_selector('product_list', 'product_rows_alt')

            if not product_rows_selector or not product_rows_alt_selector:
                self.logger.error("âŒ æœªèƒ½æ‰¾åˆ°å•†å“åˆ—è¡¨é€‰æ‹©å™¨é…ç½®")
                return []

            # æŸ¥æ‰¾å•†å“è¡¨æ ¼æˆ–åˆ—è¡¨
            product_rows = await page.query_selector_all(product_rows_selector)

            if not product_rows:
                # å°è¯•å…¶ä»–å¯èƒ½çš„é€‰æ‹©å™¨
                product_rows = await page.query_selector_all(product_rows_alt_selector)

            # for i, row in enumerate(product_rows[:max_products]):
            #     try:
            #         product_data = await self._extract_product_from_row_async(row)
            #         if product_data:
            #             products.append(product_data)
            #
            #     except Exception as e:
            #         self.logger.warning(f"æå–ç¬¬{i + 1}ä¸ªå•†å“ä¿¡æ¯å¤±è´¥: {e}")
            #         continue

            if products:
                self.logger.info(f"æˆåŠŸæå–{len(products)}ä¸ªæœ‰æ•ˆå•†å“ä¿¡æ¯")
            else:
                self.logger.warning("æœªæå–åˆ°æœ‰æ•ˆçš„å•†å“ä¿¡æ¯")
            return products

        except Exception as e:
            self.logger.error(f"æå–å•†å“åˆ—è¡¨å¤±è´¥: {e}")
            return []

    async def _extract_product_from_row_async(self, row_element) -> Optional[Dict[str, Any]]:
        """
        å¼‚æ­¥ä»è¡Œå…ƒç´ ä¸­æå–å•†å“ä¿¡æ¯å¹¶ç‚¹å‡»è¿›å…¥OZONè¯¦æƒ…é¡µ

        Args:
            row_element: è¡Œå…ƒç´ 

        Returns:
            Dict[str, Any]: å®Œæ•´çš„å•†å“ä¿¡æ¯ï¼ˆåŒ…å«OZONè¯¦æƒ…é¡µæ•°æ®ï¼‰
        """
        try:
            product_data = {}

            # ç®€åŒ–ï¼šç›´æ¥æŸ¥æ‰¾å¹¶ç‚¹å‡»å•†å“å›¾ç‰‡
            try:
                # è·å–é¡µé¢å¯¹è±¡
                page = self.browser_service.browser_driver.page

                # ä»é…ç½®æ–‡ä»¶è·å–é€‰æ‹©å™¨
                third_column_selector = get_seerfar_selector('product_list', 'third_column')
                clickable_element_selector = get_seerfar_selector('product_list', 'clickable_element')
                clickable_element_alt_selector = get_seerfar_selector('product_list', 'clickable_element_alt')

                if not third_column_selector or not clickable_element_selector or not clickable_element_alt_selector:
                    self.logger.error("âŒ æœªèƒ½æ‰¾åˆ°å•†å“è¡Œå…ƒç´ é€‰æ‹©å™¨é…ç½®")
                    return None

                # æŸ¥æ‰¾ç¬¬ä¸‰åˆ—ä¸­æœ‰onclickäº‹ä»¶çš„å…ƒç´ 
                # æ ¹æ®ç”¨æˆ·æä¾›çš„XPathï¼Œå•†å“åœ¨ç¬¬ä¸‰åˆ—ï¼ˆtd[3]ï¼‰
                td3_element = await row_element.query_selector(third_column_selector)
                if not td3_element:
                    self.logger.warning("âš ï¸ æœªæ‰¾åˆ°ç¬¬ä¸‰åˆ—ï¼Œè·³è¿‡æ­¤å•†å“")
                    return None

                # æŸ¥æ‰¾æœ‰onclickäº‹ä»¶çš„å¯ç‚¹å‡»å…ƒç´ ï¼ˆä¼˜å…ˆæŸ¥æ‰¾span.avatarï¼‰
                clickable_element = await td3_element.query_selector(clickable_element_selector)
                if not clickable_element:
                    # å¦‚æœæ²¡æœ‰onclickï¼Œå°è¯•æŸ¥æ‰¾å…¶ä»–å¯ç‚¹å‡»å…ƒç´ 
                    clickable_element = await td3_element.query_selector(clickable_element_alt_selector)
                    if not clickable_element:
                        self.logger.warning("âš ï¸ æœªæ‰¾åˆ°å¯ç‚¹å‡»çš„å•†å“å…ƒç´ ï¼Œè·³è¿‡æ­¤å•†å“")
                        return None

                # è®°å½•æ‰¾åˆ°çš„å…ƒç´ ç±»å‹ï¼Œä¾¿äºè°ƒè¯•
                element_tag = await clickable_element.evaluate("el => el.tagName")
                element_class = await clickable_element.evaluate("el => el.className")
                self.logger.info(f"ğŸ”— æ‰¾åˆ°å¯ç‚¹å‡»å…ƒç´ : {element_tag}.{element_class}")

                # ç›´æ¥æå–onclickä¸­çš„URLå¹¶æ‰“å¼€
                onclick_attr = await clickable_element.get_attribute("onclick")
                if onclick_attr and "window.open" in onclick_attr:
                    # æå–URLå¹¶åœ¨æ–°æ ‡ç­¾é¡µæ‰“å¼€
                    import re
                    url_match = re.search(r"window\.open\('([^']+)'\)", onclick_attr)
                    if url_match:
                        ozon_url = url_match.group(1)
                        self.logger.info(f"æ‰“å¼€OZON URL: {ozon_url}")

                        # æ€§èƒ½ä¼˜åŒ–ï¼šä½¿ç”¨ try-finally ç¡®ä¿é¡µé¢èµ„æºæ¸…ç†
                        new_page = None
                        try:
                            new_page = await page.context.new_page()
                            await new_page.goto(ozon_url)
                            await new_page.wait_for_load_state('domcontentloaded', timeout=5000)

                            # è°ƒç”¨ç°æœ‰çš„OzonScraperæ¥å¤„ç†OZONè¯¦æƒ…é¡µ
                            self.logger.info("ğŸ“Š è°ƒç”¨OzonScraperå¤„ç†OZONå•†å“è¯¦æƒ…é¡µ...")
                            from .ozon_scraper import OzonScraper

                            # åˆ›å»ºOzonScraperå®ä¾‹å¹¶æå–æ•°æ®
                            ozon_scraper = OzonScraper(self.config)
                            page_content = await new_page.content()
                            ozon_price_data = await ozon_scraper._extract_price_data_from_content(page_content)
                            ozon_competitor_data = await ozon_scraper._extract_competitor_stores_from_content(
                                page_content, 10)

                            # åˆå¹¶OZONæ•°æ®
                            product_data.update(ozon_price_data)
                            if ozon_competitor_data:
                                product_data['competitors'] = ozon_competitor_data

                            self.logger.info(
                                f"âœ… OZONæ•°æ®æå–å®Œæˆ: ä»·æ ¼æ•°æ®={len(ozon_price_data)}é¡¹, è·Ÿå–åº—é“º={len(ozon_competitor_data)}ä¸ª")
                            return product_data

                        finally:
                            # å…³é”®ä¿®å¤ï¼šç¡®ä¿é¡µé¢èµ„æºå§‹ç»ˆè¢«é‡Šæ”¾
                            if new_page:
                                try:
                                    await new_page.close()
                                except Exception as close_error:
                                    self.logger.warning(f"å…³é—­é¡µé¢æ—¶å‡ºé”™: {close_error}")
                else:
                    self.logger.warning("æœªæ‰¾åˆ°æœ‰æ•ˆçš„onclickäº‹ä»¶")
                    return None

                # æ€§èƒ½ä¼˜åŒ–ï¼šå‡å°‘ä¸å¿…è¦çš„é¡µé¢ç­‰å¾…æ—¶é—´
                await page.wait_for_load_state('domcontentloaded', timeout=2000)

                # è°ƒç”¨ç°æœ‰çš„OzonScraperæ¥å¤„ç†OZONè¯¦æƒ…é¡µ
                self.logger.info("ğŸ“Š è°ƒç”¨OzonScraperå¤„ç†OZONå•†å“è¯¦æƒ…é¡µ...")

                try:
                    from .ozon_scraper import OzonScraper

                    # åˆ›å»ºOzonScraperå®ä¾‹å¹¶æå–æ•°æ®
                    # ozon_scraper = OzonScraper(self.config)
                    # page_content = await page.content()
                    # ozon_price_data = await ozon_scraper._extract_price_data_from_content(page_content)
                    # ozon_competitor_data = await ozon_scraper._extract_competitor_stores_from_content(page_content, 10)

                    # åˆå¹¶OZONæ•°æ®
                    # product_data.update(ozon_price_data)
                    # if ozon_competitor_data:
                    #     product_data['competitors'] = ozon_competitor_data
                    #
                    # self.logger.info(f"âœ… OZONæ•°æ®æå–å®Œæˆ: ä»·æ ¼æ•°æ®={len(ozon_price_data)}é¡¹, è·Ÿå–åº—é“º={len(ozon_competitor_data)}ä¸ª")

                finally:
                    # æ€§èƒ½ä¼˜åŒ–ï¼šç¡®ä¿è¿”å›åŸé¡µé¢ï¼Œå‡å°‘ç­‰å¾…æ—¶é—´
                    try:
                        await page.go_back()
                        await page.wait_for_load_state('domcontentloaded', timeout=2000)
                    except Exception as nav_error:
                        self.logger.warning(f"è¿”å›åŸé¡µé¢æ—¶å‡ºé”™: {nav_error}")

            except Exception as e:
                self.logger.error(f"ç‚¹å‡»å•†å“å›¾ç‰‡æˆ–æå–OZONæ•°æ®å¤±è´¥: {e}")
                return None

            # ç”Ÿæˆå•†å“ID
            if not product_data.get('product_id'):
                if product_data.get('image_url'):
                    url_match = re.search(r'/(\d+)', product_data['image_url'])
                    if url_match:
                        product_data['product_id'] = url_match.group(1)
                    else:
                        product_data['product_id'] = str(hash(product_data['image_url']))[:10]
                else:
                    product_data['product_id'] = f"product_{int(time.time())}"

            return product_data if product_data else None

        except Exception as e:
            self.logger.error(f"æå–å•†å“ä¿¡æ¯å¤±è´¥: {e}")
            return None

    def _extract_number_from_text(self, text: str) -> Optional[float]:
        """
        ä»æ–‡æœ¬ä¸­æå–æ•°å­—
        
        Args:
            text: åŒ…å«æ•°å­—çš„æ–‡æœ¬
            
        Returns:
            float: æå–çš„æ•°å­—ï¼Œå¦‚æœæå–å¤±è´¥è¿”å›None
        """
        if not text:
            return None

        # ç§»é™¤å¸¸è§çš„éæ•°å­—å­—ç¬¦
        cleaned_text = re.sub(r'[^\d.,\-+]', '', text.replace(',', '').replace(' ', ''))

        try:
            # å°è¯•è½¬æ¢ä¸ºæµ®ç‚¹æ•°
            return float(cleaned_text)
        except (ValueError, TypeError):
            # å°è¯•æå–ç¬¬ä¸€ä¸ªæ•°å­—
            numbers = re.findall(r'-?\d+\.?\d*', text)
            if numbers:
                try:
                    return float(numbers[0])
                except (ValueError, TypeError):
                    pass

            return None

    def close(self):
        """å…³é—­æŠ“å–å™¨"""
        if hasattr(self, 'browser_service'):
            self.browser_service.close()

    def __enter__(self):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        self.close()
