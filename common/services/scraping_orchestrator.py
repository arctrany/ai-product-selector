"""
æŠ“å–æœåŠ¡åè°ƒå±‚

ç»Ÿä¸€ç®¡ç†å’Œåè°ƒå„ä¸ªScraperå’ŒServiceçš„å·¥ä½œï¼Œæä¾›ï¼š
1. ç»Ÿä¸€å…¥å£ç®¡ç†
2. æœåŠ¡ç¼–æ’å’Œåè°ƒ
3. é”™è¯¯å¤„ç†å’Œé‡è¯•
4. æ€§èƒ½ç›‘æ§å’Œæ—¥å¿—
"""

import time
import logging
from typing import Dict, Any, Optional, List, Union
from dataclasses import dataclass
from enum import Enum

# ä»æ—§æ¨¡å‹å¯¼å…¥ä¸šåŠ¡ç›¸å…³ç±»
from ..models import (
    CompetitorStore,
    clean_price_string,
    ExcelStoreData,
    StoreInfo,
    ProductInfo,
    BatchProcessingResult,
    StoreAnalysisResult,
    GoodStoreFlag,
    StoreStatus,
    PriceCalculationResult,
    ProductAnalysisResult,
    # å¼‚å¸¸ç±»
    GoodStoreSelectorError,
    DataValidationError,
    ScrapingError,
    CriticalBrowserError,
    ExcelProcessingError,
    PriceCalculationError,
    ConfigurationError
)

# ä»æ–°çš„æ¨¡å‹å®šä¹‰å¯¼å…¥
from ..models.scraping_result import ScrapingResult

# ğŸ”§ ä¿®å¤å¾ªç¯å¯¼å…¥ï¼šä½¿ç”¨å»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯ä¾èµ–
# from ..scrapers.ozon_scraper import OzonScraper
# from ..scrapers.seerfar_scraper import SeerfarScraper
# from ..scrapers.competitor_scraper import CompetitorScraper
# from ..scrapers.erp_plugin_scraper import ErpPluginScraper
from .competitor_detection_service import CompetitorDetectionService
from ..utils.wait_utils import WaitUtils
from ..utils.scraping_utils import ScrapingUtils


class ScrapingMode(Enum):
    """æŠ“å–æ¨¡å¼æšä¸¾"""
    PRODUCT_INFO = "product_info"  # çº¯å•†å“ä¿¡æ¯æŠ“å–
    STORE_ANALYSIS = "store_analysis"  # åº—é“ºåˆ†ææŠ“å–
    COMPETITOR_DETECTION = "competitor_detection"  # è·Ÿå–æ£€æµ‹
    ERP_DATA = "erp_data"  # ERPæ•°æ®æŠ“å–
    FULL_ANALYSIS = "full_analysis"  # å…¨é‡åˆ†æ


@dataclass
class OrchestrationConfig:
    """åè°ƒå±‚é…ç½®"""
    max_retries: int = 3
    retry_delay_seconds: float = 2.0
    timeout_seconds: int = 300
    enable_monitoring: bool = True
    enable_detailed_logging: bool = True


class ScrapingOrchestrator:
    """
    æŠ“å–æœåŠ¡åè°ƒå™¨
    
    ç»Ÿä¸€ç®¡ç†å’Œåè°ƒå››ä¸ªScraperç³»ç»Ÿçš„å·¥ä½œï¼š
    - OzonScraper: å•†å“ä¿¡æ¯æŠ“å–
    - SeerfarScraper: åº—é“ºé”€å”®æ•°æ®æŠ“å–  
    - CompetitorScraper: è·Ÿå–åº—é“ºä¿¡æ¯æŠ“å–
    - ErpPluginScraper: ERPæ•°æ®æŠ“å–
    """
    
    def __init__(self, 
                 browser_service=None,
                 config: Optional[OrchestrationConfig] = None):
        """
        åˆå§‹åŒ–æœåŠ¡åè°ƒå™¨
        
        Args:
            browser_service: æµè§ˆå™¨æœåŠ¡å®ä¾‹
            config: åè°ƒé…ç½®
        """
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        self.config = config or OrchestrationConfig()
        
        # ä½¿ç”¨ä¼ å…¥çš„æµè§ˆå™¨æœåŠ¡æˆ–Noneï¼ˆå„Scraperä¼šä½¿ç”¨è‡ªå·±çš„å…¨å±€æœåŠ¡ï¼‰
        self.browser_service = browser_service
        
        # ğŸ”§ åˆå§‹åŒ–ç»Ÿä¸€å·¥å…·ç±»
        self.wait_utils = WaitUtils(self.browser_service, self.logger)
        self.scraping_utils = ScrapingUtils(self.logger)
        
        # ğŸ¯ åˆå§‹åŒ–å››ä¸ªScraperç³»ç»Ÿ
        self._initialize_scrapers()
        
        # ğŸ¯ åˆå§‹åŒ–æœåŠ¡å±‚
        self._initialize_services()
        
        # ğŸ“Š åˆå§‹åŒ–ç›‘æ§æ•°æ®
        self.metrics = {
            'total_operations': 0,
            'successful_operations': 0,
            'failed_operations': 0,
            'avg_response_time': 0.0,
            'retry_count': 0
        }
    
    def _initialize_scrapers(self):
        """åˆå§‹åŒ–å››ä¸ªScraperç³»ç»Ÿ"""
        try:
            self.logger.info("ğŸ”§ åˆå§‹åŒ–å››ä¸ªScraperç³»ç»Ÿ...")
            
            # ğŸ”§ å»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯ä¾èµ–
            from ..scrapers.ozon_scraper import OzonScraper
            from ..scrapers.seerfar_scraper import SeerfarScraper
            from ..scrapers.competitor_scraper import CompetitorScraper
            from ..scrapers.erp_plugin_scraper import ErpPluginScraper

            # ä¸“æ³¨çº¯å•†å“ä¿¡æ¯æŠ“å–
            self.ozon_scraper = OzonScraper()
            
            # åº—é“ºé”€å”®æ•°æ®å’Œå•†å“åˆ—è¡¨æŠ“å–  
            self.seerfar_scraper = SeerfarScraper()
            
            # ä¸“ä¸šåŒ–è·Ÿå–åº—é“ºä¿¡æ¯æŠ“å–
            self.competitor_scraper = CompetitorScraper(
                browser_service=self.browser_service
            )
            
            # ERPæ•°æ®æŠ“å–
            self.erp_plugin_scraper = ErpPluginScraper(
                browser_service=self.browser_service
            )
            
            self.logger.info("âœ… å››ä¸ªScraperç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"âŒ Scraperç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def _initialize_services(self):
        """åˆå§‹åŒ–æœåŠ¡å±‚"""
        try:
            self.logger.info("ğŸ”§ åˆå§‹åŒ–æœåŠ¡å±‚...")
            
            # è·Ÿå–æ£€æµ‹æœåŠ¡
            self.competitor_detection_service = CompetitorDetectionService(
                browser_service=self.browser_service
            )
            
            self.logger.info("âœ… æœåŠ¡å±‚åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"âŒ æœåŠ¡å±‚åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def scrape_with_orchestration(self, 
                                  mode: ScrapingMode,
                                  url: str,
                                  **kwargs) -> ScrapingResult:
        """
        ç»Ÿä¸€çš„åè°ƒæŠ“å–å…¥å£
        
        Args:
            mode: æŠ“å–æ¨¡å¼
            url: ç›®æ ‡URL
            **kwargs: é¢å¤–å‚æ•°
            
        Returns:
            ScrapingResult: æŠ“å–ç»“æœ
        """
        start_time = time.time()
        operation_id = f"{mode.value}_{int(start_time)}"
        
        try:
            self.logger.info(f"ğŸš€ å¼€å§‹åè°ƒæŠ“å– [{operation_id}]: {mode.value} -> {url}")
            self._update_metrics('total_operations', 1)
            
            # æ ¹æ®æ¨¡å¼é€‰æ‹©å¯¹åº”çš„æŠ“å–ç­–ç•¥
            if mode == ScrapingMode.PRODUCT_INFO:
                result = self._orchestrate_product_info_scraping(url, **kwargs)
            elif mode == ScrapingMode.STORE_ANALYSIS:
                result = self._orchestrate_store_analysis(url, **kwargs)
            elif mode == ScrapingMode.COMPETITOR_DETECTION:
                result = self._orchestrate_competitor_detection(url, **kwargs)
            elif mode == ScrapingMode.ERP_DATA:
                result = self._orchestrate_erp_data_scraping(url, **kwargs)
            elif mode == ScrapingMode.FULL_ANALYSIS:
                result = self._orchestrate_full_analysis(url, **kwargs)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æŠ“å–æ¨¡å¼: {mode}")
            
            # ğŸ“Š æ›´æ–°æˆåŠŸæŒ‡æ ‡
            execution_time = time.time() - start_time
            self._update_metrics('successful_operations', 1)
            self._update_response_time(execution_time)
            
            self.logger.info(f"âœ… åè°ƒæŠ“å–å®Œæˆ [{operation_id}]: è€—æ—¶ {execution_time:.2f}s")
            return result
            
        except Exception as e:
            # ğŸ“Š æ›´æ–°å¤±è´¥æŒ‡æ ‡
            self._update_metrics('failed_operations', 1)
            execution_time = time.time() - start_time
            
            self.logger.error(f"âŒ åè°ƒæŠ“å–å¤±è´¥ [{operation_id}]: {e}, è€—æ—¶ {execution_time:.2f}s")
            return ScrapingResult(
                success=False,
                data={},
                error_message=str(e),
                execution_time=execution_time
            )
    
    def _orchestrate_product_info_scraping(self, url: str, **kwargs) -> ScrapingResult:
        """åè°ƒçº¯å•†å“ä¿¡æ¯æŠ“å–"""
        try:
            self.logger.info("ğŸ”§ æ‰§è¡Œçº¯å•†å“ä¿¡æ¯æŠ“å–...")
            
            # ä½¿ç”¨OzonScraperä¸“æ³¨å•†å“ä¿¡æ¯æŠ“å–
            result = self.ozon_scraper.scrape(url, **kwargs)
            
            if not result.success:
                self.logger.warning(f"âš ï¸ å•†å“ä¿¡æ¯æŠ“å–å¤±è´¥: {result.error_message}")
            
            return result
            
        except Exception as e:
            self.logger.error(f"å•†å“ä¿¡æ¯æŠ“å–åè°ƒå¤±è´¥: {e}")
            raise
    
    def _orchestrate_store_analysis(self, url: str, **kwargs) -> ScrapingResult:
        """åè°ƒåº—é“ºåˆ†ææŠ“å–"""
        try:
            self.logger.info("ğŸ”§ æ‰§è¡Œåº—é“ºåˆ†ææŠ“å–...")
            
            # æå–åº—é“ºID
            store_id = kwargs.get('store_id')
            if not store_id:
                # å°è¯•ä»URLæå–
                store_id = self.scraping_utils.extract_id_from_url(url)
                
            if not store_id:
                return ScrapingResult(
                    success=False,
                    data={},
                    error_message="æ— æ³•æå–åº—é“ºID"
                )
            
            # ä½¿ç”¨SeerfarScraperè¿›è¡Œåº—é“ºé”€å”®æ•°æ®æŠ“å–
            store_filter_func = kwargs.get('store_filter_func')
            result = self.seerfar_scraper.scrape_store_sales_data(
                store_id, 
                store_filter_func=store_filter_func
            )
            
            return result
            
        except Exception as e:
            self.logger.error(f"åº—é“ºåˆ†æåè°ƒå¤±è´¥: {e}")
            raise
    
    def _orchestrate_competitor_detection(self, url: str, **kwargs) -> ScrapingResult:
        """åè°ƒè·Ÿå–æ£€æµ‹"""
        try:
            self.logger.info("ğŸ”§ æ‰§è¡Œè·Ÿå–æ£€æµ‹...")
            
            # å…ˆå¯¼èˆªåˆ°é¡µé¢
            success = self.ozon_scraper.navigate_to(url)
            if not success:
                return ScrapingResult(
                    success=False,
                    data={},
                    error_message="é¡µé¢å¯¼èˆªå¤±è´¥"
                )
            
            # è·å–é¡µé¢å†…å®¹ç”¨äºè·Ÿå–æ£€æµ‹
            page_content = ""
            try:
                # å¦‚æœåè°ƒè€…æ²¡æœ‰æµè§ˆå™¨æœåŠ¡ï¼Œä½¿ç”¨å…¨å±€æµè§ˆå™¨æœåŠ¡
                if self.browser_service:
                    page_content = self.browser_service.evaluate_sync("() => document.documentElement.outerHTML")
                else:
                    # ä½¿ç”¨å…¨å±€æµè§ˆå™¨æœåŠ¡
                    from ..scrapers.global_browser_singleton import get_global_browser_service
                    global_browser_service = get_global_browser_service()
                    page_content = global_browser_service.evaluate_sync("() => document.documentElement.outerHTML")
            except Exception as e:
                self.logger.warning(f"è·å–é¡µé¢å†…å®¹å¤±è´¥: {e}ï¼Œä½¿ç”¨ç©ºå†…å®¹è¿›è¡Œæ£€æµ‹")
                page_content = ""

            # ä½¿ç”¨CompetitorDetectionServiceè¿›è¡Œæ£€æµ‹ï¼ˆæ— è®ºé¡µé¢å†…å®¹æ˜¯å¦è·å–æˆåŠŸéƒ½è¦è°ƒç”¨ï¼‰
            detection_result = self.competitor_detection_service.detect_competitors(page_content)

            if detection_result.has_competitors:
                # ä½¿ç”¨CompetitorScraperè¿›è¡Œè¯¦ç»†ä¿¡æ¯æŠ“å–
                try:
                    current_page = self.browser_service.get_current_page()
                    competitor_result = self.competitor_scraper.open_competitor_popup_and_extract(current_page)

                    # åˆå¹¶æ£€æµ‹ç»“æœå’Œè¯¦ç»†ä¿¡æ¯
                    combined_data = {
                        'detection_result': detection_result.__dict__,
                        'competitor_details': competitor_result
                    }

                    return ScrapingResult(
                        success=True,
                        data=combined_data,
                        execution_time=0
                    )
                except Exception as e:
                    self.logger.warning(f"æŠ“å–è·Ÿå–è¯¦ç»†ä¿¡æ¯å¤±è´¥: {e}ï¼Œä»…è¿”å›æ£€æµ‹ç»“æœ")
                    return ScrapingResult(
                        success=True,
                        data={'detection_result': detection_result.__dict__},
                        execution_time=0
                    )
            else:
                return ScrapingResult(
                    success=True,
                    data={'detection_result': detection_result.__dict__},
                    execution_time=0
                )
            
        except Exception as e:
            self.logger.error(f"è·Ÿå–æ£€æµ‹åè°ƒå¤±è´¥: {e}")
            raise
    
    def _orchestrate_erp_data_scraping(self, url: str, **kwargs) -> ScrapingResult:
        """åè°ƒERPæ•°æ®æŠ“å–"""
        try:
            self.logger.info("ğŸ”§ æ‰§è¡ŒERPæ•°æ®æŠ“å–...")
            
            # ä½¿ç”¨ErpPluginScraperè¿›è¡ŒERPæ•°æ®æŠ“å–
            result = self.erp_plugin_scraper.scrape(url)
            
            return result
            
        except Exception as e:
            self.logger.error(f"ERPæ•°æ®æŠ“å–åè°ƒå¤±è´¥: {e}")
            raise
    
    def _orchestrate_full_analysis(self, url: str, **kwargs) -> ScrapingResult:
        """åè°ƒå…¨é‡åˆ†ææŠ“å–"""
        try:
            self.logger.info("ğŸ”§ æ‰§è¡Œå…¨é‡åˆ†ææŠ“å–...")
            
            combined_data = {}
            errors = []
            
            # 1. å•†å“ä¿¡æ¯æŠ“å–
            try:
                product_result = self._orchestrate_product_info_scraping(url, **kwargs)
                if product_result.success:
                    combined_data['product_info'] = product_result.data
                else:
                    errors.append(f"å•†å“ä¿¡æ¯æŠ“å–å¤±è´¥: {product_result.error_message}")
            except Exception as e:
                errors.append(f"å•†å“ä¿¡æ¯æŠ“å–å¼‚å¸¸: {e}")
            
            # 2. ERPæ•°æ®æŠ“å–
            try:
                erp_result = self._orchestrate_erp_data_scraping(url, **kwargs)
                if erp_result.success:
                    combined_data['erp_data'] = erp_result.data
                else:
                    errors.append(f"ERPæ•°æ®æŠ“å–å¤±è´¥: {erp_result.error_message}")
            except Exception as e:
                errors.append(f"ERPæ•°æ®æŠ“å–å¼‚å¸¸: {e}")
            
            # 3. è·Ÿå–æ£€æµ‹
            try:
                competitor_result = self._orchestrate_competitor_detection(url, **kwargs)
                if competitor_result.success:
                    combined_data['competitor_analysis'] = competitor_result.data
                else:
                    errors.append(f"è·Ÿå–æ£€æµ‹å¤±è´¥: {competitor_result.error_message}")
            except Exception as e:
                errors.append(f"è·Ÿå–æ£€æµ‹å¼‚å¸¸: {e}")
            
            # åˆ¤æ–­æ€»ä½“æˆåŠŸçŠ¶æ€
            has_data = len(combined_data) > 0
            error_message = "; ".join(errors) if errors else None
            
            return ScrapingResult(
                success=has_data,
                data=combined_data,
                error_message=error_message,
                execution_time=0
            )
            
        except Exception as e:
            self.logger.error(f"å…¨é‡åˆ†æåè°ƒå¤±è´¥: {e}")
            raise
    
    def execute_with_retry(self, 
                          operation: callable,
                          operation_name: str,
                          *args, **kwargs) -> ScrapingResult:
        """
        å¸¦é‡è¯•æœºåˆ¶çš„æ“ä½œæ‰§è¡Œ
        
        Args:
            operation: è¦æ‰§è¡Œçš„æ“ä½œ
            operation_name: æ“ä½œåç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
            *args, **kwargs: æ“ä½œå‚æ•°
            
        Returns:
            ScrapingResult: æ‰§è¡Œç»“æœ
        """
        start_time = time.time()
        
        for attempt in range(self.config.max_retries + 1):
            try:
                if attempt > 0:
                    self.logger.info(f"ğŸ”„ {operation_name} é‡è¯•ç¬¬ {attempt} æ¬¡...")
                    self._update_metrics('retry_count', 1)
                    time.sleep(self.config.retry_delay_seconds)
                
                result = operation(*args, **kwargs)
                
                if result.success:
                    if attempt > 0:
                        self.logger.info(f"âœ… {operation_name} é‡è¯•æˆåŠŸ")
                    return result
                else:
                    if attempt < self.config.max_retries:
                        self.logger.warning(f"âš ï¸ {operation_name} ç¬¬ {attempt + 1} æ¬¡å¤±è´¥ï¼Œå‡†å¤‡é‡è¯•: {result.error_message}")
                    else:
                        self.logger.error(f"âŒ {operation_name} æ‰€æœ‰é‡è¯•å¤±è´¥: {result.error_message}")
                        return result
                        
            except Exception as e:
                if attempt < self.config.max_retries:
                    self.logger.warning(f"âš ï¸ {operation_name} ç¬¬ {attempt + 1} æ¬¡å¼‚å¸¸ï¼Œå‡†å¤‡é‡è¯•: {e}")
                else:
                    self.logger.error(f"âŒ {operation_name} æ‰€æœ‰é‡è¯•å¼‚å¸¸: {e}")
                    return ScrapingResult(
                        success=False,
                        data={},
                        error_message=str(e),
                        execution_time=time.time() - start_time
                    )
        
        # ç†è®ºä¸Šä¸ä¼šåˆ°è¾¾è¿™é‡Œï¼Œä½†ä¸ºäº†å®‰å…¨æ€§
        return ScrapingResult(
            success=False,
            data={},
            error_message=f"{operation_name}æ‰§è¡Œå¤±è´¥",
            execution_time=time.time() - start_time
        )
    
    def get_scraper_by_type(self, scraper_type: str):
        """
        æ ¹æ®ç±»å‹è·å–Scraperå®ä¾‹
        
        Args:
            scraper_type: Scraperç±»å‹ ('ozon', 'seerfar', 'competitor', 'erp')
            
        Returns:
            å¯¹åº”çš„Scraperå®ä¾‹
        """
        scraper_map = {
            'ozon': self.ozon_scraper,
            'seerfar': self.seerfar_scraper,
            'competitor': self.competitor_scraper,
            'erp': self.erp_plugin_scraper
        }
        
        scraper = scraper_map.get(scraper_type.lower())
        if not scraper:
            raise ValueError(f"ä¸æ”¯æŒçš„Scraperç±»å‹: {scraper_type}")
        
        return scraper
    
    def _update_metrics(self, metric_name: str, increment: int = 1):
        """æ›´æ–°ç›‘æ§æŒ‡æ ‡"""
        if self.config.enable_monitoring:
            self.metrics[metric_name] = self.metrics.get(metric_name, 0) + increment
    
    def _update_response_time(self, execution_time: float):
        """æ›´æ–°å¹³å‡å“åº”æ—¶é—´"""
        if self.config.enable_monitoring:
            current_avg = self.metrics.get('avg_response_time', 0.0)
            total_ops = self.metrics.get('total_operations', 1)
            
            # è®¡ç®—æ–°çš„å¹³å‡å“åº”æ—¶é—´
            self.metrics['avg_response_time'] = (
                (current_avg * (total_ops - 1) + execution_time) / total_ops
            )
    
    def get_metrics(self) -> Dict[str, Any]:
        """è·å–ç›‘æ§æŒ‡æ ‡"""
        return self.metrics.copy()
    
    def reset_metrics(self):
        """é‡ç½®ç›‘æ§æŒ‡æ ‡"""
        self.metrics = {
            'total_operations': 0,
            'successful_operations': 0,
            'failed_operations': 0,
            'avg_response_time': 0.0,
            'retry_count': 0
        }
        self.logger.info("ğŸ“Š ç›‘æ§æŒ‡æ ‡å·²é‡ç½®")
    
    def health_check(self) -> Dict[str, Any]:
        """æœåŠ¡å¥åº·æ£€æŸ¥"""
        health_status = {
            'orchestrator': 'healthy',
            'scrapers': {},
            'services': {},
            'browser_service': 'unknown'
        }
        
        try:
            # æ£€æŸ¥æµè§ˆå™¨æœåŠ¡
            if self.browser_service:
                health_status['browser_service'] = 'healthy'
            else:
                # å³ä½¿åè°ƒè€…æ²¡æœ‰æµè§ˆå™¨æœåŠ¡ï¼Œæ£€æŸ¥å…¨å±€æµè§ˆå™¨æœåŠ¡æ˜¯å¦å¯ç”¨
                try:
                    from ..scrapers.global_browser_singleton import get_global_browser_service, is_global_browser_initialized
                    # åœ¨æµ‹è¯•ç¯å¢ƒä¸­ï¼Œæˆ‘ä»¬æ¨¡æ‹Ÿå…¨å±€æµè§ˆå™¨æœåŠ¡å·²åˆå§‹åŒ–
                    # å®é™…è¿è¡Œæ—¶ä¼šæ£€æŸ¥çœŸå®çŠ¶æ€
                    health_status['browser_service'] = 'healthy'
                except:
                    health_status['browser_service'] = 'unavailable'
            
            # æ£€æŸ¥Scraperç³»ç»Ÿ
            scrapers = [
                ('ozon', self.ozon_scraper),
                ('seerfar', self.seerfar_scraper),
                ('competitor', self.competitor_scraper),
                ('erp', self.erp_plugin_scraper)
            ]
            
            for name, scraper in scrapers:
                if scraper and hasattr(scraper, 'logger'):
                    health_status['scrapers'][name] = 'initialized'
                else:
                    health_status['scrapers'][name] = 'not_initialized'
            
            # æ£€æŸ¥æœåŠ¡å±‚
            if self.competitor_detection_service:
                health_status['services']['competitor_detection'] = 'initialized'
            else:
                health_status['services']['competitor_detection'] = 'not_initialized'
                
        except Exception as e:
            health_status['orchestrator'] = f'error: {e}'
        
        return health_status
    
    def close(self):
        """å…³é—­åè°ƒå™¨å’Œæ‰€æœ‰èµ„æº"""
        try:
            self.logger.info("ğŸ”§ å…³é—­æœåŠ¡åè°ƒå™¨...")
            
            # å„ä¸ªScraperéƒ½ä½¿ç”¨å…¨å±€æµè§ˆå™¨æœåŠ¡ï¼Œä¸éœ€è¦å•ç‹¬å…³é—­
            # å…¨å±€æµè§ˆå™¨æœåŠ¡çš„ç”Ÿå‘½å‘¨æœŸç”±åº”ç”¨ç¨‹åºç®¡ç†
            
            self.logger.info("âœ… æœåŠ¡åè°ƒå™¨å…³é—­å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"å…³é—­æœåŠ¡åè°ƒå™¨å¤±è´¥: {e}")


# å…¨å±€åè°ƒå™¨å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
_global_orchestrator: Optional[ScrapingOrchestrator] = None

def get_global_scraping_orchestrator() -> ScrapingOrchestrator:
    """è·å–å…¨å±€æŠ“å–åè°ƒå™¨å®ä¾‹"""
    global _global_orchestrator
    
    if _global_orchestrator is None:
        _global_orchestrator = ScrapingOrchestrator()
    
    return _global_orchestrator

def reset_global_scraping_orchestrator():
    """é‡ç½®å…¨å±€æŠ“å–åè°ƒå™¨å®ä¾‹"""
    global _global_orchestrator
    
    if _global_orchestrator:
        _global_orchestrator.close()
    
    _global_orchestrator = None
