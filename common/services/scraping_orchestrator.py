"""
æŠ“å–æœåŠ¡åè°ƒå±‚

ç»Ÿä¸€ç®¡ç†å’Œåè°ƒå„ä¸ªScraperå’ŒServiceçš„å·¥ä½œï¼Œæä¾›ï¼š
1. ç»Ÿä¸€å…¥å£ç®¡ç†
2. æœåŠ¡ç¼–æ’å’Œåè°ƒ
3. é”™è¯¯å¤„ç†å’Œé‡è¯•
4. æ€§èƒ½ç›‘æ§å’Œæ—¥å¿—
"""

import time
import logging
from typing import Dict, Any, Optional, List, Union
from dataclasses import dataclass
from enum import Enum

# ä»æ—§æ¨¡å‹å¯¼å…¥ä¸šåŠ¡ç›¸å…³ç±»
from ..models import (
    CompetitorStore,
    clean_price_string,
    ExcelStoreData,
    StoreInfo,
    ProductInfo,
    BatchProcessingResult,
    StoreAnalysisResult,
    GoodStoreFlag,
    StoreStatus,
    PriceCalculationResult,
    ProductAnalysisResult,
    # å¼‚å¸¸ç±»
    GoodStoreSelectorError,
    DataValidationError,
    ScrapingError,
    CriticalBrowserError,
    ExcelProcessingError,
    PriceCalculationError,
    ConfigurationError
)

# ä»æ–°çš„æ¨¡å‹å®šä¹‰å¯¼å…¥
from ..models.scraping_result import ScrapingResult

# ğŸ”§ ä¿®å¤å¾ªç¯å¯¼å…¥ï¼šä½¿ç”¨å»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯ä¾èµ–
# from ..scrapers.ozon_scraper import OzonScraper
# from ..scrapers.seerfar_scraper import SeerfarScraper
# from ..scrapers.competitor_scraper import CompetitorScraper
# from ..scrapers.erp_plugin_scraper import ErpPluginScraper
# CompetitorDetectionServiceç”±CompetitorScraperç®¡ç†ï¼Œåè°ƒå™¨ä¸ç›´æ¥ä¾èµ–
from ..utils.wait_utils import WaitUtils
from ..utils.scraping_utils import ScrapingUtils


class ScrapingMode(Enum):
    """æŠ“å–æ¨¡å¼æšä¸¾"""
    PRODUCT_INFO = "product_info"  # çº¯å•†å“ä¿¡æ¯æŠ“å–
    STORE_ANALYSIS = "store_analysis"  # åº—é“ºåˆ†ææŠ“å–
    ERP_DATA = "erp_data"  # ERPæ•°æ®æŠ“å–
    FULL_CHAIN = "full_chain"  # å…¨é‡åˆ†æ


@dataclass
class OrchestrationConfig:
    """åè°ƒå±‚é…ç½®"""
    max_retries: int = 3
    retry_delay_seconds: float = 2.0
    timeout_seconds: int = 300
    enable_monitoring: bool = True
    enable_detailed_logging: bool = True


class ScrapingOrchestrator:
    """
    æŠ“å–æœåŠ¡åè°ƒå™¨
    
    ç»Ÿä¸€ç®¡ç†å’Œåè°ƒå››ä¸ªScraperç³»ç»Ÿçš„å·¥ä½œï¼š
    - OzonScraper: å•†å“ä¿¡æ¯æŠ“å–
    - SeerfarScraper: åº—é“ºé”€å”®æ•°æ®æŠ“å–  
    - CompetitorScraper: è·Ÿå–åº—é“ºä¿¡æ¯æŠ“å–
    - ErpPluginScraper: ERPæ•°æ®æŠ“å–
    """
    
    def __init__(self, 
                 browser_service=None,
                 config: Optional[OrchestrationConfig] = None,
                 competitor_detection_service=None):
        """
        åˆå§‹åŒ–æœåŠ¡åè°ƒå™¨

        Args:
            browser_service: æµè§ˆå™¨æœåŠ¡å®ä¾‹
            config: åè°ƒé…ç½®
            competitor_detection_service: è·Ÿå–æ£€æµ‹æœåŠ¡å®ä¾‹ï¼ˆç”¨äºæµ‹è¯•æ³¨å…¥ï¼‰
        """
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        self.config = config or OrchestrationConfig()

        # ä½¿ç”¨ä¼ å…¥çš„æµè§ˆå™¨æœåŠ¡æˆ–Noneï¼ˆå„Scraperä¼šä½¿ç”¨è‡ªå·±çš„å…¨å±€æœåŠ¡ï¼‰
        self.browser_service = browser_service

        # ğŸ”§ åˆå§‹åŒ–ç»Ÿä¸€å·¥å…·ç±»
        self.wait_utils = WaitUtils(self.browser_service, self.logger)
        self.scraping_utils = ScrapingUtils(self.logger)

        # ğŸ¯ åˆå§‹åŒ–å››ä¸ªScraperç³»ç»Ÿ
        self._initialize_scrapers()

        # ğŸ¯ åˆå§‹åŒ–æœåŠ¡å±‚
        if competitor_detection_service:
            self.competitor_detection_service = competitor_detection_service
        else:
            self._initialize_services()
        
        # ğŸ“Š åˆå§‹åŒ–ç›‘æ§æ•°æ®
        self.metrics = {
            'total_operations': 0,
            'successful_operations': 0,
            'failed_operations': 0,
            'avg_response_time': 0.0,
            'retry_count': 0
        }
    
    def _initialize_scrapers(self):
        """åˆå§‹åŒ–å››ä¸ªScraperç³»ç»Ÿ"""
        try:
            self.logger.info("ğŸ”§ åˆå§‹åŒ–å››ä¸ªScraperç³»ç»Ÿ...")
            
            # ğŸ”§ å»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯ä¾èµ–
            from ..scrapers.ozon_scraper import OzonScraper
            from ..scrapers.seerfar_scraper import SeerfarScraper
            from ..scrapers.competitor_scraper import CompetitorScraper
            from ..scrapers.erp_plugin_scraper import ErpPluginScraper

            # ä¸“æ³¨çº¯å•†å“ä¿¡æ¯æŠ“å–
            self.ozon_scraper = OzonScraper()
            
            # åº—é“ºé”€å”®æ•°æ®å’Œå•†å“åˆ—è¡¨æŠ“å–  
            self.seerfar_scraper = SeerfarScraper()
            
            # ä¸“ä¸šåŒ–è·Ÿå–åº—é“ºä¿¡æ¯æŠ“å–
            self.competitor_scraper = CompetitorScraper(
                browser_service=self.browser_service
            )
            
            # ERPæ•°æ®æŠ“å–
            self.erp_plugin_scraper = ErpPluginScraper(
                browser_service=self.browser_service
            )
            
            self.logger.info("âœ… å››ä¸ªScraperç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"âŒ Scraperç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def _initialize_services(self):
        """åˆå§‹åŒ–æœåŠ¡å±‚"""
        try:
            self.logger.info("ğŸ”§ åˆå§‹åŒ–æœåŠ¡å±‚...")
            
            # åè°ƒå™¨ä¸ç›´æ¥ç®¡ç†ä¸šåŠ¡æœåŠ¡
            # CompetitorDetectionServiceç”±CompetitorScraperç®¡ç†
            
            self.logger.info("âœ… æœåŠ¡å±‚åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"âŒ æœåŠ¡å±‚åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def scrape_with_orchestration(self, 
                                  mode: ScrapingMode,
                                  url: str,
                                  **kwargs) -> ScrapingResult:
        """
        ç»Ÿä¸€çš„åè°ƒæŠ“å–å…¥å£
        
        Args:
            mode: æŠ“å–æ¨¡å¼
            url: ç›®æ ‡URL
            **kwargs: é¢å¤–å‚æ•°
            
        Returns:
            ScrapingResult: æŠ“å–ç»“æœ
        """
        start_time = time.time()
        # ç¡®ä¿modeæ˜¯ScrapingModeæšä¸¾ç±»å‹ï¼Œå¦‚æœæ˜¯å­—ç¬¦ä¸²åˆ™è½¬æ¢
        if isinstance(mode, str):
            try:
                mode = ScrapingMode(mode)
            except ValueError:
                raise ValueError(f"ä¸æ”¯æŒçš„æŠ“å–æ¨¡å¼å­—ç¬¦ä¸²: {mode}")

        operation_id = f"{mode.value}_{int(start_time)}"
        
        try:
            self.logger.info(f"ğŸš€ å¼€å§‹åè°ƒæŠ“å– [{operation_id}]: {mode.value} -> {url}")
            self._update_metrics('total_operations', 1)
            
            # æ ¹æ®æ¨¡å¼é€‰æ‹©å¯¹åº”çš„æŠ“å–ç­–ç•¥
            if mode == ScrapingMode.PRODUCT_INFO:
                result = self._orchestrate_product_info_scraping(url, **kwargs)
            elif mode == ScrapingMode.STORE_ANALYSIS:
                result = self._orchestrate_store_analysis(url, **kwargs)
            elif mode == ScrapingMode.FULL_CHAIN:
                result = self._orchestrate_product_full_analysis(url, **kwargs)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æŠ“å–æ¨¡å¼: {mode}")
            
            # ğŸ“Š æ›´æ–°æˆåŠŸæŒ‡æ ‡
            execution_time = time.time() - start_time
            self._update_metrics('successful_operations', 1)
            self._update_response_time(execution_time)
            
            self.logger.info(f"âœ… åè°ƒæŠ“å–å®Œæˆ [{operation_id}]: è€—æ—¶ {execution_time:.2f}s")
            return result
            
        except Exception as e:
            # ğŸ“Š æ›´æ–°å¤±è´¥æŒ‡æ ‡
            self._update_metrics('failed_operations', 1)
            execution_time = time.time() - start_time
            
            self.logger.error(f"âŒ åè°ƒæŠ“å–å¤±è´¥ [{operation_id}]: {e}, è€—æ—¶ {execution_time:.2f}s")
            return ScrapingResult.create_failure(
                error_message=str(e),
                execution_time=execution_time
            )
    
    def _orchestrate_product_info_scraping(self, url: str, **kwargs) -> ScrapingResult:
        """åè°ƒçº¯å•†å“ä¿¡æ¯æŠ“å–"""
        try:
            self.logger.info("ğŸ”§ æ‰§è¡Œçº¯å•†å“ä¿¡æ¯æŠ“å–...")
            
            # ä½¿ç”¨OzonScraperä¸“æ³¨å•†å“ä¿¡æ¯æŠ“å–
            result = self.ozon_scraper.scrape(url, **kwargs)
            
            if not result.success:
                self.logger.warning(f"âš ï¸ å•†å“ä¿¡æ¯æŠ“å–å¤±è´¥: {result.error_message}")
            
            return result
            
        except Exception as e:
            self.logger.error(f"å•†å“ä¿¡æ¯æŠ“å–åè°ƒå¤±è´¥: {e}")
            raise
    
    def _orchestrate_store_analysis(self, url: str, **kwargs) -> ScrapingResult:
        """åè°ƒåº—é“ºåˆ†ææŠ“å–"""
        try:
            self.logger.info("ğŸ”§ æ‰§è¡Œåº—é“ºåˆ†ææŠ“å–...")
            
            # ğŸ”§ ä¿®å¤ï¼šä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„store_idå‚æ•°ï¼Œé¿å…ä¾èµ–ä¸å­˜åœ¨çš„extract_id_from_urlæ–¹æ³•
            store_id = kwargs.get('store_id')
            if not store_id:
                return ScrapingResult(
                    success=False,
                    data={},
                    error_message="ç¼ºå°‘å¿…éœ€çš„store_idå‚æ•°"
                )

            self.logger.info(f"ğŸ¯ ä½¿ç”¨åº—é“ºIDè¿›è¡Œåˆ†æ: {store_id}")
            
            # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨å®Œæ•´çš„scrapeæ–¹æ³•ï¼Œæ”¯æŒinclude_productså‚æ•°
            # è·å–ä¼ å…¥çš„å‚æ•°
            include_products = kwargs.get('include_products', False)
            max_products = kwargs.get('max_products')
            product_filter_func = kwargs.get('product_filter_func')
            store_filter_func = kwargs.get('store_filter_func')

            self.logger.info(f"ğŸ“‹ åº—é“ºåˆ†æå‚æ•°: include_products={include_products}, max_products={max_products}")

            # ä½¿ç”¨SeerfarScraperçš„å®Œæ•´scrapeæ–¹æ³•ï¼Œæ”¯æŒé”€å”®æ•°æ®+å•†å“åˆ—è¡¨
            result = self.seerfar_scraper.scrape(
                store_id=store_id,
                include_products=include_products,
                max_products=max_products,
                product_filter_func=product_filter_func,
                store_filter_func=store_filter_func
            )
            
            return result
            
        except Exception as e:
            self.logger.error(f"åº—é“ºåˆ†æåè°ƒå¤±è´¥: {e}")
            raise
    

    
    def _orchestrate_product_erp_data_scraping(self, url: str, **kwargs) -> ScrapingResult:
        """åè°ƒERPæ•°æ®æŠ“å–"""
        try:
            self.logger.info("ğŸ”§ æ‰§è¡ŒERPæ•°æ®æŠ“å–...")
            
            # ä½¿ç”¨ErpPluginScraperè¿›è¡ŒERPæ•°æ®æŠ“å–
            result = self.erp_plugin_scraper.scrape(url)
            
            return result
            
        except Exception as e:
            self.logger.error(f"ERPæ•°æ®æŠ“å–åè°ƒå¤±è´¥: {e}")
            raise


    def _orchestrate_product_full_analysis(self, url: str, **kwargs) -> ScrapingResult:
        """
        ç®€åŒ–çš„å•†å“åˆ†æåè°ƒ - åªè´Ÿè´£æ•°æ®ç»„è£…
        1. è·å–åŸå•†å“æ•°æ®
        2. è·å–è·Ÿå–å•†å“æ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        3. ç»„è£…æ•°æ®ï¼Œä¸è¿›è¡Œé€‰æ‹©å†³ç­–
        """
        start_time = time.time()
        
        try:
            self.logger.info("ğŸ”§ å¼€å§‹æ‰§è¡Œå•†å“æ•°æ®ç»„è£…...")
            
            # Step 1: è·å–åŸå•†å“æ•°æ®
            primary_result = self.ozon_scraper.scrape(url, include_competitor=False, **kwargs)
            if not primary_result.success:
                self.logger.error("âŒ åŸå•†å“æ•°æ®è·å–å¤±è´¥")
                return ScrapingResult.create_failure(
                    error_message=f"åŸå•†å“æ•°æ®è·å–å¤±è´¥: {primary_result.error_message}",
                    execution_time=time.time() - start_time
                )
            
            primary_product = self._convert_to_product_info(primary_result.data, is_primary=True)
            
            # Step 2: è·å–è·Ÿå–å•†å“æ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            competitor_product = None
            competitors_list = []
            
            competitor_result = self.ozon_scraper.scrape(url, include_competitor=True, **kwargs)
            if competitor_result.success:
                first_competitor_id = competitor_result.data.get('first_competitor_product_id')
                competitors_list = competitor_result.data.get('competitors', [])
                
                if first_competitor_id:
                    competitor_url = self._build_competitor_url(first_competitor_id)
                    comp_result = self.ozon_scraper.scrape(competitor_url, skip_competitors=True, **kwargs)
                    if comp_result.success:
                        competitor_product = self._convert_to_product_info(comp_result.data, is_primary=False)
            
            # Step 3: ç»„è£…æ•°æ®ï¼Œä½¿ç”¨æ ‡å‡†åŒ–æ ¼å¼
            return ScrapingResult.create_success(
                data={
                    "primary_product": primary_product,
                    "competitor_product": competitor_product,
                    "competitors_list": competitors_list
                },
                execution_time=time.time() - start_time
            )
            
        except Exception as e:
            self.logger.error(f"å•†å“æ•°æ®ç»„è£…å¼‚å¸¸: {e}")
            return ScrapingResult.create_failure(
                error_message=f"æ•°æ®ç»„è£…å¼‚å¸¸: {str(e)}",
                execution_time=time.time() - start_time
            )
    
    def _convert_to_product_info(self, raw_data: Dict[str, Any], is_primary: bool):
        """
        å°†åŸå§‹æ•°æ®è½¬æ¢ä¸ºæ ‡å‡† ProductInfo å¯¹è±¡
        
        Args:
            raw_data: åŸå§‹æŠ“å–æ•°æ®
            is_primary: æ˜¯å¦ä¸ºåŸå•†å“
            
        Returns:
            ProductInfo: æ ‡å‡†åŒ–çš„å•†å“ä¿¡æ¯å¯¹è±¡
        """
        from ..models.business_models import ProductInfo
        
        return ProductInfo(
            product_id=raw_data.get('product_id'),
            product_url=raw_data.get('product_url'),
            image_url=raw_data.get('product_image'),
            
            # ä»·æ ¼ä¿¡æ¯
            green_price=raw_data.get('green_price'),
            black_price=raw_data.get('black_price'),
            
            # ERPæ•°æ®
            source_price=raw_data.get('erp_data', {}).get('purchase_price') if raw_data.get('erp_data') else raw_data.get('source_price'),
            commission_rate=raw_data.get('erp_data', {}).get('commission_rate') if raw_data.get('erp_data') else raw_data.get('commission_rate'),
            weight=raw_data.get('erp_data', {}).get('weight') if raw_data.get('erp_data') else raw_data.get('weight'),
            length=raw_data.get('erp_data', {}).get('length') if raw_data.get('erp_data') else raw_data.get('length'),
            width=raw_data.get('erp_data', {}).get('width') if raw_data.get('erp_data') else raw_data.get('width'),
            height=raw_data.get('erp_data', {}).get('height') if raw_data.get('erp_data') else raw_data.get('height'),
            shelf_days=raw_data.get('erp_data', {}).get('shelf_days') if raw_data.get('erp_data') else raw_data.get('shelf_days'),
            
            # æ ‡è¯†å­—æ®µ
            source_matched=bool(raw_data.get('erp_data', {}).get('purchase_price') if raw_data.get('erp_data') else raw_data.get('source_price'))
        )
    
    def _build_competitor_url(self, competitor_product_id: str) -> str:
        """
        æ„å»ºè·Ÿå–å•†å“URL
        
        Args:
            competitor_product_id: è·Ÿå–å•†å“ID
            
        Returns:
            str: è·Ÿå–å•†å“URL
        """
        base_url = "https://www.ozon.ru/product/"
        return f"{base_url}{competitor_product_id}/"

    

    
    
    def get_scraper_by_type(self, scraper_type: str):
        """
        æ ¹æ®ç±»å‹è·å–Scraperå®ä¾‹
        
        Args:
            scraper_type: Scraperç±»å‹ ('ozon', 'seerfar', 'competitor', 'erp')
            
        Returns:
            å¯¹åº”çš„Scraperå®ä¾‹
        """
        scraper_map = {
            'ozon': self.ozon_scraper,
            'seerfar': self.seerfar_scraper,
            'competitor': self.competitor_scraper,
            'erp': self.erp_plugin_scraper
        }
        
        scraper = scraper_map.get(scraper_type.lower())
        if not scraper:
            raise ValueError(f"ä¸æ”¯æŒçš„Scraperç±»å‹: {scraper_type}")
        
        return scraper
    
    def _update_metrics(self, metric_name: str, increment: int = 1):
        """æ›´æ–°ç›‘æ§æŒ‡æ ‡"""
        if self.config.enable_monitoring:
            self.metrics[metric_name] = self.metrics.get(metric_name, 0) + increment
    
    def _update_response_time(self, execution_time: float):
        """æ›´æ–°å¹³å‡å“åº”æ—¶é—´"""
        if self.config.enable_monitoring:
            current_avg = self.metrics.get('avg_response_time', 0.0)
            total_ops = self.metrics.get('total_operations', 1)
            
            # è®¡ç®—æ–°çš„å¹³å‡å“åº”æ—¶é—´
            self.metrics['avg_response_time'] = (
                (current_avg * (total_ops - 1) + execution_time) / total_ops
            )
    
    def get_metrics(self) -> Dict[str, Any]:
        """è·å–ç›‘æ§æŒ‡æ ‡"""
        return self.metrics.copy()
    
    def reset_metrics(self):
        """é‡ç½®ç›‘æ§æŒ‡æ ‡"""
        self.metrics = {
            'total_operations': 0,
            'successful_operations': 0,
            'failed_operations': 0,
            'avg_response_time': 0.0,
            'retry_count': 0
        }
        self.logger.info("ğŸ“Š ç›‘æ§æŒ‡æ ‡å·²é‡ç½®")
    
    def health_check(self) -> Dict[str, Any]:
        """æœåŠ¡å¥åº·æ£€æŸ¥"""
        health_status = {
            'orchestrator': 'healthy',
            'scrapers': {},
            'services': {},
            'browser_service': 'unknown'
        }
        
        try:
            # æ£€æŸ¥æµè§ˆå™¨æœåŠ¡
            if self.browser_service:
                health_status['browser_service'] = 'healthy'
            else:
                # å³ä½¿åè°ƒè€…æ²¡æœ‰æµè§ˆå™¨æœåŠ¡ï¼Œæ£€æŸ¥å…¨å±€æµè§ˆå™¨æœåŠ¡æ˜¯å¦å¯ç”¨
                try:
                    from rpa.browser.browser_service import SimplifiedBrowserService
                    # åœ¨æµ‹è¯•ç¯å¢ƒä¸­ï¼Œæˆ‘ä»¬æ¨¡æ‹Ÿå…¨å±€æµè§ˆå™¨æœåŠ¡å·²åˆå§‹åŒ–
                    # å®é™…è¿è¡Œæ—¶ä¼šæ£€æŸ¥çœŸå®çŠ¶æ€
                    health_status['browser_service'] = 'healthy'
                except:
                    health_status['browser_service'] = 'unavailable'
            
            # æ£€æŸ¥Scraperç³»ç»Ÿ
            scrapers = [
                ('ozon', self.ozon_scraper),
                ('seerfar', self.seerfar_scraper),
                ('competitor', self.competitor_scraper),
                ('erp', self.erp_plugin_scraper)
            ]
            
            for name, scraper in scrapers:
                if scraper and hasattr(scraper, 'logger'):
                    health_status['scrapers'][name] = 'initialized'
                else:
                    health_status['scrapers'][name] = 'not_initialized'
            
            # æœåŠ¡å±‚ç”±å„ä¸ªscraperç®¡ç†
            health_status['services']['note'] = 'services_managed_by_scrapers'
                
        except Exception as e:
            health_status['orchestrator'] = f'error: {e}'
        
        return health_status
    
    def close(self):
        """å…³é—­åè°ƒå™¨å’Œæ‰€æœ‰èµ„æº"""
        try:
            self.logger.info("ğŸ”§ å…³é—­æœåŠ¡åè°ƒå™¨...")
            
            # å„ä¸ªScraperéƒ½ä½¿ç”¨å…¨å±€æµè§ˆå™¨æœåŠ¡ï¼Œä¸éœ€è¦å•ç‹¬å…³é—­
            # å…¨å±€æµè§ˆå™¨æœåŠ¡çš„ç”Ÿå‘½å‘¨æœŸç”±åº”ç”¨ç¨‹åºç®¡ç†
            
            self.logger.info("âœ… æœåŠ¡åè°ƒå™¨å…³é—­å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"å…³é—­æœåŠ¡åè°ƒå™¨å¤±è´¥: {e}")


# å…¨å±€åè°ƒå™¨å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
_global_orchestrator: Optional[ScrapingOrchestrator] = None

def get_global_scraping_orchestrator() -> ScrapingOrchestrator:
    """è·å–å…¨å±€æŠ“å–åè°ƒå™¨å®ä¾‹"""
    global _global_orchestrator
    
    if _global_orchestrator is None:
        _global_orchestrator = ScrapingOrchestrator()
    
    return _global_orchestrator

def reset_global_scraping_orchestrator():
    """é‡ç½®å…¨å±€æŠ“å–åè°ƒå™¨å®ä¾‹"""
    global _global_orchestrator
    
    if _global_orchestrator:
        _global_orchestrator.close()
    
    _global_orchestrator = None
