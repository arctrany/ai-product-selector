"""
CompetitorScraper é›†æˆæµ‹è¯•

ä½¿ç”¨çœŸå®çš„æµè§ˆå™¨æœåŠ¡å’Œç½‘ç»œè¯·æ±‚ï¼Œæµ‹è¯•ä¸å¤–éƒ¨ç³»ç»Ÿçš„é›†æˆã€‚

ğŸ”§ ä¿®å¤è¯´æ˜ï¼š
- æŒ‰ç…§xpå‘½ä»¤çš„æˆåŠŸæ¨¡å¼é…ç½®æµè§ˆå™¨å¯åŠ¨ç¯å¢ƒ
- ä½¿ç”¨ä¸xpå‘½ä»¤ç›¸åŒçš„Edgeæµè§ˆå™¨é…ç½®å’Œåæ£€æµ‹å‚æ•°
- ç¡®ä¿Profileæ£€æµ‹å’Œç”¨æˆ·çŠ¶æ€ä¿æŒåŠŸèƒ½æ­£å¸¸å·¥ä½œ
"""

import pytest
import json
import time
import os
import sys
from pathlib import Path
from unittest.mock import patch
from typing import List, Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„ä»¥è§£å†³å¯¼å…¥é—®é¢˜
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from common.scrapers.competitor_scraper import CompetitorScraper
from common.models.scraping_result import ScrapingResult
from common.config.ozon_selectors_config import OzonSelectorsConfig
from rpa.browser.browser_service import SimplifiedBrowserService


def load_test_cases() -> List[Dict[str, Any]]:
    """åŠ è½½æµ‹è¯•ç”¨ä¾‹æ•°æ®

    ä» tests/test_data/ozon_test_cases.json æ–‡ä»¶ä¸­åŠ è½½æµ‹è¯•ç”¨ä¾‹ã€‚
    å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¿”å›é»˜è®¤æµ‹è¯•ç”¨ä¾‹ã€‚

    Returns:
        List[Dict[str, Any]]: æµ‹è¯•ç”¨ä¾‹åˆ—è¡¨
    """
    try:
        test_data_file = Path(__file__).parent.parent / "test_data" / "ozon_test_cases.json"
        with open(test_data_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            return data.get('test_cases', [])
    except FileNotFoundError:
        # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¿”å›é»˜è®¤æµ‹è¯•ç”¨ä¾‹
        return [
            {
                "id": "default_test_case",
                "name": "é»˜è®¤æµ‹è¯•ç”¨ä¾‹",
                "url": "https://www.ozon.ru/product/1176594312",
                "description": "é»˜è®¤æµ‹è¯•å•†å“",
                "expected": {
                    "green_price": None,
                    "black_price": None,
                    "competitor_count": 0,
                    "has_competitors": False,
                    "has_image": True
                },
                "test_options": {
                    "include_competitors": True,
                    "max_competitors": 10
                }
            }
        ]


@pytest.mark.integration
class TestCompetitorScraperIntegration:
    """CompetitorScraper é›†æˆæµ‹è¯•ç±»"""

    def setup_method(self):
        """æµ‹è¯•æ–¹æ³•è®¾ç½® - æŒ‰ç…§xpå‘½ä»¤çš„æˆåŠŸæ¨¡å¼é…ç½®æµè§ˆå™¨ç¯å¢ƒ"""
        # ğŸ¯ å…³é”®ä¿®å¤ï¼šè®¾ç½®ä¸xpå‘½ä»¤ç›¸åŒçš„ç¯å¢ƒå˜é‡é…ç½®
        # è¿™äº›ç¯å¢ƒå˜é‡ä¼šè¢«SimplifiedBrowserService._create_default_global_config()ä½¿ç”¨
        os.environ['PREFERRED_BROWSER'] = 'edge'  # æ˜ç¡®æŒ‡å®šä½¿ç”¨Edgeæµè§ˆå™¨
        os.environ['BROWSER_DEBUG_PORT'] = '9222'  # è®¾ç½®CDPè°ƒè¯•ç«¯å£
        os.environ['BROWSER_HEADLESS'] = 'false'  # ç¡®ä¿éæ— å¤´æ¨¡å¼ï¼Œä¾¿äºè°ƒè¯•

        print(f"ğŸ” æµè§ˆå™¨é…ç½®ç¯å¢ƒå˜é‡å·²è®¾ç½®:")
        print(f"   PREFERRED_BROWSER: {os.environ.get('PREFERRED_BROWSER')}")
        print(f"   BROWSER_DEBUG_PORT: {os.environ.get('BROWSER_DEBUG_PORT')}")
        print(f"   BROWSER_HEADLESS: {os.environ.get('BROWSER_HEADLESS')}")

        # åˆ›å»ºçœŸå®çš„é€‰æ‹©å™¨é…ç½®
        self.selectors_config = OzonSelectorsConfig()

        # ğŸ¯ å…³é”®ä¿®å¤ï¼šä½¿ç”¨å…¨å±€æµè§ˆå™¨å•ä¾‹ï¼ˆæŒ‰ç…§xpå‘½ä»¤æ¨¡å¼ï¼‰
        print(f"ğŸš€ è·å–å…¨å±€æµè§ˆå™¨æœåŠ¡å®ä¾‹...")
        self.browser_service = SimplifiedBrowserService.get_global_instance()

        # ğŸ”§ ä½¿ç”¨åŒæ­¥æ–¹æ³•å¯åŠ¨æµè§ˆå™¨æœåŠ¡ï¼ˆé¿å…å¼‚æ­¥åŒæ­¥æ··åˆè°ƒç”¨é—®é¢˜ï¼‰
        print(f"ğŸš€ æ­£åœ¨å¯åŠ¨æµè§ˆå™¨æœåŠ¡...")
        try:
            # æŒ‰ç…§xpå‘½ä»¤æ¨¡å¼ä½¿ç”¨åŒæ­¥å¯åŠ¨
            success = self.browser_service.start_browser_sync()
            if success:
                print(f"âœ… æµè§ˆå™¨æœåŠ¡å¯åŠ¨æˆåŠŸ")
            else:
                print(f"âŒ æµè§ˆå™¨æœåŠ¡å¯åŠ¨å¤±è´¥")
                # å¦‚æœå¯åŠ¨å¤±è´¥ï¼Œå°è¯•å¯¼èˆªè§¦å‘åˆå§‹åŒ–
                print(f"ğŸ”§ å°è¯•é€šè¿‡å¯¼èˆªè§¦å‘æµè§ˆå™¨åˆå§‹åŒ–...")
                nav_success = self.browser_service.navigate_to_sync("about:blank")
                if nav_success:
                    print(f"âœ… é€šè¿‡å¯¼èˆªæˆåŠŸè§¦å‘æµè§ˆå™¨åˆå§‹åŒ–")
                else:
                    print(f"âŒ å¯¼èˆªè§¦å‘åˆå§‹åŒ–å¤±è´¥")

        except Exception as e:
            print(f"âŒ æµè§ˆå™¨æœåŠ¡å¯åŠ¨å¼‚å¸¸: {e}")

        # ğŸ¯ å…³é”®ä¿®å¤ï¼šåˆ›å»ºCompetitorScraperå®ä¾‹æ—¶ä½¿ç”¨å…¨å±€æµè§ˆå™¨æœåŠ¡
        self.scraper = CompetitorScraper(
            selectors_config=self.selectors_config,
            browser_service=self.browser_service
        )

        # éªŒè¯æµè§ˆå™¨æœåŠ¡çŠ¶æ€
        if self.scraper.browser_service:
            print(f"âœ… CompetitorScraperæµè§ˆå™¨æœåŠ¡å·²è®¾ç½®")
            browser_driver = getattr(self.scraper.browser_service, 'browser_driver', None)
            if browser_driver:
                print(f"âœ… æµè§ˆå™¨é©±åŠ¨å·²åˆå§‹åŒ–: {browser_driver.is_initialized()}")
            else:
                print(f"âš ï¸  æµè§ˆå™¨é©±åŠ¨æœªæ‰¾åˆ°")

        # åŠ è½½æµ‹è¯•æ•°æ®
        self.test_cases = load_test_cases()
        self.test_urls = self._convert_test_cases_to_urls()

    def teardown_method(self):
        """æµ‹è¯•æ–¹æ³•æ¸…ç† - æ¯ä¸ªæµ‹è¯•æ–¹æ³•æ‰§è¡Œåè°ƒç”¨"""
        # ğŸ”§ æ¸…ç†ç¯å¢ƒå˜é‡é…ç½®ï¼ˆå¯é€‰ï¼Œé¿å…å½±å“å…¶ä»–æµ‹è¯•ï¼‰
        # é€šå¸¸ä¿ç•™ç¯å¢ƒå˜é‡é…ç½®ï¼Œå› ä¸ºå…¶ä»–æµ‹è¯•ä¹Ÿå¯èƒ½éœ€è¦ç›¸åŒé…ç½®

        # æ¸…ç†ï¼šå…³é—­æµè§ˆå™¨æœåŠ¡ï¼ˆå¦‚æœéœ€è¦ï¼‰
        # æ³¨æ„ï¼šé€šå¸¸ä¸éœ€è¦å…³é—­å…¨å±€å•ä¾‹æœåŠ¡
        time.sleep(1)  # ç»™ç³»ç»Ÿä¸€äº›æ—¶é—´è¿›è¡Œæ¸…ç†

    def _convert_test_cases_to_urls(self):
        """å°†æµ‹è¯•ç”¨ä¾‹è½¬æ¢ä¸ºURLåˆ—è¡¨æ ¼å¼ï¼Œå…¼å®¹ç°æœ‰æµ‹è¯•æ–¹æ³•"""
        test_urls = []
        for test_case in self.test_cases:
            test_urls.append({
                "url": test_case["url"],
                "description": test_case.get("description", test_case["name"]),
                "expected_competitors": test_case["expected"].get("has_competitors", False),
                "max_wait_time": test_case["test_options"].get("max_competitors", 10) * 3 + 15,  # åŠ¨æ€è®¡ç®—è¶…æ—¶æ—¶é—´
                "test_case_id": test_case["id"],
                "test_case_name": test_case["name"]
            })
        return test_urls

    # ========== åŸºç¡€é›†æˆæµ‹è¯• ==========

    @pytest.mark.network
    def test_scraper_initialization_with_real_services(self):
        """æµ‹è¯•ä½¿ç”¨çœŸå®æœåŠ¡çš„åˆå§‹åŒ–"""
        # Act
        scraper = CompetitorScraper()

        # Assert
        assert scraper.browser_service is not None
        assert scraper.selectors_config is not None
        assert scraper.wait_utils is not None
        assert scraper.scraping_utils is not None

    @pytest.mark.network
    @pytest.mark.slow
    def test_real_browser_service_integration(self):
        """æµ‹è¯•ä¸çœŸå®æµè§ˆå™¨æœåŠ¡çš„é›†æˆï¼ŒåŒ…å«URLè·³è½¬åŠŸèƒ½éªŒè¯"""
        # Arrange
        test_url = self.test_urls[0]["url"]

        # ğŸ¯ å…³é”®éªŒè¯ï¼šæ£€æŸ¥æµè§ˆå™¨æœåŠ¡æ˜¯å¦æŒ‰xpå‘½ä»¤æ¨¡å¼æ­£ç¡®åˆå§‹åŒ–
        print(f"ğŸ” éªŒè¯æµè§ˆå™¨æœåŠ¡çŠ¶æ€:")
        print(f"   browser_serviceå­˜åœ¨: {self.scraper.browser_service is not None}")

        if self.scraper.browser_service:
            # éªŒè¯æµè§ˆå™¨é©±åŠ¨æ˜¯å¦æ­£ç¡®åˆå§‹åŒ–
            browser_driver = getattr(self.scraper.browser_service, 'browser_driver', None)
            print(f"   browser_driverå­˜åœ¨: {browser_driver is not None}")

            if browser_driver:
                print(f"   browser_driverå·²åˆå§‹åŒ–: {browser_driver.is_initialized()}")

        # ğŸ¯ å…³é”®ä¿®å¤ï¼šæµ‹è¯•URLè·³è½¬åŠŸèƒ½
        print(f"ğŸŒ æµ‹è¯•URLè·³è½¬åŠŸèƒ½: {test_url}")
        try:
            # ä½¿ç”¨æµè§ˆå™¨æœåŠ¡å¯¼èˆªåˆ°æµ‹è¯•URL
            nav_success = self.scraper.browser_service.navigate_to_sync(test_url, wait_until="domcontentloaded")
            print(f"   URLå¯¼èˆªç»“æœ: {nav_success}")

            if nav_success:
                # éªŒè¯é¡µé¢æ˜¯å¦æˆåŠŸåŠ è½½
                page = self.scraper.browser_service.get_page()
                assert page is not None, "å¯¼èˆªæˆåŠŸä½†get_page()è¿”å›None"

                # è·å–å½“å‰é¡µé¢URLéªŒè¯å¯¼èˆªæ˜¯å¦æˆåŠŸ
                try:
                    current_url = page.url
                    print(f"   å½“å‰é¡µé¢URL: {current_url}")
                    # éªŒè¯URLæ˜¯å¦åŒ…å«é¢„æœŸçš„åŸŸå
                    assert "ozon.ru" in current_url, f"é¡µé¢URLä¸æ­£ç¡®: {current_url}"
                except Exception as url_e:
                    print(f"   è·å–é¡µé¢URLå¤±è´¥: {url_e}")
            else:
                print(f"   âš ï¸  URLå¯¼èˆªå¤±è´¥ï¼Œä½†ç»§ç»­æµ‹è¯•")

        except Exception as nav_e:
            print(f"   âŒ URLå¯¼èˆªå¼‚å¸¸: {nav_e}")

        # Act - é€šè¿‡å®é™…æŠ“å–æ¥æµ‹è¯•æµè§ˆå™¨æœåŠ¡
        result = self.scraper.scrape(url=test_url, max_competitors=1)

        # Assert - éªŒè¯æµè§ˆå™¨æœåŠ¡èƒ½å¤Ÿæ­£å¸¸å·¥ä½œ
        assert isinstance(result, ScrapingResult)

        # ğŸ¯ å…³é”®éªŒè¯ï¼šç¡®ä¿get_page()ä¸å†è¿”å›None
        page = self.scraper.browser_service.get_page()
        assert page is not None, "æµè§ˆå™¨æœåŠ¡çš„get_page()è¿”å›äº†Noneï¼Œè¯´æ˜Edgeæµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥"

        print(f"âœ… æµè§ˆå™¨æœåŠ¡é›†æˆæµ‹è¯•é€šè¿‡ï¼Œpageå¯¹è±¡: {type(page)}")

    # ========== çœŸå®URLæµ‹è¯• ==========

    @pytest.mark.network
    @pytest.mark.slow
    @pytest.mark.parametrize("test_case", [
        pytest.param(
            test_case,
            id=f"{test_case['id']}_{test_case['name'][:20]}"
        ) for test_case in load_test_cases()
    ])
    def test_scrape_real_urls(self, test_case):
        """å‚æ•°åŒ–æµ‹è¯•çœŸå®URLçš„æŠ“å–

        Args:
            test_case: æ¥è‡ª test_data/ozon_test_cases.json çš„æµ‹è¯•ç”¨ä¾‹
        """
        # Arrange
        url = test_case["url"]
        expected_competitors = test_case["expected"].get("has_competitors", False)
        max_competitors = test_case["test_options"].get("max_competitors", 10)
        max_wait_time = max_competitors * 3 + 15  # åŠ¨æ€è®¡ç®—è¶…æ—¶æ—¶é—´

        # ğŸ¯ å…³é”®ä¿®å¤ï¼šåœ¨scrapeå‰å…ˆå¯¼èˆªåˆ°ç›®æ ‡é¡µé¢
        print(f"ğŸŒ å¯¼èˆªåˆ°æµ‹è¯•URL: {url}")
        nav_success = self.scraper.browser_service.navigate_to_sync(url, wait_until="domcontentloaded")
        print(f"   å¯¼èˆªç»“æœ: {nav_success}")

        # Act
        start_time = time.time()
        # æ„å»º contextï¼ŒåŒ…å«é¢„æœŸçš„ç«å“æ•°é‡ä»¥è§¦å‘å±•å¼€é€»è¾‘
        context = {
            'competitor_cnt': test_case["expected"].get("competitor_count", 0)
        }
        result = self.scraper.scrape(url=url, max_competitors=max_competitors, context=context)
        execution_time = time.time() - start_time

        # Assert
        assert isinstance(result, ScrapingResult)
        assert execution_time < max_wait_time, f"æ‰§è¡Œæ—¶é—´è¶…è¿‡ {max_wait_time} ç§’"
        
        if expected_competitors:
            # å¦‚æœæœŸæœ›æœ‰ç«å“ï¼Œæ£€æŸ¥ç»“æœ
            if result.success:
                assert isinstance(result.data, list)
                # æ³¨æ„ï¼šçœŸå®ç½‘é¡µå¯èƒ½æ²¡æœ‰ç«å“ï¼Œæ‰€ä»¥ä¸å¼ºåˆ¶è¦æ±‚æœ‰æ•°æ®
            else:
                # å¦‚æœå¤±è´¥ï¼Œè‡³å°‘åº”è¯¥æœ‰é”™è¯¯ä¿¡æ¯
                assert result.error is not None
        
        # éªŒè¯ç»“æœç»“æ„çš„å®Œæ•´æ€§
        assert hasattr(result, 'success')
        assert hasattr(result, 'data')
        assert hasattr(result, 'execution_time')

    @pytest.mark.network
    @pytest.mark.slow
    @pytest.mark.timeout(120)  # 2åˆ†é’Ÿè¶…æ—¶ï¼Œè€ƒè™‘å±•å¼€åŠŸèƒ½çš„å¤æ‚æ€§
    def test_scrape_with_expand_functionality(self):
        """æµ‹è¯•å±•å¼€åŠŸèƒ½çš„çœŸå®é›†æˆ"""
        # Arrange
        url = self.test_urls[0]["url"]
        context = {'competitor_count': 15}  # æ¨¡æ‹Ÿéœ€è¦å±•å¼€çš„åœºæ™¯

        # ğŸ¯ å…³é”®ä¿®å¤ï¼šåœ¨scrapeå‰å…ˆå¯¼èˆªåˆ°ç›®æ ‡é¡µé¢
        print(f"ğŸŒ å¯¼èˆªåˆ°æµ‹è¯•URL: {url}")
        nav_success = self.scraper.browser_service.navigate_to_sync(url, wait_until="domcontentloaded")
        print(f"   å¯¼èˆªç»“æœ: {nav_success}")

        # Act
        result = self.scraper.scrape(url=url, context=context, max_competitors=10)

        # Assert
        assert isinstance(result, ScrapingResult)
        # éªŒè¯å±•å¼€åŠŸèƒ½æ˜¯å¦è¢«æ­£ç¡®è°ƒç”¨ï¼ˆé€šè¿‡ç»“æœç»“æ„ï¼‰
        if result.success:
            assert isinstance(result.data, list)

    # ========== ç«¯åˆ°ç«¯æµç¨‹æµ‹è¯• ==========

    @pytest.mark.network
    @pytest.mark.slow
    def test_end_to_end_competitor_extraction(self):
        """ç«¯åˆ°ç«¯ç«å“æå–æµç¨‹æµ‹è¯•"""
        # Arrange
        url = self.test_urls[0]["url"]

        # ğŸ¯ å…³é”®ä¿®å¤ï¼šåœ¨scrapeå‰å…ˆå¯¼èˆªåˆ°ç›®æ ‡é¡µé¢
        print(f"ğŸŒ å¯¼èˆªåˆ°æµ‹è¯•URL: {url}")
        nav_success = self.scraper.browser_service.navigate_to_sync(url, wait_until="domcontentloaded")
        print(f"   å¯¼èˆªç»“æœ: {nav_success}")

        # Act - æ‰§è¡Œå®Œæ•´çš„æŠ“å–æµç¨‹
        result = self.scraper.scrape(url=url, max_competitors=3)

        # Assert - éªŒè¯å®Œæ•´çš„æ•°æ®ç»“æ„
        assert isinstance(result, ScrapingResult)
        
        if result.success and result.data:
            # éªŒè¯æ¯ä¸ªç«å“æ•°æ®çš„ç»“æ„
            for competitor in result.data:
                assert isinstance(competitor, dict)
                # éªŒè¯å…³é”®å­—æ®µå­˜åœ¨
                expected_fields = ['ranking']
                for field in expected_fields:
                    assert field in competitor, f"ç¼ºå°‘å­—æ®µ: {field}"
                
                # éªŒè¯æ•°æ®ç±»å‹
                if 'ranking' in competitor:
                    assert isinstance(competitor['ranking'], int)
                if 'store_name' in competitor:
                    assert isinstance(competitor['store_name'], str)
                if 'price' in competitor:
                    assert isinstance(competitor['price'], (str, int, float))

    @pytest.mark.network
    @pytest.mark.slow
    def test_multiple_urls_sequential(self):
        """æµ‹è¯•é¡ºåºå¤„ç†å¤šä¸ªURL"""
        results = []
        
        for test_data in self.test_urls[:2]:  # åªæµ‹è¯•å‰ä¸¤ä¸ªURL
            # ğŸ¯ å…³é”®ä¿®å¤ï¼šåœ¨scrapeå‰å…ˆå¯¼èˆªåˆ°ç›®æ ‡é¡µé¢
            url = test_data["url"]
            print(f"ğŸŒ å¯¼èˆªåˆ°æµ‹è¯•URL: {url}")
            nav_success = self.scraper.browser_service.navigate_to_sync(url, wait_until="domcontentloaded")
            print(f"   å¯¼èˆªç»“æœ: {nav_success}")

            # Act
            result = self.scraper.scrape(url=url, max_competitors=2)
            results.append(result)
            
            # æ·»åŠ å»¶æ—¶ï¼Œé¿å…å¯¹æœåŠ¡å™¨é€ æˆå‹åŠ›
            time.sleep(2)

        # Assert
        assert len(results) == 2
        for result in results:
            assert isinstance(result, ScrapingResult)

    # ========== æ€§èƒ½å’Œç¨³å®šæ€§æµ‹è¯• ==========

    @pytest.mark.network
    @pytest.mark.slow
    def test_scraper_performance_baseline(self):
        """æµ‹è¯•æŠ“å–æ€§èƒ½åŸºçº¿"""
        # Arrange
        url = self.test_urls[0]["url"]
        performance_threshold = 45  # 45ç§’æ€§èƒ½é˜ˆå€¼

        # ğŸ¯ å…³é”®ä¿®å¤ï¼šåœ¨scrapeå‰å…ˆå¯¼èˆªåˆ°ç›®æ ‡é¡µé¢
        print(f"ğŸŒ å¯¼èˆªåˆ°æµ‹è¯•URL: {url}")
        nav_success = self.scraper.browser_service.navigate_to_sync(url, wait_until="domcontentloaded")
        print(f"   å¯¼èˆªç»“æœ: {nav_success}")

        # Act
        start_time = time.time()
        result = self.scraper.scrape(url=url, max_competitors=5)
        execution_time = time.time() - start_time

        # Assert
        assert execution_time < performance_threshold, f"æ€§èƒ½è¶…å‡ºé˜ˆå€¼: {execution_time}s > {performance_threshold}s"
        assert isinstance(result, ScrapingResult)

    @pytest.mark.network
    @pytest.mark.slow
    def test_scraper_stability_multiple_calls(self):
        """æµ‹è¯•å¤šæ¬¡è°ƒç”¨çš„ç¨³å®šæ€§"""
        # Arrange
        url = self.test_urls[0]["url"]
        call_count = 3
        results = []

        # Act
        for i in range(call_count):
            # ğŸ¯ å…³é”®ä¿®å¤ï¼šæ¯æ¬¡scrapeå‰å…ˆå¯¼èˆªåˆ°ç›®æ ‡é¡µé¢
            print(f"ğŸŒ å¯¼èˆªåˆ°æµ‹è¯•URL (ç¬¬{i+1}æ¬¡): {url}")
            nav_success = self.scraper.browser_service.navigate_to_sync(url, wait_until="domcontentloaded")
            print(f"   å¯¼èˆªç»“æœ: {nav_success}")

            result = self.scraper.scrape(url=url, max_competitors=2)
            results.append(result)
            
            # æ·»åŠ å»¶æ—¶
            if i < call_count - 1:
                time.sleep(3)

        # Assert
        assert len(results) == call_count
        
        # éªŒè¯æ‰€æœ‰ç»“æœéƒ½æ˜¯æœ‰æ•ˆçš„
        for i, result in enumerate(results):
            assert isinstance(result, ScrapingResult), f"ç¬¬ {i+1} æ¬¡è°ƒç”¨ç»“æœæ— æ•ˆ"

    # ========== é”™è¯¯å¤„ç†å’Œå®¹é”™æ€§æµ‹è¯• ==========

    @pytest.mark.network
    def test_invalid_url_handling(self):
        """æµ‹è¯•æ— æ•ˆURLçš„å¤„ç†"""
        # Arrange
        invalid_urls = [
            "https://invalid-domain-that-does-not-exist.com/product/123",
            "not-a-url",
            "",
        ]

        for invalid_url in invalid_urls:
            # Act
            result = self.scraper.scrape(url=invalid_url)

            # Assert
            assert isinstance(result, ScrapingResult)
            # æ— æ•ˆURLåº”è¯¥å¤±è´¥ï¼Œä½†ä¸åº”è¯¥æŠ›å‡ºå¼‚å¸¸
            assert result.success is False or result.data == []

    @pytest.mark.network
    @pytest.mark.slow
    def test_timeout_handling(self):
        """æµ‹è¯•è¶…æ—¶å¤„ç†ï¼ˆå¦‚æœæ”¯æŒï¼‰"""
        # Arrange
        url = self.test_urls[0]["url"]

        # ğŸ¯ å…³é”®ä¿®å¤ï¼šåœ¨scrapeå‰å…ˆå¯¼èˆªåˆ°ç›®æ ‡é¡µé¢
        print(f"ğŸŒ å¯¼èˆªåˆ°æµ‹è¯•URL: {url}")
        nav_success = self.scraper.browser_service.navigate_to_sync(url, wait_until="domcontentloaded")
        print(f"   å¯¼èˆªç»“æœ: {nav_success}")

        # Act & Assert
        # å³ä½¿åœ¨å¯èƒ½çš„è¶…æ—¶æƒ…å†µä¸‹ï¼Œä¹Ÿåº”è¯¥è¿”å›æœ‰æ•ˆçš„ç»“æœå¯¹è±¡
        result = self.scraper.scrape(url=url, max_competitors=1)
        assert isinstance(result, ScrapingResult)

    @pytest.mark.network
    def test_malformed_page_content_handling(self):
        """æµ‹è¯•å¤„ç†æ ¼å¼é”™è¯¯çš„é¡µé¢å†…å®¹"""
        # Arrange - ä½¿ç”¨å¯èƒ½æ²¡æœ‰ç«å“ä¿¡æ¯çš„é¡µé¢
        non_product_url = "https://www.ozon.ru/"

        # ğŸ¯ å…³é”®ä¿®å¤ï¼šåœ¨scrapeå‰å…ˆå¯¼èˆªåˆ°ç›®æ ‡é¡µé¢
        print(f"ğŸŒ å¯¼èˆªåˆ°æµ‹è¯•URL: {non_product_url}")
        nav_success = self.scraper.browser_service.navigate_to_sync(non_product_url, wait_until="domcontentloaded")
        print(f"   å¯¼èˆªç»“æœ: {nav_success}")

        # Act
        result = self.scraper.scrape(url=non_product_url)

        # Assert
        assert isinstance(result, ScrapingResult)
        # éäº§å“é¡µé¢åº”è¯¥æ²¡æœ‰ç«å“æ•°æ®ï¼Œä½†ä¸åº”è¯¥æŠ›å‡ºå¼‚å¸¸
        if result.success:
            assert isinstance(result.data, list)
            assert len(result.data) == 0  # åº”è¯¥æ²¡æœ‰ç«å“æ•°æ®

    # ========== é…ç½®å’Œç¯å¢ƒæµ‹è¯• ==========

    @pytest.mark.network
    def test_different_selector_configurations(self):
        """æµ‹è¯•ä¸åŒçš„é€‰æ‹©å™¨é…ç½®"""
        # Arrange
        custom_config = OzonSelectorsConfig()
        scraper_with_custom_config = CompetitorScraper(selectors_config=custom_config)
        url = self.test_urls[0]["url"]

        # ğŸ¯ å…³é”®ä¿®å¤ï¼šåœ¨scrapeå‰å…ˆå¯¼èˆªåˆ°ç›®æ ‡é¡µé¢
        print(f"ğŸŒ å¯¼èˆªåˆ°æµ‹è¯•URL: {url}")
        nav_success = scraper_with_custom_config.browser_service.navigate_to_sync(url, wait_until="domcontentloaded")
        print(f"   å¯¼èˆªç»“æœ: {nav_success}")

        # Act
        result = scraper_with_custom_config.scrape(url=url, max_competitors=2)

        # Assert
        assert isinstance(result, ScrapingResult)

    @pytest.mark.network
    @pytest.mark.slow  
    def test_concurrent_scraper_instances(self):
        """æµ‹è¯•å¹¶å‘æŠ“å–å™¨å®ä¾‹ï¼ˆå¦‚æœæ”¯æŒï¼‰"""
        # Arrange
        url = self.test_urls[0]["url"]
        scraper1 = CompetitorScraper()
        scraper2 = CompetitorScraper()

        # Act
        result1 = scraper1.scrape(url=url, max_competitors=1)
        result2 = scraper2.scrape(url=url, max_competitors=1)

        # Assert
        assert isinstance(result1, ScrapingResult)
        assert isinstance(result2, ScrapingResult)

    # ========== æ•°æ®éªŒè¯æµ‹è¯• ==========

    @pytest.mark.network
    @pytest.mark.slow
    def test_competitor_data_consistency(self):
        """æµ‹è¯•ç«å“æ•°æ®çš„ä¸€è‡´æ€§"""
        # Arrange
        url = self.test_urls[0]["url"]

        # Act - è¿›è¡Œä¸¤æ¬¡æŠ“å–
        # ğŸ¯ å…³é”®ä¿®å¤ï¼šç¬¬ä¸€æ¬¡scrapeå‰å¯¼èˆª
        print(f"ğŸŒ å¯¼èˆªåˆ°æµ‹è¯•URL (ç¬¬1æ¬¡): {url}")
        nav_success = self.scraper.browser_service.navigate_to_sync(url, wait_until="domcontentloaded")
        print(f"   å¯¼èˆªç»“æœ: {nav_success}")

        result1 = self.scraper.scrape(url=url, max_competitors=3)
        time.sleep(2)  # ç­‰å¾…é—´éš”

        # ğŸ¯ å…³é”®ä¿®å¤ï¼šç¬¬äºŒæ¬¡scrapeå‰å¯¼èˆª
        print(f"ğŸŒ å¯¼èˆªåˆ°æµ‹è¯•URL (ç¬¬2æ¬¡): {url}")
        nav_success = self.scraper.browser_service.navigate_to_sync(url, wait_until="domcontentloaded")
        print(f"   å¯¼èˆªç»“æœ: {nav_success}")

        result2 = self.scraper.scrape(url=url, max_competitors=3)

        # Assert
        assert isinstance(result1, ScrapingResult)
        assert isinstance(result2, ScrapingResult)
        
        # å¦‚æœä¸¤æ¬¡éƒ½æˆåŠŸï¼Œæ•°æ®ç»“æ„åº”è¯¥ä¸€è‡´
        if result1.success and result2.success:
            assert isinstance(result1.data, list)
            assert isinstance(result2.data, list)

    @pytest.mark.network
    @pytest.mark.slow
    def test_max_competitors_limit_enforcement(self):
        """æµ‹è¯•æœ€å¤§ç«å“æ•°é‡é™åˆ¶çš„æ‰§è¡Œ"""
        # Arrange
        url = self.test_urls[0]["url"]
        max_competitors_values = [1, 3, 5]

        for max_competitors in max_competitors_values:
            # Act
            result = self.scraper.scrape(url=url, max_competitors=max_competitors)

            # Assert
            assert isinstance(result, ScrapingResult)
            if result.success and result.data:
                assert len(result.data) <= max_competitors, f"è¿”å›çš„ç«å“æ•°é‡ {len(result.data)} è¶…è¿‡é™åˆ¶ {max_competitors}"

    # ========== æ¸…ç†å’Œèµ„æºç®¡ç†æµ‹è¯• ==========

    @pytest.mark.network
    def test_resource_cleanup_after_scraping(self):
        """æµ‹è¯•æŠ“å–åçš„èµ„æºæ¸…ç†"""
        # Arrange
        url = self.test_urls[0]["url"]
        initial_page = self.scraper.browser_service.get_page()

        # Act
        result = self.scraper.scrape(url=url, max_competitors=1)
        post_scrape_page = self.scraper.browser_service.get_page()

        # Assert
        assert isinstance(result, ScrapingResult)
        # éªŒè¯æµè§ˆå™¨æœåŠ¡ä»ç„¶å¯ç”¨
        assert post_scrape_page is not None
        # å¯ä»¥æ·»åŠ æ›´å¤šèµ„æºçŠ¶æ€æ£€æŸ¥

    def teardown_method(self):
        """æµ‹è¯•æ–¹æ³•æ¸…ç† - æ¯ä¸ªæµ‹è¯•æ–¹æ³•æ‰§è¡Œåè°ƒç”¨"""
        # ğŸ”§ ä½¿ç”¨å…¨å±€æµè§ˆå™¨å•ä¾‹ï¼Œé€šå¸¸ä¸éœ€è¦å…³é—­æµè§ˆå™¨æœåŠ¡
        # è®©æµè§ˆå™¨æœåŠ¡ä¿æŒè¿è¡Œï¼Œä¾›åç»­æµ‹è¯•ä½¿ç”¨
        print(f"ğŸ§¹ æµ‹è¯•æ¸…ç†å®Œæˆ")
        time.sleep(1)  # ç»™ç³»ç»Ÿä¸€äº›æ—¶é—´è¿›è¡Œæ¸…ç†
