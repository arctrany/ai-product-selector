# #!/usr/bin/env python3
# # -*- coding: utf-8 -*-
#
# """
# OzonScraper å®Œæ•´å•å…ƒæµ‹è¯•
#
# åŸºäºç°æœ‰çš„BaseScraperTeståŸºç±»ï¼Œä¸ºOzonScraperç¼–å†™å…¨é¢çš„å•å…ƒæµ‹è¯•ã€‚
# åŒ…å«æ–¹æ³•çº§æµ‹è¯•ã€Mockæµ‹è¯•ã€é”™è¯¯å¤„ç†æµ‹è¯•å’ŒçœŸå®æ•°æ®éªŒè¯ã€‚
#
# ä½œè€…: Aone Copilot
# åˆ›å»ºæ—¶é—´: 2025-11-25
# """
#
# import json
# import unittest
# import pytest
# from pathlib import Path
# from unittest.mock import Mock, MagicMock, patch
# from bs4 import BeautifulSoup
# from typing import Dict, Any, Optional
#
# from common.scrapers.ozon_scraper import OzonScraper
# from common.config.base_config import get_config
# from common.services.scraping_orchestrator import ScrapingMode
# from tests.rpa.base_scraper_test import BaseScraperTest, BaseScraperRealBrowserTest
#
#
# class TestOzonScraperUnit(BaseScraperTest):
#     """OzonScraper å®Œæ•´å•å…ƒæµ‹è¯•ç±»"""
#
#     def setUp(self):
#         """æµ‹è¯•åˆå§‹åŒ–"""
#         super().setUp()
#         self.config = get_config()
#
#         # åˆ›å»ºOzonScraperå®ä¾‹å¹¶æ³¨å…¥MockæœåŠ¡
#         with patch('common.scrapers.ozon_scraper.get_global_browser_service') as mock_get_browser:
#             mock_get_browser.return_value = self.mock_browser_service
#             self.scraper = OzonScraper(self.config)
#             self.scraper.browser_service = self.mock_browser_service
#
#         # åŠ è½½æµ‹è¯•æ•°æ®
#         self.test_cases_data = self._load_test_cases()
#
#     def tearDown(self):
#         """æµ‹è¯•æ¸…ç†"""
#         if hasattr(self.scraper, 'close'):
#             self.scraper.close()
#         super().tearDown()
#
#     def _load_test_cases(self) -> Dict[str, Any]:
#         """åŠ è½½æµ‹è¯•ç”¨ä¾‹æ•°æ®"""
#         try:
#             test_data_path = Path(__file__).parent / "test_data" / "ozon_test_cases.json"
#             if test_data_path.exists():
#                 with open(test_data_path, 'r', encoding='utf-8') as f:
#                     return json.load(f)
#             return {}
#         except Exception as e:
#             self.logger.warning(f"åŠ è½½æµ‹è¯•æ•°æ®å¤±è´¥: {e}")
#             return {}
#
#     # =============================================================================
#     # æ ¸å¿ƒæ–¹æ³•å•å…ƒæµ‹è¯•
#     # =============================================================================
#
#     def test_scrape_product_info_mode(self):
#         """æµ‹è¯•scrape()æ–¹æ³• - PRODUCT_INFOæ¨¡å¼"""
#         # Arrange
#         test_url = "https://www.ozon.ru/product/1756017628/"
#         mock_html = self.create_mock_page_content(price="1500 â‚½")
#
#         self.mock_browser_navigate(success=True)
#         self.mock_browser_page_content(mock_html)
#
#         # å…³é”®ä¿®å¤ï¼šMock scraping_utilsæ–¹æ³•
#         self.scraper.scraping_utils.extract_data_with_js = Mock(return_value=mock_html)
#         self.scraper.scraping_utils.extract_price_from_soup = Mock()
#         self.scraper.scraping_utils.extract_price_from_soup.side_effect = lambda soup, price_type: {
#             'green': 1400.0,
#             'black': 1500.0
#         }.get(price_type)
#
#         # Act
#         result = self.scraper.scrape(test_url, mode=ScrapingMode.PRODUCT_INFO)
#
#         # Assert
#         self.assert_scraping_result_success(result)
#         self.assertIn('green_price', result.data)
#         self.mock_browser_service.navigate_to_sync.assert_called_once()
#
#     def test_scrape_comprehensive_mode(self):
#         """æµ‹è¯•scrape()æ–¹æ³• - ç»¼åˆæ¨¡å¼"""
#         # Arrange
#         test_url = "https://www.ozon.ru/product/144042159/"
#         mock_html = self.create_mock_page_content(price="2000 â‚½", has_competitors=True)
#
#         self.mock_browser_navigate(success=True)
#         self.mock_browser_page_content(mock_html)
#
#         # Mock ERP scraper result
#         mock_erp_result = Mock()
#         mock_erp_result.success = True
#         mock_erp_result.data = {'commission': 15.0, 'weight': 500}
#         self.scraper.erp_scraper.scrape = Mock(return_value=mock_erp_result)
#
#         # Act
#         result = self.scraper.scrape(test_url, mode=ScrapingMode.PRODUCT_INFO)
#
#         # Assert
#         self.assert_scraping_result_success(result)
#         # ä¿®æ­£ï¼šæ£€æŸ¥å®é™…è¿”å›çš„ä»·æ ¼å­—æ®µè€Œä¸æ˜¯basic_data
#         self.assertIn('green_price', result.data)
#         self.assertIn('black_price', result.data)
#
#     def test_extract_basic_product_info(self):
#         """æµ‹è¯•_extract_basic_product_info()æ–¹æ³•"""
#         # Arrange
#         mock_html = '''
#         <html><body>
#             <span class="tsHeadline600Large">1200 â‚½</span>
#             <span class="tsBodyControl500Medium">900 â‚½</span>
#         </body></html>
#         '''
#         self.mock_browser_service.evaluate_sync.return_value = mock_html
#
#         # Mock scraping_utils methods
#         self.scraper.scraping_utils.extract_price_from_soup = Mock()
#         self.scraper.scraping_utils.extract_price_from_soup.side_effect = lambda soup, price_type: {
#             'green': 900.0,
#             'black': 1200.0
#         }.get(price_type)
#
#         # Act
#         result = self.scraper._extract_basic_product_info()
#
#         # Assert
#         self.assertIsInstance(result, dict)
#         self.assertEqual(result.get('green_price'), 900.0)
#         self.assertEqual(result.get('black_price'), 1200.0)
#
#
#
#     def test_extract_product_image(self):
#         """æµ‹è¯•_extract_product_image()æ–¹æ³•"""
#         # Arrange
#         soup = BeautifulSoup('<img src="https://cdn.ozon.ru/multimedia/wc750/test-image.jpg" />', 'html.parser')
#         expected_url = "https://cdn.ozon.ru/multimedia/wc1000/test-image.jpg"
#
#         # Mock scraping_utils.extract_product_image
#         self.scraper.scraping_utils.extract_product_image = Mock(return_value=expected_url)
#
#         # Act
#         result = self.scraper._extract_product_image(soup)
#
#         # Assert
#         self.assertEqual(result, expected_url)
#         self.scraper.scraping_utils.extract_product_image.assert_called_once()
#
#     # =============================================================================
#     # é”™è¯¯å¤„ç†æµ‹è¯•
#     # =============================================================================
#
#     def test_scrape_navigation_failure(self):
#         """æµ‹è¯•å¯¼èˆªå¤±è´¥çš„é”™è¯¯å¤„ç†"""
#         # Arrange
#         test_url = "https://invalid-url.com"
#         self.mock_browser_navigate(success=False)
#
#         # Act - ä¿®å¤ï¼šæ£€æŸ¥å®é™…è¿”å›å¤±è´¥ç»“æœè€Œä¸æ˜¯å¼‚å¸¸
#         result = self.scraper.scrape(test_url)
#
#         # Assert - ä¿®æ­£ï¼šåº”è¯¥è¿”å›å¤±è´¥ç»“æœè€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
#         self.assert_scraping_result_failure(result)
#         self.assertIn("æ— æ³•å¯¼èˆªåˆ°å•†å“é¡µé¢", result.error_message)
#
#
#
#
#
#
#
#     def test_extract_basic_product_info_exception(self):
#         """æµ‹è¯•åŸºç¡€ä¿¡æ¯æå–å¼‚å¸¸å¤„ç†"""
#         # Arrange
#         self.mock_browser_service.evaluate_sync.side_effect = Exception("Browser error")
#
#         # Act
#         result = self.scraper._extract_basic_product_info()
#
#         # Assert
#         self.assertEqual(result, {})
#
#     def test_extract_product_image_exception(self):
#         """æµ‹è¯•å›¾ç‰‡æå–å¼‚å¸¸å¤„ç†"""
#         # Arrange
#         soup = BeautifulSoup('<html></html>', 'html.parser')
#         self.scraper.scraping_utils.extract_product_image = Mock(side_effect=Exception("Image error"))
#
#         # Act
#         result = self.scraper._extract_product_image(soup)
#
#         # Assert
#         self.assertIsNone(result)
#
#     # =============================================================================
#     # è¾¹ç•Œæ¡ä»¶æµ‹è¯•
#     # =============================================================================
#
#     def test_scrape_empty_options(self):
#         """æµ‹è¯•ç©ºé€‰é¡¹å‚æ•°"""
#         # Arrange
#         test_url = "https://www.ozon.ru/product/2369901364/"
#         self.mock_browser_navigate(success=True)
#         mock_html = self.create_mock_page_content()
#         self.mock_browser_page_content(mock_html)
#
#         # Act
#         result = self.scraper.scrape(test_url, options={})
#
#         # Assert
#         self.assert_scraping_result_success(result)
#
#     def test_scrape_none_mode(self):
#         """æµ‹è¯•Noneæ¨¡å¼å‚æ•°"""
#         # Arrange
#         test_url = "https://www.ozon.ru/product/1176594312/"
#         self.mock_browser_navigate(success=True)
#         mock_html = self.create_mock_page_content()
#         self.mock_browser_page_content(mock_html)
#
#         # Mock ERP scraper
#         mock_erp_result = Mock()
#         mock_erp_result.success = True
#         mock_erp_result.data = {}
#         self.scraper.erp_scraper.scrape = Mock(return_value=mock_erp_result)
#
#         # Act
#         result = self.scraper.scrape(test_url, mode=None)
#
#         # Assert
#         self.assert_scraping_result_success(result)
#         # é»˜è®¤åº”è¯¥èµ°comprehensiveæ¨¡å¼
#         self.assertIn('basic_data', result.data)
#         self.assertIn('erp_data', result.data)
#
#     # =============================================================================
#     # çœŸå®æµ‹è¯•æ•°æ®éªŒè¯
#     # =============================================================================
#
#     def test_with_real_test_data_scenario_1(self):
#         """ä½¿ç”¨çœŸå®æµ‹è¯•æ•°æ® - åœºæ™¯1ï¼šæ— è·Ÿå–åº—é“º"""
#         if not self.test_cases_data:
#             self.skipTest("æµ‹è¯•æ•°æ®æœªåŠ è½½")
#
#         test_cases = self.test_cases_data.get('test_cases', [])
#         scenario_1 = next((case for case in test_cases if case['id'] == 'scenario_1_no_competitors'), None)
#         if not scenario_1:
#             self.skipTest("åœºæ™¯1æµ‹è¯•æ•°æ®ä¸å­˜åœ¨")
#
#         # Arrange
#         test_url = scenario_1['url']
#         expected = scenario_1['expected']
#
#         # Mock browser behavior for no competitors scenario
#         self.mock_browser_navigate(success=True)
#         mock_html = self.create_mock_page_content(price="1500 â‚½", has_competitors=False)
#         self.mock_browser_page_content(mock_html)
#
#         # Act
#         result = self.scraper.scrape(test_url, mode=ScrapingMode.PRODUCT_INFO)
#
#         # Assert
#         self.assert_scraping_result_success(result)
#         self.assertEqual(expected['has_competitors'], False)
#         self.assertEqual(expected['competitor_count'], 0)
#
#     def test_with_real_test_data_scenario_2(self):
#         """ä½¿ç”¨çœŸå®æµ‹è¯•æ•°æ® - åœºæ™¯2ï¼šæœ‰è·Ÿå–åº—é“º"""
#         if not self.test_cases_data:
#             self.skipTest("æµ‹è¯•æ•°æ®æœªåŠ è½½")
#
#         test_cases = self.test_cases_data.get('test_cases', [])
#         scenario_2 = next((case for case in test_cases if case['id'] == 'scenario_2_with_competitors'), None)
#         if not scenario_2:
#             self.skipTest("åœºæ™¯2æµ‹è¯•æ•°æ®ä¸å­˜åœ¨")
#
#         # Arrange
#         test_url = scenario_2['url']
#         expected = scenario_2['expected']
#
#         # Mock browser behavior for competitors scenario
#         self.mock_browser_navigate(success=True)
#         mock_html = self.create_mock_page_content(price="1600 â‚½", has_competitors=True)
#         self.mock_browser_page_content(mock_html)
#
#         # Act
#         result = self.scraper.scrape(test_url, mode=ScrapingMode.PRODUCT_INFO)
#
#         # Assert
#         self.assert_scraping_result_success(result)
#         self.assertEqual(expected['has_competitors'], True)
#         self.assertGreater(expected['competitor_count'], 0)
#
#     def test_with_real_test_data_scenario_3(self):
#         """ä½¿ç”¨çœŸå®æµ‹è¯•æ•°æ® - åœºæ™¯3ï¼šå¤§é‡è·Ÿå–åº—é“º"""
#         if not self.test_cases_data:
#             self.skipTest("æµ‹è¯•æ•°æ®æœªåŠ è½½")
#
#         test_cases = self.test_cases_data.get('test_cases', [])
#         scenario_3 = next((case for case in test_cases if case['id'] == 'scenario_3_many_competitors'), None)
#         if not scenario_3:
#             self.skipTest("åœºæ™¯3æµ‹è¯•æ•°æ®ä¸å­˜åœ¨")
#
#         # Arrange
#         test_url = scenario_3['url']
#         expected = scenario_3['expected']
#
#         # Mock browser behavior for many competitors scenario
#         self.mock_browser_navigate(success=True)
#         mock_html = self.create_mock_page_content(price="1400 â‚½", has_competitors=True)
#         self.mock_browser_page_content(mock_html)
#
#         # Act
#         result = self.scraper.scrape(test_url, mode=ScrapingMode.PRODUCT_INFO)
#
#         # Assert
#         self.assert_scraping_result_success(result)
#         self.assertEqual(expected['has_competitors'], True)
#         self.assertEqual(expected['competitor_count'], 8)
#
#     # =============================================================================
#     # Mockç­–ç•¥æµ‹è¯•
#     # =============================================================================
#
#     def test_mock_browser_service_integration(self):
#         """æµ‹è¯•Mockæµè§ˆå™¨æœåŠ¡é›†æˆ"""
#         # Arrange
#         test_url = "https://www.ozon.ru/product/1756017628/"
#
#         # Act
#         self.scraper.navigate_to(test_url)
#
#         # Assert
#         self.mock_browser_service.navigate_to_sync.assert_called()
#
#     def test_mock_scraping_utils_integration(self):
#         """æµ‹è¯•MockæŠ“å–å·¥å…·é›†æˆ"""
#         # Arrange
#         soup = BeautifulSoup('<div>test</div>', 'html.parser')
#
#         # Act
#         self.scraper._extract_product_image(soup)
#
#         # Assert
#         self.assertIsNotNone(self.scraper.scraping_utils)
#
#     def test_mock_erp_scraper_integration(self):
#         """æµ‹è¯•Mock ERPæŠ“å–å™¨é›†æˆ"""
#         # Arrange
#         mock_result = Mock()
#         mock_result.success = True
#         mock_result.data = {'test': 'data'}
#         self.scraper.erp_scraper.scrape = Mock(return_value=mock_result)
#
#         # Act
#         result = self.scraper.erp_scraper.scrape()
#
#         # Assert
#         self.assertTrue(result.success)
#         self.assertEqual(result.data['test'], 'data')
#
#     # =============================================================================
#     # æ€§èƒ½å’Œè´¨é‡æµ‹è¯•
#     # =============================================================================
#
#     def test_scraper_initialization(self):
#         """æµ‹è¯•Scraperåˆå§‹åŒ–"""
#         # Assert - éªŒè¯åŸºæœ¬å±æ€§å­˜åœ¨
#         self.assertIsNotNone(self.scraper.config)
#         self.assertIsNotNone(self.scraper.selectors_config)
#         self.assertIsNotNone(self.scraper.currency_config)
#         self.assertIsNotNone(self.scraper.browser_service)
#         self.assertIsNotNone(self.scraper.scraping_utils)
#         self.assertIsNotNone(self.scraper.erp_scraper)
#         self.assertIsNotNone(self.scraper.wait_utils)
#
#     def test_scraper_logger_setup(self):
#         """æµ‹è¯•æ—¥å¿—å™¨è®¾ç½®"""
#         # Assert
#         self.assertIsNotNone(self.scraper.logger)
#         self.assertEqual(self.scraper.logger.name, 'common.scrapers.ozon_scraper.OzonScraper')
#
#     def test_method_call_patterns(self):
#         """æµ‹è¯•æ–¹æ³•è°ƒç”¨æ¨¡å¼"""
#         # Arrange
#         test_url = "https://www.ozon.ru/product/144042159/"
#         self.mock_browser_navigate(success=True)
#         mock_html = self.create_mock_page_content()
#         self.mock_browser_page_content(mock_html)
#
#         # Act
#         result = self.scraper.scrape(test_url, mode=ScrapingMode.PRODUCT_INFO)
#
#         # Assert - éªŒè¯è°ƒç”¨é“¾
#         self.assert_scraping_result_success(result)
#         self.mock_browser_service.navigate_to_sync.assert_called()
#         self.mock_browser_service.evaluate_sync.assert_called()
#
#     # =============================================================================
#     # æ•°æ®éªŒè¯æµ‹è¯•
#     # =============================================================================
#
#     def test_price_data_structure(self):
#         """æµ‹è¯•ä»·æ ¼æ•°æ®ç»“æ„"""
#         # Arrange
#         mock_html = self.create_mock_page_content(price="1800 â‚½")
#         self.mock_browser_service.evaluate_sync.return_value = mock_html
#
#         # Mock price extraction
#         self.scraper.scraping_utils.extract_price_from_soup = Mock()
#         self.scraper.scraping_utils.extract_price_from_soup.side_effect = lambda soup, price_type: {
#             'green': 1600.0,
#             'black': 1800.0
#         }.get(price_type)
#
#         # Act
#         result = self.scraper._extract_basic_product_info()
#
#         # Assert
#         self.assertIsInstance(result, dict)
#         self.assertIn('green_price', result)
#         self.assertIn('black_price', result)
#         self.assertIsInstance(result['green_price'], float)
#         self.assertIsInstance(result['black_price'], float)
#
#     def test_comprehensive_data_structure(self):
#         """æµ‹è¯•ç»¼åˆæ•°æ®ç»“æ„"""
#         # Arrange
#         test_url = "https://www.ozon.ru/product/2369901364/"
#         self.mock_browser_service.evaluate_sync.return_value = self.create_mock_page_content()
#
#         # Mock ERP result
#         mock_erp_result = Mock()
#         mock_erp_result.success = True
#         mock_erp_result.data = {'commission': 12.5}
#         self.scraper.erp_scraper.scrape = Mock(return_value=mock_erp_result)
#
#         # Act
#         result = self.scraper._extract_comprehensive_data(test_url)
#
#         # Assert
#         self.assertIsInstance(result, dict)
#         self.assertIn('product_url', result)
#         self.assertIn('basic_data', result)
#         self.assertIn('erp_data', result)
#         self.assertEqual(result['product_url'], test_url)
#
#     def test_empty_data_handling(self):
#         """æµ‹è¯•ç©ºæ•°æ®å¤„ç†"""
#         # Arrange - Mockè¿”å›ç©ºæ•°æ®
#         self.scraper.scraping_utils.extract_price_from_soup = Mock(return_value=None)
#         self.mock_browser_service.evaluate_sync.return_value = "<html></html>"
#
#         # Act
#         result = self.scraper._extract_basic_product_info()
#
#         # Assert
#         self.assertIsInstance(result, dict)
#         # ç©ºå€¼åº”è¯¥è¢«è¿‡æ»¤æ‰
#         self.assertNotIn('green_price', result)
#         self.assertNotIn('black_price', result)
#
#
# class TestOzonScraperParameterized(BaseScraperTest):
#     """OzonScraper å‚æ•°åŒ–æµ‹è¯•"""
#
#     def setUp(self):
#         """æµ‹è¯•åˆå§‹åŒ–"""
#         super().setUp()
#         with patch('common.scrapers.ozon_scraper.get_global_browser_service') as mock_get_browser:
#             mock_get_browser.return_value = self.mock_browser_service
#             self.scraper = OzonScraper(get_config())
#
#
#
#     def test_url_variations(self):
#         """æµ‹è¯•ä¸åŒURLæ ¼å¼"""
#         urls = [
#             "https://www.ozon.ru/product/1756017628/",
#             "https://www.ozon.ru/product/144042159/",
#             "https://www.ozon.ru/product/2369901364/",
#         ]
#
#         for url in urls:
#             with self.subTest(url=url):
#                 # Arrange
#                 self.mock_browser_navigate(success=True)
#                 mock_html = self.create_mock_page_content()
#                 self.mock_browser_page_content(mock_html)
#
#                 # Act & Assert
#                 try:
#                     result = self.scraper.scrape(url, mode=ScrapingMode.PRODUCT_INFO)
#                     self.assert_scraping_result_success(result)
#                 except Exception as e:
#                     self.fail(f"URL {url} æµ‹è¯•å¤±è´¥: {e}")
#
#
# @pytest.mark.integration
# @pytest.mark.browser
# @pytest.mark.slow
# @pytest.mark.network
# class TestOzonScraperRealBrowser(BaseScraperRealBrowserTest):
#     """OzonScraperçœŸå®æµè§ˆå™¨é›†æˆæµ‹è¯•ç±»"""
#
#     def setUp(self):
#         """æµ‹è¯•åˆå§‹åŒ–"""
#         super().setUp()
#
#         # ğŸ”§ å…³é”®ä¿®å¤ï¼šè®©OzonScraperä½¿ç”¨çœŸå®æµè§ˆå™¨æœåŠ¡
#         # å…ˆåˆ›å»ºå¹¶åˆå§‹åŒ–çœŸå®æµè§ˆå™¨æœåŠ¡
#         self.real_browser_service = self._create_real_browser_service()
#         success = self.real_browser_service.initialize()
#         if not success:
#             self.fail("âŒ çœŸå®æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥")
#
#         # ğŸ”§ å…³é”®ä¿®å¤ï¼šé€šè¿‡æ„é€ å‡½æ•°ç›´æ¥æ³¨å…¥çœŸå®æµè§ˆå™¨æœåŠ¡
#         self.scraper = OzonScraper(get_config(), browser_service=self.real_browser_service)
#
#         # ä¸éœ€è¦Mock scraping_utilsï¼Œä½¿ç”¨çœŸå®çš„
#         self.logger.info("âœ… OzonScraperçœŸå®æµè§ˆå™¨æµ‹è¯•åˆå§‹åŒ–å®Œæˆ")
#
#     # ğŸ”§ ä¿®å¤ï¼šç»§æ‰¿BaseScraperTestçš„æ–­è¨€æ–¹æ³•
#     def assert_scraping_result_success(self, result):
#         """æ–­è¨€æŠ“å–ç»“æœæˆåŠŸ"""
#         self.assertIsNotNone(result, "Result should not be None")
#         self.assertTrue(hasattr(result, 'success'), "Result should have 'success' attribute")
#         self.assertTrue(result.success, "Result should be successful")
#         self.assertIsNotNone(result.data, "Result data should not be None")
#
#     def assert_scraping_result_failure(self, result, expected_error=None):
#         """æ–­è¨€æŠ“å–ç»“æœå¤±è´¥"""
#         self.assertIsNotNone(result, "Result should not be None")
#         self.assertTrue(hasattr(result, 'success'), "Result should have 'success' attribute")
#         self.assertFalse(result.success, "Result should be failed")
#
#         if expected_error:
#             self.assertIsNotNone(result.error_message, "Error message should not be None")
#             self.assertIn(expected_error, result.error_message,
#                          f"Error message should contain '{expected_error}'")
#
#     def tearDown(self):
#         """æµ‹è¯•æ¸…ç†"""
#         try:
#             if hasattr(self.scraper, 'browser_service') and self.scraper.browser_service:
#                 # æµè§ˆå™¨æœåŠ¡å°†åœ¨çˆ¶ç±»ä¸­å…³é—­
#                 pass
#         except Exception as e:
#             self.logger.warning(f"æµ‹è¯•æ¸…ç†è­¦å‘Š: {e}")
#
#         super().tearDown()
#
#     def test_real_browser_scrape_product_info_mode(self):
#         """æµ‹è¯•çœŸå®æµè§ˆå™¨PRODUCT_INFOæ¨¡å¼æŠ“å–"""
#         test_url = "https://www.ozon.ru/product/1756017628/"
#
#         # å¯¼èˆªåˆ°é¡µé¢
#         self.assert_real_browser_navigation_success(test_url)
#
#         # éªŒè¯é¡µé¢åŠ è½½
#         self.assert_page_loaded_successfully()
#
#         # æ‰§è¡ŒæŠ“å–
#         result = self.scraper.scrape(test_url, mode=ScrapingMode.PRODUCT_INFO)
#
#         # éªŒè¯ç»“æœ
#         self.assert_scraping_result_success(result)
#         self.logger.info(f"âœ… æŠ“å–ç»“æœ: {result.data}")
#
#         # éªŒè¯åŒ…å«ä»·æ ¼æ•°æ®ï¼ˆçœŸå®æ•°æ®å¯èƒ½ä¸åŒï¼‰
#         self.assertIsInstance(result.data, dict, "ç»“æœåº”è¯¥æ˜¯å­—å…¸")
#
#     def test_real_browser_navigation_and_content(self):
#         """æµ‹è¯•çœŸå®æµè§ˆå™¨å¯¼èˆªå’Œå†…å®¹è·å–"""
#         test_url = "https://www.ozon.ru/product/144042159/"
#
#         # å¯¼èˆªæµ‹è¯•
#         success = self.navigate_to_url(test_url, timeout=30)
#         self.assertTrue(success, "åº”è¯¥æˆåŠŸå¯¼èˆªåˆ°Ozonäº§å“é¡µé¢")
#
#         # ç­‰å¾…é¡µé¢å…ƒç´ åŠ è½½
#         price_selectors = self.test_data['expected_selectors']
#         element_found = False
#
#         for selector in price_selectors:
#             if self.wait_for_element(selector, timeout=10):
#                 element_found = True
#                 self.logger.info(f"âœ… æ‰¾åˆ°ä»·æ ¼å…ƒç´ : {selector}")
#                 break
#
#         # è·å–é¡µé¢å†…å®¹
#         content = self.get_page_content()
#         self.assertIsNotNone(content, "é¡µé¢å†…å®¹ä¸åº”ä¸ºç©º")
#         self.assertIn("ozon", content.lower(), "é¡µé¢åº”åŒ…å«ozonç›¸å…³å†…å®¹")
#
#         self.logger.info(f"âœ… é¡µé¢å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
#
#     def test_real_browser_error_handling(self):
#         """æµ‹è¯•çœŸå®æµè§ˆå™¨é”™è¯¯å¤„ç†"""
#         invalid_url = "https://www.ozon.ru/product/invalid_product_id/"
#
#         # æµ‹è¯•æ— æ•ˆURLçš„å¤„ç†
#         try:
#             result = self.scraper.scrape(invalid_url, mode=ScrapingMode.PRODUCT_INFO)
#             # å³ä½¿URLæ— æ•ˆï¼Œä¹Ÿåº”è¯¥è¿”å›ç»“æœå¯¹è±¡ï¼Œè€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
#             self.assertIsNotNone(result, "åº”è¯¥è¿”å›ç»“æœå¯¹è±¡")
#             self.logger.info(f"âœ… é”™è¯¯å¤„ç†æµ‹è¯•å®Œæˆï¼Œç»“æœ: {result.success}")
#         except Exception as e:
#             # å¦‚æœæŠ›å‡ºå¼‚å¸¸ï¼Œè®°å½•ä½†ä¸å¤±è´¥ï¼ˆè¿™å–å†³äºå…·ä½“å®ç°ï¼‰
#             self.logger.info(f"â„¹ï¸ å¼‚å¸¸å¤„ç†: {e}")
#
#     def test_real_browser_performance(self):
#         """æµ‹è¯•çœŸå®æµè§ˆå™¨æ€§èƒ½"""
#         test_url = "https://www.ozon.ru/product/2369901364/"
#
#         import time
#         start_time = time.time()
#
#         # æ‰§è¡Œå¯¼èˆªå’ŒæŠ“å–
#         self.assert_real_browser_navigation_success(test_url)
#         result = self.scraper.scrape(test_url, mode=ScrapingMode.PRODUCT_INFO)
#
#         execution_time = time.time() - start_time
#
#         # æ€§èƒ½æ–­è¨€ï¼ˆåˆç†çš„è¶…æ—¶æ—¶é—´ï¼‰
#         self.assertLess(execution_time, 60, "æ•´ä¸ªæŠ“å–è¿‡ç¨‹åº”è¯¥åœ¨60ç§’å†…å®Œæˆ")
#         self.logger.info(f"âœ… æŠ“å–è€—æ—¶: {execution_time:.2f} ç§’")
#
#         # éªŒè¯ç»“æœ
#         if result and result.success:
#             self.logger.info(f"âœ… æ€§èƒ½æµ‹è¯•æˆåŠŸï¼Œæ•°æ®: {result.data}")
#         else:
#             self.logger.warning(f"âš ï¸ æŠ“å–æœªæˆåŠŸï¼Œä½†æ€§èƒ½æµ‹è¯•å®Œæˆ")
#
#
# if __name__ == '__main__':
#     # è¿è¡Œå•å…ƒæµ‹è¯•
#     unittest.main(verbosity=2)
