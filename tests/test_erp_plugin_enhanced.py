#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ERPæ’ä»¶æŠ“å–å™¨å¢å¼ºæµ‹è¯•å¥—ä»¶

åŸºäºozon_test_cases.jsonçš„æµ‹è¯•æ•°æ®ï¼Œæä¾›å…¨é¢çš„ERPæ’ä»¶åŠŸèƒ½éªŒè¯
åŒ…æ‹¬å¤šåœºæ™¯æµ‹è¯•ã€æ•°æ®éªŒè¯ã€æ€§èƒ½æµ‹è¯•ç­‰
"""

import asyncio
import json
import os
import sys
import time
import unittest
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from common.config import get_config
from common.scrapers.erp_plugin_scraper import ErpPluginScraper
from common.scrapers.xuanping_browser_service import XuanpingBrowserService
from common.models import ScrapingResult


class ErpPluginEnhancedTester:
    """ERPæ’ä»¶å¢å¼ºæµ‹è¯•å™¨"""

    def __init__(self):
        """åˆå§‹åŒ–æµ‹è¯•å™¨"""
        self.config = get_config()
        self.scraper = None
        self.browser_service = None
        self.test_cases_file = project_root / "tests" / "test_data" / "ozon_test_cases.json"
        self.test_cases = []
        self.validation_rules = {}
        self.test_results = []

    async def setup(self):
        """å¼‚æ­¥åˆå§‹åŒ–"""
        print("ğŸš€ å¼€å§‹ ERPæ’ä»¶å¢å¼ºæµ‹è¯•å¥—ä»¶")
        print("=" * 80)
        
        # åŠ è½½æµ‹è¯•ç”¨ä¾‹
        await self._load_test_cases()
        
        # åˆå§‹åŒ–æµè§ˆå™¨æœåŠ¡
        await self._setup_browser_service()
        
        # åˆå§‹åŒ–æŠ“å–å™¨
        await self._setup_scraper()

    async def teardown(self):
        """æ¸…ç†èµ„æº"""
        if self.scraper:
            await self.scraper.close()
        if self.browser_service:
            await self.browser_service.close()
        print("âœ… ERPæ’ä»¶å¢å¼ºæµ‹è¯•å¥—ä»¶å®Œæˆ")

    async def _load_test_cases(self):
        """åŠ è½½æµ‹è¯•ç”¨ä¾‹æ•°æ®"""
        try:
            with open(self.test_cases_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                self.test_cases = data.get('test_cases', [])
                self.validation_rules = data.get('validation_rules', {})
            print(f"ğŸ“‹ åŠ è½½äº† {len(self.test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
        except Exception as e:
            raise Exception(f"åŠ è½½æµ‹è¯•ç”¨ä¾‹å¤±è´¥: {e}")

    async def _setup_browser_service(self):
        """è®¾ç½®æµè§ˆå™¨æœåŠ¡"""
        try:
            browser_config = {
                'browser_type': 'edge',
                'headless': False,
                'port': 9222
            }
            self.browser_service = XuanpingBrowserService(browser_config)
            await self.browser_service.initialize()
            await self.browser_service.start_browser()
            print("âœ… æµè§ˆå™¨æœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            raise Exception(f"æµè§ˆå™¨æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")

    async def _setup_scraper(self):
        """è®¾ç½®æŠ“å–å™¨"""
        try:
            self.scraper = ErpPluginScraper(self.config, self.browser_service)
            print("âœ… ERPæ’ä»¶æŠ“å–å™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            raise Exception(f"ERPæ’ä»¶æŠ“å–å™¨åˆå§‹åŒ–å¤±è´¥: {e}")

    def _validate_erp_data_completeness(self, erp_data: Dict[str, Any], test_case: Dict[str, Any]) -> Dict[str, Any]:
        """éªŒè¯ERPæ•°æ®å®Œæ•´æ€§"""
        validation = {
            'required_fields': [],
            'optional_fields': [],
            'missing_fields': [],
            'present_fields': [],
            'completeness_score': 0.0
        }

        # å®šä¹‰å¿…éœ€å­—æ®µå’Œå¯é€‰å­—æ®µ
        required_fields = [
            'category', 'sku', 'brand_name', 'monthly_sales_volume',
            'shipping_mode', 'rfbs_commission_rates'
        ]
        
        optional_fields = [
            'dimensions', 'weight', 'listing_date_parsed', 'shelf_days',
            'competitor_list', 'competitor_min_price', 'competitor_max_price',
            'daily_sales_volume', 'daily_sales_amount', 'ad_cost_ratio'
        ]

        validation['required_fields'] = required_fields
        validation['optional_fields'] = optional_fields

        # æ£€æŸ¥å¿…éœ€å­—æ®µ
        for field in required_fields:
            if field in erp_data and erp_data[field] is not None:
                validation['present_fields'].append(field)
            else:
                validation['missing_fields'].append(field)

        # æ£€æŸ¥å¯é€‰å­—æ®µ
        for field in optional_fields:
            if field in erp_data and erp_data[field] is not None:
                validation['present_fields'].append(field)

        # è®¡ç®—å®Œæ•´æ€§åˆ†æ•°
        total_fields = len(required_fields) + len(optional_fields)
        present_count = len(validation['present_fields'])
        validation['completeness_score'] = (present_count / total_fields) * 100

        return validation

    def _validate_erp_data_quality(self, erp_data: Dict[str, Any]) -> Dict[str, Any]:
        """éªŒè¯ERPæ•°æ®è´¨é‡"""
        quality_validation = {
            'data_types': {},
            'value_ranges': {},
            'format_validation': {},
            'quality_score': 0.0,
            'issues': []
        }

        # æ•°æ®ç±»å‹éªŒè¯
        type_checks = {
            'sku': (str, int),
            'monthly_sales_volume': (str, int, float),
            'length': (int, float, type(None)),
            'width': (int, float, type(None)),
            'height': (int, float, type(None)),
            'weight': (int, float, type(None)),
            'shelf_days': (int, type(None)),
            'rfbs_commission_rates': (list, type(None))
        }

        for field, expected_types in type_checks.items():
            if field in erp_data:
                value = erp_data[field]
                is_valid_type = isinstance(value, expected_types)
                quality_validation['data_types'][field] = {
                    'value': value,
                    'expected_types': [t.__name__ for t in expected_types if t != type(None)],
                    'actual_type': type(value).__name__,
                    'valid': is_valid_type
                }
                if not is_valid_type:
                    quality_validation['issues'].append(f"{field} ç±»å‹é”™è¯¯: æœŸæœ› {expected_types}, å®é™… {type(value)}")

        # æ•°å€¼èŒƒå›´éªŒè¯
        range_checks = {
            'length': (0, 10000),  # mm
            'width': (0, 10000),   # mm
            'height': (0, 10000),  # mm
            'weight': (0, 100000), # g
            'shelf_days': (0, 10000)  # days
        }

        for field, (min_val, max_val) in range_checks.items():
            if field in erp_data and isinstance(erp_data[field], (int, float)):
                value = erp_data[field]
                is_in_range = min_val <= value <= max_val
                quality_validation['value_ranges'][field] = {
                    'value': value,
                    'min': min_val,
                    'max': max_val,
                    'valid': is_in_range
                }
                if not is_in_range:
                    quality_validation['issues'].append(f"{field} è¶…å‡ºåˆç†èŒƒå›´: {value} (æœŸæœ› {min_val}-{max_val})")

        # æ ¼å¼éªŒè¯
        if 'rfbs_commission_rates' in erp_data and isinstance(erp_data['rfbs_commission_rates'], list):
            rates = erp_data['rfbs_commission_rates']
            is_valid_rates = all(isinstance(rate, (int, float)) and 0 <= rate <= 100 for rate in rates)
            quality_validation['format_validation']['rfbs_commission_rates'] = {
                'value': rates,
                'valid': is_valid_rates
            }
            if not is_valid_rates:
                quality_validation['issues'].append(f"ä½£é‡‘ç‡æ ¼å¼é”™è¯¯: {rates}")

        # è®¡ç®—è´¨é‡åˆ†æ•°
        total_checks = len(quality_validation['data_types']) + len(quality_validation['value_ranges']) + len(quality_validation['format_validation'])
        if total_checks > 0:
            valid_checks = sum(1 for checks in [quality_validation['data_types'], quality_validation['value_ranges'], quality_validation['format_validation']] 
                             for check in checks.values() if check.get('valid', False))
            quality_validation['quality_score'] = (valid_checks / total_checks) * 100

        return quality_validation

    async def _test_single_scenario(self, test_case: Dict[str, Any]) -> Dict[str, Any]:
        """æµ‹è¯•å•ä¸ªåœºæ™¯"""
        test_id = test_case['id']
        test_name = test_case['name']
        url = test_case['url']
        description = test_case['description']

        print(f"\nğŸ§ª æµ‹è¯•åœºæ™¯: {test_name}")
        print(f"ğŸ“‹ æè¿°: {description}")
        print(f"ğŸ“ URL: {url}")

        start_time = time.time()
        result = {
            'test_id': test_id,
            'test_name': test_name,
            'url': url,
            'start_time': start_time,
            'success': False,
            'erp_data': {},
            'validation': {},
            'performance': {},
            'errors': []
        }

        try:
            # æ‰§è¡ŒERPæ•°æ®æŠ“å–
            print("ğŸ”„ å¼€å§‹æŠ“å–ERPæ•°æ®...")
            scrape_result = await self.scraper.scrape(product_url=url)
            
            execution_time = time.time() - start_time
            result['performance']['execution_time'] = execution_time
            result['performance']['erp_detection_time'] = getattr(scrape_result, 'erp_detection_time', None)

            if scrape_result.success and scrape_result.data:
                result['success'] = True
                result['erp_data'] = scrape_result.data
                
                print(f"âœ… ERPæ•°æ®æŠ“å–æˆåŠŸ")
                print(f"ğŸ“Š æå–å­—æ®µæ•°é‡: {len(scrape_result.data)}")
                print(f"â±ï¸ æ‰§è¡Œæ—¶é—´: {execution_time:.2f}ç§’")

                # æ•°æ®å®Œæ•´æ€§éªŒè¯
                completeness_validation = self._validate_erp_data_completeness(scrape_result.data, test_case)
                result['validation']['completeness'] = completeness_validation

                # æ•°æ®è´¨é‡éªŒè¯
                quality_validation = self._validate_erp_data_quality(scrape_result.data)
                result['validation']['quality'] = quality_validation

                # æ˜¾ç¤ºéªŒè¯ç»“æœ
                print(f"ğŸ“ˆ æ•°æ®å®Œæ•´æ€§: {completeness_validation['completeness_score']:.1f}%")
                print(f"ğŸ“ˆ æ•°æ®è´¨é‡: {quality_validation['quality_score']:.1f}%")

                if completeness_validation['missing_fields']:
                    print(f"âš ï¸ ç¼ºå¤±å­—æ®µ: {completeness_validation['missing_fields']}")

                if quality_validation['issues']:
                    print(f"âš ï¸ è´¨é‡é—®é¢˜: {quality_validation['issues']}")

            else:
                result['success'] = False
                error_msg = scrape_result.error_message if hasattr(scrape_result, 'error_message') else "æœªçŸ¥é”™è¯¯"
                result['errors'].append(f"ERPæ•°æ®æŠ“å–å¤±è´¥: {error_msg}")
                print(f"âŒ ERPæ•°æ®æŠ“å–å¤±è´¥: {error_msg}")

        except Exception as e:
            result['success'] = False
            result['errors'].append(f"æµ‹è¯•å¼‚å¸¸: {str(e)}")
            print(f"âŒ æµ‹è¯•å¼‚å¸¸: {e}")

        result['end_time'] = time.time()
        return result

    async def test_all_scenarios(self) -> List[Dict[str, Any]]:
        """æµ‹è¯•æ‰€æœ‰åœºæ™¯"""
        print(f"\nğŸ“‹ å¼€å§‹æµ‹è¯• {len(self.test_cases)} ä¸ªåœºæ™¯")
        print("=" * 80)

        results = []
        
        for i, test_case in enumerate(self.test_cases, 1):
            print(f"\n[{i}/{len(self.test_cases)}] æµ‹è¯•åœºæ™¯")
            result = await self._test_single_scenario(test_case)
            results.append(result)
            
            # çŸ­æš‚ä¼‘æ¯ï¼Œé¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
            if i < len(self.test_cases):
                print("â¸ï¸ ä¼‘æ¯ 2 ç§’...")
                await asyncio.sleep(2)

        self.test_results = results
        return results

    def generate_test_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        if not self.test_results:
            return {"error": "æ²¡æœ‰æµ‹è¯•ç»“æœ"}

        report = {
            'summary': {
                'total_tests': len(self.test_results),
                'successful_tests': 0,
                'failed_tests': 0,
                'success_rate': 0.0,
                'average_execution_time': 0.0,
                'average_completeness_score': 0.0,
                'average_quality_score': 0.0
            },
            'scenarios': [],
            'performance_analysis': {},
            'data_quality_analysis': {},
            'recommendations': []
        }

        total_execution_time = 0
        total_completeness_score = 0
        total_quality_score = 0
        completeness_count = 0
        quality_count = 0

        for result in self.test_results:
            # ç»Ÿè®¡æˆåŠŸ/å¤±è´¥
            if result['success']:
                report['summary']['successful_tests'] += 1
            else:
                report['summary']['failed_tests'] += 1

            # ç»Ÿè®¡æ‰§è¡Œæ—¶é—´
            if 'performance' in result and 'execution_time' in result['performance']:
                total_execution_time += result['performance']['execution_time']

            # ç»Ÿè®¡å®Œæ•´æ€§å’Œè´¨é‡åˆ†æ•°
            if 'validation' in result:
                if 'completeness' in result['validation']:
                    total_completeness_score += result['validation']['completeness']['completeness_score']
                    completeness_count += 1
                
                if 'quality' in result['validation']:
                    total_quality_score += result['validation']['quality']['quality_score']
                    quality_count += 1

            # æ·»åŠ åœºæ™¯è¯¦æƒ…
            scenario_summary = {
                'test_id': result['test_id'],
                'test_name': result['test_name'],
                'success': result['success'],
                'execution_time': result.get('performance', {}).get('execution_time', 0),
                'completeness_score': result.get('validation', {}).get('completeness', {}).get('completeness_score', 0),
                'quality_score': result.get('validation', {}).get('quality', {}).get('quality_score', 0),
                'errors': result.get('errors', [])
            }
            report['scenarios'].append(scenario_summary)

        # è®¡ç®—å¹³å‡å€¼
        total_tests = len(self.test_results)
        if total_tests > 0:
            report['summary']['success_rate'] = (report['summary']['successful_tests'] / total_tests) * 100
            report['summary']['average_execution_time'] = total_execution_time / total_tests

        if completeness_count > 0:
            report['summary']['average_completeness_score'] = total_completeness_score / completeness_count

        if quality_count > 0:
            report['summary']['average_quality_score'] = total_quality_score / quality_count

        # æ€§èƒ½åˆ†æ
        execution_times = [r.get('performance', {}).get('execution_time', 0) for r in self.test_results if r.get('performance', {}).get('execution_time')]
        if execution_times:
            report['performance_analysis'] = {
                'min_time': min(execution_times),
                'max_time': max(execution_times),
                'avg_time': sum(execution_times) / len(execution_times),
                'total_time': sum(execution_times)
            }

        # ç”Ÿæˆå»ºè®®
        if report['summary']['success_rate'] < 80:
            report['recommendations'].append("æˆåŠŸç‡è¾ƒä½ï¼Œå»ºè®®æ£€æŸ¥ERPæ’ä»¶æ£€æµ‹é€»è¾‘")
        
        if report['summary']['average_completeness_score'] < 70:
            report['recommendations'].append("æ•°æ®å®Œæ•´æ€§è¾ƒä½ï¼Œå»ºè®®å¢å¼ºå­—æ®µæå–é€»è¾‘")
        
        if report['summary']['average_quality_score'] < 80:
            report['recommendations'].append("æ•°æ®è´¨é‡è¾ƒä½ï¼Œå»ºè®®æ”¹è¿›æ•°æ®éªŒè¯å’Œæ¸…æ´—é€»è¾‘")

        if report['summary']['average_execution_time'] > 10:
            report['recommendations'].append("æ‰§è¡Œæ—¶é—´è¾ƒé•¿ï¼Œå»ºè®®ä¼˜åŒ–æ€§èƒ½")

        return report

    def print_test_report(self, report: Dict[str, Any]):
        """æ‰“å°æµ‹è¯•æŠ¥å‘Š"""
        print("\n" + "=" * 80)
        print("ğŸ“Š ERPæ’ä»¶å¢å¼ºæµ‹è¯•æŠ¥å‘Š")
        print("=" * 80)

        # æ€»ç»“
        summary = report['summary']
        print(f"\nğŸ“‹ æµ‹è¯•æ€»ç»“:")
        print(f"  æ€»æµ‹è¯•æ•°: {summary['total_tests']}")
        print(f"  æˆåŠŸæµ‹è¯•: {summary['successful_tests']}")
        print(f"  å¤±è´¥æµ‹è¯•: {summary['failed_tests']}")
        print(f"  æˆåŠŸç‡: {summary['success_rate']:.1f}%")
        print(f"  å¹³å‡æ‰§è¡Œæ—¶é—´: {summary['average_execution_time']:.2f}ç§’")
        print(f"  å¹³å‡å®Œæ•´æ€§åˆ†æ•°: {summary['average_completeness_score']:.1f}%")
        print(f"  å¹³å‡è´¨é‡åˆ†æ•°: {summary['average_quality_score']:.1f}%")

        # åœºæ™¯è¯¦æƒ…
        print(f"\nğŸ“‹ åœºæ™¯è¯¦æƒ…:")
        for scenario in report['scenarios']:
            status = "âœ…" if scenario['success'] else "âŒ"
            print(f"  {status} {scenario['test_name']}")
            print(f"    æ‰§è¡Œæ—¶é—´: {scenario['execution_time']:.2f}ç§’")
            print(f"    å®Œæ•´æ€§: {scenario['completeness_score']:.1f}%")
            print(f"    è´¨é‡: {scenario['quality_score']:.1f}%")
            if scenario['errors']:
                print(f"    é”™è¯¯: {scenario['errors']}")

        # æ€§èƒ½åˆ†æ
        if 'performance_analysis' in report and report['performance_analysis']:
            perf = report['performance_analysis']
            print(f"\nâš¡ æ€§èƒ½åˆ†æ:")
            print(f"  æœ€å¿«æ—¶é—´: {perf['min_time']:.2f}ç§’")
            print(f"  æœ€æ…¢æ—¶é—´: {perf['max_time']:.2f}ç§’")
            print(f"  å¹³å‡æ—¶é—´: {perf['avg_time']:.2f}ç§’")
            print(f"  æ€»è€—æ—¶: {perf['total_time']:.2f}ç§’")

        # å»ºè®®
        if report['recommendations']:
            print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
            for i, recommendation in enumerate(report['recommendations'], 1):
                print(f"  {i}. {recommendation}")

    async def run_enhanced_tests(self) -> bool:
        """è¿è¡Œå¢å¼ºæµ‹è¯•"""
        try:
            await self.setup()
            
            # è¿è¡Œæ‰€æœ‰åœºæ™¯æµ‹è¯•
            results = await self.test_all_scenarios()
            
            # ç”Ÿæˆå¹¶æ˜¾ç¤ºæŠ¥å‘Š
            report = self.generate_test_report()
            self.print_test_report(report)
            
            # åˆ¤æ–­æ•´ä½“æ˜¯å¦æˆåŠŸ
            success_rate = report['summary']['success_rate']
            return success_rate >= 75  # 75%ä»¥ä¸ŠæˆåŠŸç‡è®¤ä¸ºé€šè¿‡
            
        except Exception as e:
            print(f"âŒ å¢å¼ºæµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
        finally:
            await self.teardown()


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨ ERPæ’ä»¶å¢å¼ºæµ‹è¯•å¥—ä»¶")
    
    tester = ErpPluginEnhancedTester()
    
    try:
        success = await tester.run_enhanced_tests()
        
        if success:
            print("\nğŸ‰ ERPæ’ä»¶å¢å¼ºæµ‹è¯•é€šè¿‡ï¼")
            return 0
        else:
            print("\nâŒ ERPæ’ä»¶å¢å¼ºæµ‹è¯•å¤±è´¥ï¼")
            return 1
            
    except KeyboardInterrupt:
        print("\nâš ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        return 1
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    # è¿è¡Œå¢å¼ºæµ‹è¯•
    exit_code = asyncio.run(main())
    sys.exit(exit_code)


class TestErpPluginEnhanced(unittest.IsolatedAsyncioTestCase):
    """ERPæ’ä»¶å¢å¼ºå•å…ƒæµ‹è¯•"""
    
    async def asyncSetUp(self):
        """å¼‚æ­¥æµ‹è¯•åˆå§‹åŒ–"""
        self.tester = ErpPluginEnhancedTester()
        await self.tester.setup()
    
    async def asyncTearDown(self):
        """å¼‚æ­¥æµ‹è¯•æ¸…ç†"""
        await self.tester.teardown()
    
    async def test_scenario_1_no_competitors(self):
        """æµ‹è¯•åœºæ™¯1ï¼šæ— è·Ÿå–åº—é“º"""
        test_case = next((tc for tc in self.tester.test_cases if tc['id'] == 'scenario_1_no_competitors'), None)
        self.assertIsNotNone(test_case, "æœªæ‰¾åˆ°åœºæ™¯1æµ‹è¯•ç”¨ä¾‹")
        
        result = await self.tester._test_single_scenario(test_case)
        self.assertTrue(result['success'], f"åœºæ™¯1æµ‹è¯•å¤±è´¥: {result.get('errors', [])}")
    
    async def test_scenario_2_with_competitors(self):
        """æµ‹è¯•åœºæ™¯2ï¼šæœ‰è·Ÿå–åº—é“º"""
        test_case = next((tc for tc in self.tester.test_cases if tc['id'] == 'scenario_2_with_competitors'), None)
        self.assertIsNotNone(test_case, "æœªæ‰¾åˆ°åœºæ™¯2æµ‹è¯•ç”¨ä¾‹")
        
        result = await self.tester._test_single_scenario(test_case)
        self.assertTrue(result['success'], f"åœºæ™¯2æµ‹è¯•å¤±è´¥: {result.get('errors', [])}")
    
    async def test_scenario_3_many_competitors(self):
        """æµ‹è¯•åœºæ™¯3ï¼šå¤§é‡è·Ÿå–åº—é“º"""
        test_case = next((tc for tc in self.tester.test_cases if tc['id'] == 'scenario_3_many_competitors'), None)
        self.assertIsNotNone(test_case, "æœªæ‰¾åˆ°åœºæ™¯3æµ‹è¯•ç”¨ä¾‹")
        
        result = await self.tester._test_single_scenario(test_case)
        self.assertTrue(result['success'], f"åœºæ™¯3æµ‹è¯•å¤±è´¥: {result.get('errors', [])}")
    
    async def test_scenario_4_product_1176594312(self):
        """æµ‹è¯•åœºæ™¯4ï¼šç‰¹å®šå•†å“ID"""
        test_case = next((tc for tc in self.tester.test_cases if tc['id'] == 'scenario_4_product_1176594312'), None)
        self.assertIsNotNone(test_case, "æœªæ‰¾åˆ°åœºæ™¯4æµ‹è¯•ç”¨ä¾‹")
        
        result = await self.tester._test_single_scenario(test_case)
        self.assertTrue(result['success'], f"åœºæ™¯4æµ‹è¯•å¤±è´¥: {result.get('errors', [])}")
    
    async def test_all_scenarios_batch(self):
        """æ‰¹é‡æµ‹è¯•æ‰€æœ‰åœºæ™¯"""
        results = await self.tester.test_all_scenarios()
        
        # éªŒè¯è‡³å°‘æœ‰ä¸€äº›æµ‹è¯•æˆåŠŸ
        successful_count = sum(1 for r in results if r['success'])
        total_count = len(results)
        success_rate = (successful_count / total_count) * 100 if total_count > 0 else 0
        
        self.assertGreater(success_rate, 50, f"æˆåŠŸç‡è¿‡ä½: {success_rate:.1f}% ({successful_count}/{total_count})")
        
        # éªŒè¯æ¯ä¸ªæµ‹è¯•éƒ½æœ‰åŸºæœ¬çš„ç»“æœç»“æ„
        for result in results:
            self.assertIn('test_id', result)
            self.assertIn('success', result)
            self.assertIn('erp_data', result)
