#!/usr/bin/env python3
"""
è°ƒè¯•ä»·æ ¼æå–çš„è„šæœ¬
"""

import asyncio
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from apps.xuanping.common.scrapers.ozon_scraper import OzonScraper
from apps.xuanping.common.config import get_config

async def debug_price_extraction():
    """è°ƒè¯•ä»·æ ¼æå–"""
    print("ğŸ” è°ƒè¯•ä»·æ ¼æå– - å•†å“ 1756017628")
    print("="*50)
    
    config = get_config()
    scraper = OzonScraper(config)
    
    # æµ‹è¯•URL
    url = "https://www.ozon.ru/product/1756017628"
    print(f"ğŸ“ æµ‹è¯•URL: {url}")
    
    try:
        # è·å–é¡µé¢å†…å®¹
        async def get_page_content(browser_service):
            """è·å–é¡µé¢å†…å®¹"""
            try:
                # ç­‰å¾…é¡µé¢åŠ è½½
                await asyncio.sleep(2)
                # è·å–é¡µé¢å†…å®¹
                page_content = await browser_service.get_page_content()
                return {"success": True, "content": page_content}
            except Exception as e:
                return {"success": False, "error": str(e)}
        
        # ä½¿ç”¨æµè§ˆå™¨æœåŠ¡è·å–é¡µé¢å†…å®¹
        result = scraper.browser_service.scrape_page_data(url, get_page_content)
        
        if result.success:
            print("âœ… æˆåŠŸè·å–é¡µé¢å†…å®¹")
            page_content = result.data.get('content', '')
            print(f"ğŸ“„ é¡µé¢å†…å®¹é•¿åº¦: {len(page_content)} å­—ç¬¦")
            
            # ä¿å­˜é¡µé¢å†…å®¹åˆ°æ–‡ä»¶ä»¥ä¾¿åˆ†æ
            with open('debug_page_content.html', 'w', encoding='utf-8') as f:
                f.write(page_content)
            print("ğŸ’¾ é¡µé¢å†…å®¹å·²ä¿å­˜åˆ° debug_page_content.html")
            
            # æŸ¥æ‰¾ä»·æ ¼ç›¸å…³çš„å†…å®¹
            print("\nğŸ’° æŸ¥æ‰¾ä»·æ ¼ç›¸å…³ä¿¡æ¯:")
            
            # æŸ¥æ‰¾åŒ…å«â‚½ç¬¦å·çš„è¡Œ
            lines = page_content.split('\n')
            price_lines = [line for line in lines if 'â‚½' in line and len(line.strip()) > 0]
            
            print(f"ğŸ” æ‰¾åˆ° {len(price_lines)} è¡ŒåŒ…å«â‚½ç¬¦å·çš„å†…å®¹:")
            for i, line in enumerate(price_lines[:10], 1):  # åªæ˜¾ç¤ºå‰10è¡Œ
                # æ¸…ç†è¡Œå†…å®¹ï¼Œåªä¿ç•™å…³é”®ä¿¡æ¯
                cleaned_line = line.strip()
                if len(cleaned_line) > 100:
                    cleaned_line = cleaned_line[:100] + "..."
                print(f"   {i}. {cleaned_line}")
            
            # ç‰¹åˆ«æŸ¥æ‰¾ä»·æ ¼ 15949 å’Œ 16952
            print("\nğŸ¯ æŸ¥æ‰¾ç‰¹å®šä»·æ ¼:")
            target_prices = ['15949', '16952']
            for price in target_prices:
                if price in page_content:
                    print(f"   âœ… æ‰¾åˆ°ä»·æ ¼ {price}")
                    # æŸ¥æ‰¾åŒ…å«è¯¥ä»·æ ¼çš„è¡Œ
                    for line in lines:
                        if price in line and 'â‚½' in line:
                            cleaned_line = line.strip()
                            if len(cleaned_line) > 100:
                                cleaned_line = cleaned_line[:100] + "..."
                            print(f"      {cleaned_line}")
                else:
                    print(f"   âŒ æœªæ‰¾åˆ°ä»·æ ¼ {price}")
        else:
            print(f"âŒ è·å–é¡µé¢å†…å®¹å¤±è´¥: {result.error_message}")
            
    except Exception as e:
        print(f"âŒ è°ƒè¯•è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
    finally:
        scraper.close()

if __name__ == "__main__":
    asyncio.run(debug_price_extraction())