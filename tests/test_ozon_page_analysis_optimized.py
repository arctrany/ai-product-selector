#!/usr/bin/env python3
"""
OZONé¡µé¢åˆ†ææµ‹è¯•ç¨‹åº - åŸºäºKeyåŒ¹é…çš„æŠ“å–å™¨
é‡å†™ç‰ˆæœ¬ï¼šä¸“æ³¨äºåŸºäºä¸­æ–‡å­—æ®µåè¿›è¡Œæ•°æ®åŒ¹é…å’Œæå–
æ”¯æŒseefarå’Œdianpengä¸¤ç§æ•°æ®æºæ ¼å¼
æ³¨æ„ï¼šä½¿ç”¨keyåŒ¹é…è€Œéæ•°å€¼åŒ¹é…ï¼Œç¡®ä¿æ•°æ®æå–çš„ç¨³å®šæ€§
"""

import re
import json
from typing import Dict, List, Tuple, Any, Optional

# å®šä¹‰æœŸæœ›çš„æ•°æ®ç»“æ„
seefar_data_tuples = [
    ("sku", "SKU", "2423301080"),
    ("sales_quantity_30days", "è¿‘30å¤©é”€é‡", 6),
    ("sales_amount_30days", "è¿‘30å¤©é”€å”®é¢", "18 538 â‚½"),
    ("gross_profit_margin", "æ¯›åˆ©ç‡", "50%"),
    ("return_cancel_rate", "é€€è´§å–æ¶ˆç‡", "0%"),
    ("exposure_count", "æ›å…‰é‡", 124),
    ("product_card_views", "äº§å“å¡æµè§ˆé‡", 238),
    ("add_to_cart_rate", "åŠ è´­ç‡", "4.62%"),
    ("advertising_cost_share", "å¹¿å‘Šè´¹ç”¨ä»½é¢", "0%"),
    ("brand", "å“ç‰Œ", "-"),
    ("seller", "å–å®¶", "ZONFANT"),
    ("delivery_type", "é…é€", "RFBS"),
    ("variant_count", "å˜ä½“æ•°", 3),
    ("competitor_count", "è·Ÿå–æ•°", 0),
    ("weight", "é‡é‡", "2500 g"),
    ("dimensions", "ä½“ç§¯", "550Ã—500Ã—100mm"),
    ("category", "ç±»ç›®", "åå¤‡ç®±å«"),
    ("inventory", "åº“å­˜", "-"),
    ("listing_time", "ä¸Šæ¶æ—¶é—´", "2025-07-07(3 ä¸ªæœˆ)")
]

dianpeng_data_tuples = [
    ("sku", "sku", "2423301080"),
    ("brand", "å“ç‰Œ", "Ğ±ĞµĞ· Ğ±Ñ€ĞµĞ½Ğ´Ğ°"),
    ("category_level_1", "ä¸€çº§ç±»ç›®", "ĞĞ²Ñ‚Ğ¾Ñ‚Ğ¾Ğ²Ğ°Ñ€Ñ‹"),
    ("category_level_3", "ä¸‰çº§ç±»ç›®", "ĞšĞ¾Ğ²Ñ€Ğ¸Ğº Ğ² Ğ±Ğ°Ğ³Ğ°Ğ¶Ğ½Ğ¸Ğº"),
    ("product_code", "è´§å·", "CS95HBXD-2"),
    ("promotion_activity", "ä¿ƒé”€æ´»åŠ¨", "28å¤©å‚ä¸28å¤©"),
    ("monthly_sales_amount", "æœˆé”€å”®é¢", "24711.102â‚½"),
    ("monthly_sales_quantity", "æœˆé”€é‡", 7),
    ("follow_count", "è¢«è·Ÿæ•°é‡", "N/A"),
    ("min_price", "æœ€ä½ä»·", "N/A"),
    ("max_price", "æœ€é«˜ä»·", "N/A"),
    ("product_clicks", "å•†å“ç‚¹å‡»é‡", 253),
    ("cart_conversion_rate", "è´­ç‰©è½¦è½¬åŒ–ç‡", "4.74%"),
    ("total_impressions", "å•†å“å±•ç¤ºæ€»é‡", 1169),
    ("impression_conversion_rate", "å±•ç¤ºè½¬åŒ–ç‡", "0.598%"),
    ("transaction_rate", "æˆäº¤ç‡", "85.7% (é™¤å»æœªå–æ¶ˆ/æœªé€€å›çš„è®¢å•)"),
    ("commission_rates", "ä½£é‡‘è´¹ç‡", "ä»·æ ¼â‰¤1500å¢å¸ƒ:12.0%\nä»·æ ¼>1501å¢å¸ƒä»·æ ¼â‰¤5000å¢å¸ƒ:17.0%\nä»·æ ¼>5001å¢å¸ƒ:17.0%"),
    ("volume", "ä½“ç§¯", "27.5 å…¬å‡(é•¿xå®½xé«˜)"),
    ("average_price", "å¹³å‡ä»·æ ¼", "3530â‚½"),
    ("seller_type", "å–å®¶ç±»å‹", "FBS"),
    ("turnover_dynamics", "å‘¨è½¬åŠ¨æ€", "254.6%"),
    ("product_creation_time", "å•†å“åˆ›å»ºæ—¶é—´", "07.07.2025 (å·²åˆ›å»º 106 å¤©)"),
    ("length", "é•¿åº¦", "550mm"),
    ("width", "å®½åº¦", "500mm"),
    ("height", "é«˜åº¦", "100mm"),
    ("weight", "é‡é‡", "2500g")
]

@dataclass
class ProductData:
    """å•†å“æ•°æ®æ¨¡å‹"""
    sku: Optional[str] = None
    brand: Optional[str] = None
    category_1: Optional[str] = None
    category_3: Optional[str] = None
    product_code: Optional[str] = None
    monthly_sales: Optional[str] = None
    monthly_revenue: Optional[str] = None
    click_count: Optional[str] = None
    cart_conversion_rate: Optional[str] = None
    display_count: Optional[str] = None
    display_conversion_rate: Optional[str] = None
    transaction_rate: Optional[str] = None
    commission_rates: Optional[Dict[str, str]] = None
    volume_capacity: Optional[str] = None
    average_price: Optional[str] = None
    seller_type: Optional[str] = None
    seller_name: Optional[str] = None
    turnover_dynamics: Optional[str] = None
    creation_time: Optional[str] = None
    dimensions: Optional[Dict[str, str]] = None
    weight: Optional[str] = None
    inventory: Optional[str] = None
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {k: v for k, v in asdict(self).items() if v is not None}

@dataclass
class ExtractionResult:
    """æå–ç»“æœæ¨¡å‹"""
    success: bool
    data: ProductData
    metadata: Dict[str, Any]
    performance_stats: Dict[str, float]
    
    def to_json(self) -> str:
        """è½¬æ¢ä¸ºJSONæ ¼å¼"""
        result = {
            'success': self.success,
            'data': self.data.to_dict(),
            'metadata': self.metadata,
            'performance_stats': self.performance_stats
        }
        return json.dumps(result, ensure_ascii=False, indent=2)

class OptimizedRegexMatcher:
    """ä¼˜åŒ–çš„æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…å™¨"""
    
    def __init__(self):
        # é¢„ç¼–è¯‘æ‰€æœ‰æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
        self.compiled_patterns = self._compile_patterns()
    
    def _compile_patterns(self) -> List[Tuple[re.Pattern, str]]:
        """é¢„ç¼–è¯‘æ‰€æœ‰æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼"""
        patterns = [
            # SKUæ¨¡å¼ - ç²¾ç¡®åŒ¹é…æ•°å­—
            (r'sku[:\s]*(\d{8,15})', 'sku'),
            (r'å•†å“ç¼–å·[:\s]*(\d{8,15})', 'sku'),
            (r'Ğ°Ñ€Ñ‚Ğ¸ĞºÑƒĞ»[:\s]*(\d{8,15})', 'sku'),
            (r'(\d{10})', 'sku'),  # åŒ¹é…10ä½æ•°å­—ä½œä¸ºSKU

            # å“ç‰Œæ¨¡å¼ - åŒ¹é…å“ç‰Œå
            (r'å“ç‰Œ[:\s]*([^\s]{1,30})', 'å“ç‰Œ'),
            (r'Ğ±Ñ€ĞµĞ½Ğ´[:\s]*([^\s]{1,30})', 'å“ç‰Œ'),
            (r'brand[:\s]*([^\s]{1,30})', 'å“ç‰Œ'),
            (r'(Ğ±ĞµĞ· Ğ±Ñ€ĞµĞ½Ğ´Ğ°)', 'å“ç‰Œ'),  # åŒ¹é…"Ğ±ĞµĞ· Ğ±Ñ€ĞµĞ½Ğ´Ğ°"

            # ç±»ç›®æ¨¡å¼ - åŒ¹é…åˆ†ç±»ä¿¡æ¯
            (r'ä¸€çº§ç±»ç›®[:\s]*([^\s]{1,50})', 'ä¸€çº§ç±»ç›®'),
            (r'ä¸‰çº§ç±»ç›®[:\s]*([^\s]{1,50})', 'ä¸‰çº§ç±»ç›®'),
            (r'ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ñ[:\s]*([^\s]{1,50})', 'ä¸€çº§ç±»ç›®'),

            # è´§å·æ¨¡å¼
            (r'è´§å·[:\s]*([A-Za-z0-9\-]{1,20})', 'è´§å·'),
            (r'Ğ°Ñ€Ñ‚Ğ¸ĞºÑƒĞ»[:\s]*([A-Za-z0-9\-]{1,20})', 'è´§å·'),

            # ä¿ƒé”€æ´»åŠ¨æ¨¡å¼
            (r'ä¿ƒé”€æ´»åŠ¨[:\s]*([^\så•†è´­å±•æˆä½£ä½“å¹³å–å‘¨é•¿å®½é«˜é‡]{1,30})', 'ä¿ƒé”€æ´»åŠ¨'),
            (r'(\d+å¤©å‚ä¸\d+å¤©)', 'ä¿ƒé”€æ´»åŠ¨'),  # åŒ¹é…"28å¤©å‚ä¸28å¤©"æ ¼å¼

            # é”€é‡æ¨¡å¼ - ç²¾ç¡®åŒ¹é…æ•°å­—
            (r'æœˆé”€é‡[:\s]*(\d+)', 'æœˆé”€é‡'),
            (r'é”€é‡[:\s]*(\d+)', 'æœˆé”€é‡'),
            (r'Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ¶Ğ¸[:\s]*(\d+)', 'æœˆé”€é‡'),

            # é”€å”®é¢æ¨¡å¼ - åŒ¹é…â‚½ç¬¦å·çš„é‡‘é¢
            (r'æœˆé”€å”®é¢[:\s]*([0-9.,]+â‚½)', 'æœˆé”€å”®é¢'),
            (r'è¿‘30å¤©é”€å”®é¢[:\s]*([0-9\s.,]+â‚½)', 'è¿‘30å¤©é”€å”®é¢'),
            (r'([0-9]+\.[0-9]+â‚½)', 'æœˆé”€å”®é¢'),  # åŒ¹é…"24711.102â‚½"æ ¼å¼

            # é‡é‡æ¨¡å¼ - ç²¾ç¡®åŒ¹é…æ•°å­—+å•ä½
            (r'é‡é‡[:\s]*(\d+g)', 'é‡é‡'),
            (r'Ğ²ĞµÑ[:\s]*(\d+\s*[gĞºĞ³])', 'é‡é‡'),
            (r'Ğ’ĞµÑ[:\s]*(\d+\s*[gĞºĞ³])', 'é‡é‡'),
            (r'(\d+g)(?=\s|$|Ğš)', 'é‡é‡'),  # åŒ¹é… "2500g" æ ¼å¼

            # å°ºå¯¸æ¨¡å¼ - åŒ¹é…mmå•ä½
            (r'é•¿åº¦[:\s]*(\d+mm)', 'é•¿åº¦'),
            (r'å®½åº¦[:\s]*(\d+mm)', 'å®½åº¦'),
            (r'é«˜åº¦[:\s]*(\d+mm)', 'é«˜åº¦'),

            # å„ç§è½¬åŒ–ç‡æ¨¡å¼ - åŒ¹é…ç™¾åˆ†æ¯”
            (r'æ¯›åˆ©ç‡[:\s]*(\d+\.?\d*%)', 'æ¯›åˆ©ç‡'),
            (r'è´­ç‰©è½¦è½¬åŒ–ç‡[:\s]*(\d+\.?\d*%)', 'è´­ç‰©è½¦è½¬åŒ–ç‡'),
            (r'å±•ç¤ºè½¬åŒ–ç‡[:\s]*(\d+\.?\d*%)', 'å±•ç¤ºè½¬åŒ–ç‡'),
            (r'æˆäº¤ç‡[:\s]*(\d+\.?\d*%[^)]*(?:\([^)]*\))?)', 'æˆäº¤ç‡'),  # åŒ…å«æ‹¬å·è¯´æ˜
            (r'å‘¨è½¬åŠ¨æ€[:\s]*(\d+\.?\d*%)', 'å‘¨è½¬åŠ¨æ€'),

            # æ›å…‰é‡å’Œç‚¹å‡»é‡æ¨¡å¼
            (r'æ›å…‰é‡[:\s]*(\d+)', 'æ›å…‰é‡'),
            (r'å•†å“ç‚¹å‡»é‡[:\s]*(\d+)', 'å•†å“ç‚¹å‡»é‡'),
            (r'å•†å“å±•ç¤ºæ€»é‡[:\s]*(\d+)', 'å•†å“å±•ç¤ºæ€»é‡'),

            # åŠ è´­ç‡æ¨¡å¼
            (r'åŠ è´­ç‡[:\s]*(\d+\.?\d*%)', 'åŠ è´­ç‡'),

            # å–å®¶ç›¸å…³æ¨¡å¼ - åŒ¹é…å®é™…æ ¼å¼
            (r'å–å®¶[:\s]*([A-Za-z0-9\u4e00-\u9fff\u0400-\u04ff]{1,30})', 'å–å®¶'),
            (r'å–å®¶ç±»å‹[:\s]*([A-Za-z0-9]{1,10})', 'å–å®¶ç±»å‹'),
            (r'é…é€[:\s]*([A-Za-z0-9]{1,10})', 'é…é€'),
            (r'Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ²ĞµÑ†[:\s]*([A-Za-z0-9\u0400-\u04ff]{1,30})', 'å–å®¶'),
            # ç‰¹æ®ŠåŒ¹é…ZONFANTå’ŒRFBS
            (r'(ZONFANT)', 'å–å®¶'),
            (r'(RFBS)', 'é…é€'),

            # æ—¶é—´æ¨¡å¼ - åŒ¹é…æ—¥æœŸæ ¼å¼
            (r'ä¸Šæ¶æ—¶é—´[:\s]*([0-9\-]+(?:\([^)]+\))?)', 'ä¸Šæ¶æ—¶é—´'),
            (r'å•†å“åˆ›å»ºæ—¶é—´[:\s]*([0-9.]+\s*\([^)]+\))', 'å•†å“åˆ›å»ºæ—¶é—´'),
            (r'(\d{2}\.\d{2}\.\d{4}\s*\([^)]+\))', 'å•†å“åˆ›å»ºæ—¶é—´'),  # åŒ¹é…"07.07.2025 (å·²åˆ›å»º 106 å¤©)"
            (r'å•†å“åˆ›å»ºæ—¶é—´[:\s]*(\d{2}\.\d{2}\.\d{4}\s*\([^)]+\))', 'å•†å“åˆ›å»ºæ—¶é—´'),  # å®Œæ•´åŒ¹é…"å•†å“åˆ›å»ºæ—¶é—´:07.07.2025 (å·²åˆ›å»º 106 å¤©)"

            # ä»·æ ¼æ¨¡å¼ - åŒ¹é…â‚½ç¬¦å·å’Œç‰¹æ®Šå€¼
            (r'å¹³å‡ä»·æ ¼[:\s]*([0-9]+â‚½)', 'å¹³å‡ä»·æ ¼'),
            (r'æœ€ä½ä»·[:\s]*([0-9]+â‚½|N/A|æ— |--)', 'æœ€ä½ä»·'),
            (r'æœ€é«˜ä»·[:\s]*([0-9]+â‚½|N/A|æ— |--)', 'æœ€é«˜ä»·'),

            # ä½£é‡‘è´¹ç‡æ¨¡å¼ - åŒ¹é…å¤æ‚çš„è´¹ç‡ç»“æ„
            (r'ä»·æ ¼â‰¤1500å¢å¸ƒ[:\s]*(\d+\.?\d*%)', 'ä½£é‡‘è´¹ç‡1500ä»¥ä¸‹'),
            (r'ä»·æ ¼>1501å¢å¸ƒä»·æ ¼â‰¤5000å¢å¸ƒ[:\s]*(\d+\.?\d*%)', 'ä½£é‡‘è´¹ç‡1501-5000'),
            (r'ä»·æ ¼>5001å¢å¸ƒ[:\s]*(\d+\.?\d*%)', 'ä½£é‡‘è´¹ç‡5001ä»¥ä¸Š'),
            # å®Œæ•´ä½£é‡‘è´¹ç‡ç»“æ„åŒ¹é…
            (r'ä½£é‡‘è´¹ç‡[:\s]*\n?(?:ä»·æ ¼â‰¤1500å¢å¸ƒ[:\s]*(\d+\.?\d*%)\n?)?(?:ä»·æ ¼>1501å¢å¸ƒä»·æ ¼â‰¤5000å¢å¸ƒ[:\s]*(\d+\.?\d*%)\n?)?(?:ä»·æ ¼>5001å¢å¸ƒ[:\s]*(\d+\.?\d*%)\n?)?', 'å®Œæ•´ä½£é‡‘è´¹ç‡'),

            # ä½“ç§¯æ¨¡å¼ - åŒ¹é…ä¸åŒæ ¼å¼
            (r'ä½“ç§¯[:\s]*(\d+Ã—\d+Ã—\d+\s*mm)', 'ä½“ç§¯'),
            (r'ä½“ç§¯[:\s]*([0-9.]+\s*å…¬å‡[^å•†è´­å±•æˆä½£å¹³å–å‘¨é•¿å®½é«˜é‡]*)', 'ä½“ç§¯å®¹é‡'),
            (r'([0-9.]+\s*å…¬å‡\([^)]+\))', 'ä½“ç§¯å®¹é‡'),  # åŒ¹é…"27.5 å…¬å‡(é•¿xå®½xé«˜)"
            (r'ä½“ç§¯[:\s]*([0-9.]+\s*å…¬å‡\([^)]+\))', 'ä½“ç§¯å®¹é‡'),  # å®Œæ•´åŒ¹é…"ä½“ç§¯:27.5 å…¬å‡(é•¿xå®½xé«˜)"
            (r'Ñ€Ğ°Ğ·Ğ¼ĞµÑ€[:\s]*(\d+Ã—\d+Ã—\d+\s*Ğ¼Ğ¼)', 'ä½“ç§¯'),
        ]
        
        return [(re.compile(pattern, re.IGNORECASE | re.MULTILINE), key) 
                for pattern, key in patterns]
    
    def extract_data_parallel(self, text: str, source_name: str) -> Dict[str, str]:
        """å¹¶è¡Œæå–æ•°æ®"""
        def match_pattern(pattern_key_pair):
            pattern, key = pattern_key_pair
            matches = pattern.findall(text)
            if matches:
                raw_value = matches[0].strip()
                if raw_value:
                    cleaned_value = self._clean_extracted_value(raw_value, key)
                    if cleaned_value:
                        return key, cleaned_value
            return None, None
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†æ­£åˆ™åŒ¹é…
        with ThreadPoolExecutor(max_workers=8) as executor:
            results = list(executor.map(match_pattern, self.compiled_patterns))
        
        # æ”¶é›†æœ‰æ•ˆç»“æœ
        kv_dict = {}
        for key, value in results:
            if key and value and key not in kv_dict:
                kv_dict[key] = value
                print(f"   ğŸ“ {source_name} æå–: {key} = {value}")
        
        return kv_dict
    
    def _parse_commission_rates(self, text: str) -> Dict[str, str]:
        """è§£æå®Œæ•´çš„ä½£é‡‘è´¹ç‡ç»“æ„"""
        commission_rates = {}

        # åŒ¹é…å®Œæ•´çš„ä½£é‡‘è´¹ç‡å—
        commission_pattern = r'ä½£é‡‘è´¹ç‡[:\s]*\n?((?:ä»·æ ¼[^:]+:[^%]+%\n?)+)'
        commission_match = re.search(commission_pattern, text, re.IGNORECASE | re.MULTILINE)

        if commission_match:
            commission_block = commission_match.group(1)

            # è§£æå„ä¸ªè´¹ç‡æ®µ
            rate_patterns = [
                (r'ä»·æ ¼â‰¤1500å¢å¸ƒ[:\s]*(\d+\.?\d*%)', 'ä½£é‡‘è´¹ç‡1500ä»¥ä¸‹'),
                (r'ä»·æ ¼>1501å¢å¸ƒä»·æ ¼â‰¤5000å¢å¸ƒ[:\s]*(\d+\.?\d*%)', 'ä½£é‡‘è´¹ç‡1501-5000'),
                (r'ä»·æ ¼>5001å¢å¸ƒ[:\s]*(\d+\.?\d*%)', 'ä½£é‡‘è´¹ç‡5001ä»¥ä¸Š')
            ]

            for pattern, key in rate_patterns:
                match = re.search(pattern, commission_block, re.IGNORECASE)
                if match:
                    commission_rates[key] = match.group(1)

        return commission_rates

    def _clean_extracted_value(self, value: str, key: str) -> str:
        """æ¸…ç†æå–çš„å€¼"""
        if not value:
            return ""

        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        value = re.sub(r'\s+', ' ', value).strip()

        # å¤„ç†å®Œæ•´ä½£é‡‘è´¹ç‡
        if key == 'å®Œæ•´ä½£é‡‘è´¹ç‡':
            # å¯¹äºå®Œæ•´ä½£é‡‘è´¹ç‡ï¼Œè¿”å›åŸå§‹æ–‡æœ¬ç”¨äºåç»­è§£æ
            return value

        # æ ¹æ®ä¸åŒçš„é”®è¿›è¡Œç‰¹å®šæ¸…ç†
        if key == 'sku':
            # SKUåªä¿ç•™æ•°å­—
            value = re.sub(r'[^\d]', '', value)
            if len(value) < 5:  # SKUè‡³å°‘5ä½æ•°å­—
                return ""

        elif key == 'å“ç‰Œ':
            # å“ç‰Œåæ¸…ç†ï¼šç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™å­—æ¯æ•°å­—å’Œå¸¸è§ç¬¦å·
            value = re.sub(r'[^\w\s\-\.&]', '', value)
            if len(value) > 50:  # å“ç‰Œåä¸åº”è¯¥å¤ªé•¿
                return ""

        elif key == 'é‡é‡':
            # é‡é‡æ ¼å¼æ ‡å‡†åŒ–
            value = re.sub(r'\s+', ' ', value)
            if not re.match(r'\d+\s*[gĞºĞ³]', value):
                return ""

        elif key == 'ä½“ç§¯':
            # ä½“ç§¯æ ¼å¼æ ‡å‡†åŒ–
            value = re.sub(r'\s+', '', value)
            if not re.match(r'\d+Ã—\d+Ã—\d+mm', value):
                return ""

        elif key == 'å–å®¶':
            # å–å®¶åæ¸…ç†
            value = re.sub(r'[^\w\s\-\.]', '', value)
            if len(value) > 30:  # å–å®¶åä¸åº”è¯¥å¤ªé•¿
                return ""

        elif key in ['æ¯›åˆ©ç‡', 'åŠ è´­ç‡']:
            # ç™¾åˆ†æ¯”æ ¼å¼æ£€æŸ¥
            if not value.endswith('%'):
                return ""

        elif key in ['é”€é‡', 'æ›å…‰é‡']:
            # æ•°å­—æ ¼å¼æ£€æŸ¥
            if not value.isdigit():
                return ""

        return value

class OptimizedOzonPageAnalyzer:
    """ä¼˜åŒ–çš„OZONé¡µé¢åˆ†æå™¨"""

    def __init__(self, debug_mode=True):
        self.debug_mode = debug_mode
        self.logger = get_logger(debug_mode)
        self.browser_service = None
        self.regex_matcher = OptimizedRegexMatcher()
        self.performance_stats = {}
    
    async def analyze_product_page(self, url: str) -> Optional[ExtractionResult]:
        """åˆ†æå•†å“é¡µé¢ç»“æ„ - æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬"""
        start_time = time.time()
        
        try:
            # åˆå§‹åŒ–æµè§ˆå™¨æœåŠ¡
            print("ğŸš€ åˆå§‹åŒ–æµè§ˆå™¨æœåŠ¡...")
            init_start = time.time()
            self.browser_service = BrowserService(debug_port=9222, headless=False)
            
            if not await self.browser_service.init_browser():
                print("âŒ æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥")
                return None
            
            self.performance_stats['browser_init_time'] = time.time() - init_start

            # è·å–é¡µé¢å¯¹è±¡å¹¶è®¿é—®URL
            page_start = time.time()
            page = await self.browser_service.get_page()
            if not page:
                print("âŒ æ— æ³•è·å–é¡µé¢å¯¹è±¡")
                return None

            print(f"ğŸŒ æ­£åœ¨è®¿é—®é¡µé¢: {url}")
            try:
                await page.goto(url, wait_until='domcontentloaded', timeout=60000)
                print("âœ… é¡µé¢DOMåŠ è½½å®Œæˆ")
            except Exception as e:
                print(f"âš ï¸ é¡µé¢åŠ è½½è¶…æ—¶ï¼Œå°è¯•ç»§ç»­åˆ†æ: {str(e)}")

            # ç­‰å¾…é¡µé¢ç¨³å®š
            await asyncio.sleep(3)  # å‡å°‘ç­‰å¾…æ—¶é—´
            self.performance_stats['page_load_time'] = time.time() - page_start

            # è·å–é¡µé¢æ ‡é¢˜
            title = await page.title()
            print(f"ğŸ“„ é¡µé¢æ ‡é¢˜: {title}")

            # å¹¶è¡Œæå–æ‰€æœ‰äº§å“æ•°æ®
            extraction_start = time.time()
            result = await self._extract_all_product_data_parallel(page)
            self.performance_stats['data_extraction_time'] = time.time() - extraction_start
            
            # æ€»æ‰§è¡Œæ—¶é—´
            self.performance_stats['total_time'] = time.time() - start_time
            
            # åˆ›å»ºç»“æœå¯¹è±¡
            extraction_result = ExtractionResult(
                success=True,
                data=result,
                metadata={
                    'url': url,
                    'title': title,
                    'extraction_timestamp': time.time(),
                    'total_fields_extracted': len(result.to_dict())
                },
                performance_stats=self.performance_stats
            )
            
            return extraction_result

        except Exception as e:
            print(f"âŒ é¡µé¢åˆ†æå¤±è´¥: {str(e)}")
            return ExtractionResult(
                success=False,
                data=ProductData(),
                metadata={'error': str(e)},
                performance_stats=self.performance_stats
            )
        finally:
            if self.browser_service:
                print("ğŸ”„ å…³é—­æµè§ˆå™¨æœåŠ¡...")
                await self.browser_service.close_browser()

    async def _extract_all_product_data_parallel(self, page) -> ProductData:
        """å¹¶è¡Œæå–æ‰€æœ‰å•†å“æ•°æ®"""
        print("\n" + "="*80)
        print("ğŸš€ å¼€å§‹å¹¶è¡Œæ•°æ®æå–")
        print("="*80)

        # å¹¶è¡Œæ‰§è¡Œå¤šä¸ªæå–ä»»åŠ¡
        tasks = [
            self._extract_dianpeng_data_optimized(page),
            self._extract_seefar_data_optimized(page),
            self._extract_additional_data_optimized(page)
        ]
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # åˆå¹¶æ‰€æœ‰ç»“æœ
        all_data = {}
        for i, result in enumerate(results):
            if isinstance(result, dict):
                all_data.update(result)
                print(f"âœ… ä»»åŠ¡ {i+1} å®Œæˆï¼Œæå–åˆ° {len(result)} ä¸ªå­—æ®µ")
            else:
                print(f"âš ï¸ ä»»åŠ¡ {i+1} å¤±è´¥: {result}")

        # è½¬æ¢ä¸ºProductDataå¯¹è±¡
        product_data = self._convert_to_product_data(all_data)
        
        print(f"\nğŸ¯ æ€»è®¡æå–å­—æ®µæ•°: {len(all_data)}")
        return product_data

    async def _extract_dianpeng_data_optimized(self, page) -> Dict[str, str]:
        """ä¼˜åŒ–çš„ç”µé¹åŒºåŸŸæ•°æ®æå–"""
        print("\nğŸ“Š ç”µé¹åŒºåŸŸæ•°æ®æå– (å¹¶è¡Œä¼˜åŒ–)")
        
        # å¹¶è¡ŒæŸ¥è¯¢å¤šä¸ªé€‰æ‹©å™¨
        selectors = [
            '[data-widget="webProductHeading"]',
            '[data-widget="webSale"]',
            '[data-widget="webCurrentSeller"]',
            '[data-widget="webDetailSKU"]',
            'div:has-text("sku")',
            'div:has-text("é”€é‡")',
            'div:has-text("é”€å”®é¢")'
        ]
        
        # å¹¶è¡Œè·å–æ‰€æœ‰å…ƒç´ çš„æ–‡æœ¬
        tasks = []
        for selector in selectors:
            tasks.append(self._get_elements_text_safe(page, selector))
        
        texts = await asyncio.gather(*tasks, return_exceptions=True)
        
        # åˆå¹¶æ‰€æœ‰æ–‡æœ¬
        combined_text = ""
        for text in texts:
            if isinstance(text, str):
                combined_text += text + " "
        
        # ä½¿ç”¨ä¼˜åŒ–çš„æ­£åˆ™åŒ¹é…å™¨æå–æ•°æ®
        extracted_data = self.regex_matcher.extract_data_parallel(combined_text, "ç”µé¹åŒºåŸŸ")

        # å¦‚æœæå–åˆ°å®Œæ•´ä½£é‡‘è´¹ç‡ï¼Œè¿›è¡Œè¿›ä¸€æ­¥è§£æ
        if 'å®Œæ•´ä½£é‡‘è´¹ç‡' in extracted_data:
            commission_rates = self.regex_matcher._parse_commission_rates(combined_text)
            extracted_data.update(commission_rates)
            # ç§»é™¤åŸå§‹çš„å®Œæ•´ä½£é‡‘è´¹ç‡å­—æ®µ
            del extracted_data['å®Œæ•´ä½£é‡‘è´¹ç‡']

        return extracted_data

    async def _extract_seefar_data_optimized(self, page) -> Dict[str, str]:
        """ä¼˜åŒ–çš„seefaråŒºåŸŸæ•°æ®æå–"""
        print("\nğŸ“Š seefaråŒºåŸŸæ•°æ®æå– (å¹¶è¡Œä¼˜åŒ–)")
        
        # å¹¶è¡ŒæŸ¥è¯¢å¤šä¸ªé€‰æ‹©å™¨
        selectors = [
            'div[data-widget*="seefar"]',
            'div[class*="seefar"]',
            'span:has-text("é”€é‡")',
            'span:has-text("é”€å”®é¢")',
            'span:has-text("è½¬åŒ–ç‡")',
            'div:has-text("å–å®¶")'
        ]
        
        # å¹¶è¡Œè·å–æ‰€æœ‰å…ƒç´ çš„æ–‡æœ¬
        tasks = []
        for selector in selectors:
            tasks.append(self._get_elements_text_safe(page, selector))
        
        texts = await asyncio.gather(*tasks, return_exceptions=True)
        
        # åˆå¹¶æ‰€æœ‰æ–‡æœ¬
        combined_text = ""
        for text in texts:
            if isinstance(text, str):
                combined_text += text + " "
        
        # ä½¿ç”¨ä¼˜åŒ–çš„æ­£åˆ™åŒ¹é…å™¨æå–æ•°æ®
        extracted_data = self.regex_matcher.extract_data_parallel(combined_text, "seefaråŒºåŸŸ")

        # å¦‚æœæå–åˆ°å®Œæ•´ä½£é‡‘è´¹ç‡ï¼Œè¿›è¡Œè¿›ä¸€æ­¥è§£æ
        if 'å®Œæ•´ä½£é‡‘è´¹ç‡' in extracted_data:
            commission_rates = self.regex_matcher._parse_commission_rates(combined_text)
            extracted_data.update(commission_rates)
            # ç§»é™¤åŸå§‹çš„å®Œæ•´ä½£é‡‘è´¹ç‡å­—æ®µ
            del extracted_data['å®Œæ•´ä½£é‡‘è´¹ç‡']

        return extracted_data

    async def _extract_additional_data_optimized(self, page) -> Dict[str, str]:
        """æå–é¢å¤–çš„å•†å“æ•°æ®"""
        print("\nğŸ“Š é¢å¤–æ•°æ®æå– (å¹¶è¡Œä¼˜åŒ–)")
        
        # å¹¶è¡ŒæŸ¥è¯¢é¡µé¢ä¸­çš„å…¶ä»–é‡è¦ä¿¡æ¯
        selectors = [
            'div[data-widget="webProductHeading"]',
            'div[data-widget="webPrice"]',
            'div[data-widget="webGallery"]',
            'div[data-widget="webFeatures"]',
            'div[data-widget="webCharacteristics"]'
        ]
        
        # å¹¶è¡Œè·å–æ‰€æœ‰å…ƒç´ çš„æ–‡æœ¬
        tasks = []
        for selector in selectors:
            tasks.append(self._get_elements_text_safe(page, selector))
        
        texts = await asyncio.gather(*tasks, return_exceptions=True)
        
        # åˆå¹¶æ‰€æœ‰æ–‡æœ¬
        combined_text = ""
        for text in texts:
            if isinstance(text, str):
                combined_text += text + " "
        
        # ä½¿ç”¨ä¼˜åŒ–çš„æ­£åˆ™åŒ¹é…å™¨æå–æ•°æ®
        extracted_data = self.regex_matcher.extract_data_parallel(combined_text, "é¢å¤–åŒºåŸŸ")

        # å¦‚æœæå–åˆ°å®Œæ•´ä½£é‡‘è´¹ç‡ï¼Œè¿›è¡Œè¿›ä¸€æ­¥è§£æ
        if 'å®Œæ•´ä½£é‡‘è´¹ç‡' in extracted_data:
            commission_rates = self.regex_matcher._parse_commission_rates(combined_text)
            extracted_data.update(commission_rates)
            # ç§»é™¤åŸå§‹çš„å®Œæ•´ä½£é‡‘è´¹ç‡å­—æ®µ
            del extracted_data['å®Œæ•´ä½£é‡‘è´¹ç‡']

        return extracted_data

    async def _get_elements_text_safe(self, page, selector: str) -> str:
        """å®‰å…¨åœ°è·å–å…ƒç´ æ–‡æœ¬å†…å®¹"""
        try:
            elements = await page.query_selector_all(selector)
            if elements:
                texts = []
                for element in elements[:5]:  # é™åˆ¶å¤„ç†æ•°é‡
                    try:
                        text = await element.text_content()
                        if text:
                            texts.append(text.strip())
                    except:
                        continue
                return " ".join(texts)
        except:
            pass
        return ""

    def _convert_to_product_data(self, data_dict: Dict[str, str]) -> ProductData:
        """å°†å­—å…¸æ•°æ®è½¬æ¢ä¸ºProductDataå¯¹è±¡"""
        product_data = ProductData()
        
        # æ˜ å°„å­—æ®µ
        field_mapping = {
            'sku': 'sku',
            'å“ç‰Œ': 'brand',
            'ä¸€çº§ç±»ç›®': 'category_1',
            'ä¸‰çº§ç±»ç›®': 'category_3',
            'è´§å·': 'product_code',
            'æœˆé”€é‡': 'monthly_sales',
            'æœˆé”€å”®é¢': 'monthly_revenue',
            'å•†å“ç‚¹å‡»é‡': 'click_count',
            'è´­ç‰©è½¦è½¬åŒ–ç‡': 'cart_conversion_rate',
            'å•†å“å±•ç¤ºæ€»é‡': 'display_count',
            'å±•ç¤ºè½¬åŒ–ç‡': 'display_conversion_rate',
            'æˆäº¤ç‡': 'transaction_rate',
            'ä½“ç§¯å®¹é‡': 'volume_capacity',
            'å¹³å‡ä»·æ ¼': 'average_price',
            'å–å®¶ç±»å‹': 'seller_type',
            'å–å®¶': 'seller_name',
            'å‘¨è½¬åŠ¨æ€': 'turnover_dynamics',
            'å•†å“åˆ›å»ºæ—¶é—´': 'creation_time',
            'é‡é‡': 'weight',
            'åº“å­˜': 'inventory'
        }
        
        # è®¾ç½®åŸºæœ¬å­—æ®µ
        for key, field in field_mapping.items():
            if key in data_dict:
                setattr(product_data, field, data_dict[key])
        
        # å¤„ç†ä½£é‡‘è´¹ç‡
        commission_rates = {}
        for key in ['ä½£é‡‘è´¹ç‡1500ä»¥ä¸‹', 'ä½£é‡‘è´¹ç‡1501-5000', 'ä½£é‡‘è´¹ç‡5001ä»¥ä¸Š']:
            if key in data_dict:
                commission_rates[key] = data_dict[key]
        if commission_rates:
            product_data.commission_rates = commission_rates
        
        # å¤„ç†å°ºå¯¸ä¿¡æ¯
        dimensions = {}
        for key in ['é•¿åº¦', 'å®½åº¦', 'é«˜åº¦']:
            if key in data_dict:
                dimensions[key] = data_dict[key]
        if dimensions:
            product_data.dimensions = dimensions
        
        return product_data

def test_commission_rate_parsing():
    """æµ‹è¯•ä½£é‡‘è´¹ç‡è§£æåŠŸèƒ½"""
    print("\n" + "="*80)
    print("ğŸ§ª æµ‹è¯•ä½£é‡‘è´¹ç‡è§£æåŠŸèƒ½")
    print("="*80)

    # æµ‹è¯•ç”¨æˆ·æä¾›çš„ä½£é‡‘è´¹ç‡å­—ç¬¦ä¸²
    test_text = """
    ä½£é‡‘è´¹ç‡:
    ä»·æ ¼â‰¤1500å¢å¸ƒ:12.0%
    ä»·æ ¼>1501å¢å¸ƒä»·æ ¼â‰¤5000å¢å¸ƒ:17.0%
    ä»·æ ¼>5001å¢å¸ƒ:17.0%
    """

    matcher = OptimizedRegexMatcher()

    # æµ‹è¯•å®Œæ•´ä½£é‡‘è´¹ç‡è§£æ
    commission_rates = matcher._parse_commission_rates(test_text)

    print("ğŸ“Š è§£æç»“æœ:")
    if commission_rates:
        for key, value in commission_rates.items():
            print(f"   âœ… {key}: {value}")

        # éªŒè¯é¢„æœŸç»“æœ
        expected = {
            'ä½£é‡‘è´¹ç‡1500ä»¥ä¸‹': '12.0%',
            'ä½£é‡‘è´¹ç‡1501-5000': '17.0%',
            'ä½£é‡‘è´¹ç‡5001ä»¥ä¸Š': '17.0%'
        }

        print("\nğŸ” éªŒè¯ç»“æœ:")
        all_correct = True
        for key, expected_value in expected.items():
            if key in commission_rates and commission_rates[key] == expected_value:
                print(f"   âœ… {key}: {commission_rates[key]} (æ­£ç¡®)")
            else:
                print(f"   âŒ {key}: æœŸæœ› {expected_value}, å®é™… {commission_rates.get(key, 'æœªæ‰¾åˆ°')}")
                all_correct = False

        if all_correct:
            print("\nğŸ‰ ä½£é‡‘è´¹ç‡è§£ææµ‹è¯•é€šè¿‡ï¼")
        else:
            print("\nâŒ ä½£é‡‘è´¹ç‡è§£ææµ‹è¯•å¤±è´¥ï¼")
    else:
        print("   âŒ æœªè§£æåˆ°ä»»ä½•ä½£é‡‘è´¹ç‡ä¿¡æ¯")

    return commission_rates

def test_new_string_formats():
    """æµ‹è¯•æ–°çš„å­—ç¬¦ä¸²æ ¼å¼è§£æåŠŸèƒ½"""
    print("\n" + "="*80)
    print("ğŸ§ª æµ‹è¯•æ–°å­—ç¬¦ä¸²æ ¼å¼è§£æåŠŸèƒ½")
    print("="*80)

    matcher = OptimizedRegexMatcher()

    # æµ‹è¯•ä½“ç§¯ä¿¡æ¯æ ¼å¼
    print("\nğŸ“¦ æµ‹è¯•ä½“ç§¯ä¿¡æ¯è§£æ:")
    volume_test_text = "ä½“ç§¯:27.5 å…¬å‡(é•¿xå®½xé«˜)"
    volume_result = matcher.extract_data_parallel(volume_test_text, "ä½“ç§¯æµ‹è¯•")

    expected_volume = "27.5 å…¬å‡(é•¿xå®½xé«˜)"
    if 'ä½“ç§¯å®¹é‡' in volume_result and volume_result['ä½“ç§¯å®¹é‡'] == expected_volume:
        print(f"   âœ… ä½“ç§¯è§£ææ­£ç¡®: {volume_result['ä½“ç§¯å®¹é‡']}")
        volume_test_passed = True
    else:
        print(f"   âŒ ä½“ç§¯è§£æå¤±è´¥: æœŸæœ› '{expected_volume}', å®é™… '{volume_result.get('ä½“ç§¯å®¹é‡', 'æœªæ‰¾åˆ°')}'")
        volume_test_passed = False

    # æµ‹è¯•å•†å“åˆ›å»ºæ—¶é—´æ ¼å¼
    print("\nğŸ“… æµ‹è¯•å•†å“åˆ›å»ºæ—¶é—´è§£æ:")
    time_test_text = "å•†å“åˆ›å»ºæ—¶é—´:07.07.2025 (å·²åˆ›å»º 106 å¤©)"
    time_result = matcher.extract_data_parallel(time_test_text, "æ—¶é—´æµ‹è¯•")

    expected_time = "07.07.2025 (å·²åˆ›å»º 106 å¤©)"
    if 'å•†å“åˆ›å»ºæ—¶é—´' in time_result and time_result['å•†å“åˆ›å»ºæ—¶é—´'] == expected_time:
        print(f"   âœ… å•†å“åˆ›å»ºæ—¶é—´è§£ææ­£ç¡®: {time_result['å•†å“åˆ›å»ºæ—¶é—´']}")
        time_test_passed = True
    else:
        print(f"   âŒ å•†å“åˆ›å»ºæ—¶é—´è§£æå¤±è´¥: æœŸæœ› '{expected_time}', å®é™… '{time_result.get('å•†å“åˆ›å»ºæ—¶é—´', 'æœªæ‰¾åˆ°')}'")
        time_test_passed = False

    # æµ‹è¯•æ··åˆæ–‡æœ¬
    print("\nğŸ”„ æµ‹è¯•æ··åˆæ–‡æœ¬è§£æ:")
    mixed_test_text = """
    å•†å“ä¿¡æ¯:
    ä½“ç§¯:27.5 å…¬å‡(é•¿xå®½xé«˜)
    å•†å“åˆ›å»ºæ—¶é—´:07.07.2025 (å·²åˆ›å»º 106 å¤©)
    å…¶ä»–ä¿¡æ¯...
    """
    mixed_result = matcher.extract_data_parallel(mixed_test_text, "æ··åˆæµ‹è¯•")

    mixed_test_passed = True
    if 'ä½“ç§¯å®¹é‡' in mixed_result and mixed_result['ä½“ç§¯å®¹é‡'] == expected_volume:
        print(f"   âœ… æ··åˆæ–‡æœ¬ä¸­ä½“ç§¯è§£ææ­£ç¡®: {mixed_result['ä½“ç§¯å®¹é‡']}")
    else:
        print(f"   âŒ æ··åˆæ–‡æœ¬ä¸­ä½“ç§¯è§£æå¤±è´¥")
        mixed_test_passed = False

    if 'å•†å“åˆ›å»ºæ—¶é—´' in mixed_result and mixed_result['å•†å“åˆ›å»ºæ—¶é—´'] == expected_time:
        print(f"   âœ… æ··åˆæ–‡æœ¬ä¸­å•†å“åˆ›å»ºæ—¶é—´è§£ææ­£ç¡®: {mixed_result['å•†å“åˆ›å»ºæ—¶é—´']}")
    else:
        print(f"   âŒ æ··åˆæ–‡æœ¬ä¸­å•†å“åˆ›å»ºæ—¶é—´è§£æå¤±è´¥")
        mixed_test_passed = False

    # æ€»ç»“æµ‹è¯•ç»“æœ
    print(f"\nğŸ“Š æµ‹è¯•æ€»ç»“:")
    print(f"   ä½“ç§¯ä¿¡æ¯è§£æ: {'âœ… é€šè¿‡' if volume_test_passed else 'âŒ å¤±è´¥'}")
    print(f"   å•†å“åˆ›å»ºæ—¶é—´è§£æ: {'âœ… é€šè¿‡' if time_test_passed else 'âŒ å¤±è´¥'}")
    print(f"   æ··åˆæ–‡æœ¬è§£æ: {'âœ… é€šè¿‡' if mixed_test_passed else 'âŒ å¤±è´¥'}")

    all_tests_passed = volume_test_passed and time_test_passed and mixed_test_passed
    if all_tests_passed:
        print("\nğŸ‰ æ‰€æœ‰æ–°å­—ç¬¦ä¸²æ ¼å¼è§£ææµ‹è¯•é€šè¿‡ï¼")
    else:
        print("\nâŒ éƒ¨åˆ†æ–°å­—ç¬¦ä¸²æ ¼å¼è§£ææµ‹è¯•å¤±è´¥ï¼")

    return all_tests_passed

def test_field_separation_fix():
    """æµ‹è¯•å­—æ®µåˆ†ç¦»ä¿®å¤åŠŸèƒ½"""
    print("\n" + "="*80)
    print("ğŸ§ª æµ‹è¯•å­—æ®µåˆ†ç¦»ä¿®å¤åŠŸèƒ½")
    print("="*80)

    matcher = OptimizedRegexMatcher()

    # æµ‹è¯•é—®é¢˜å­—ç¬¦ä¸²ï¼šæœ€é«˜ä»·å­—æ®µè¢«é”™è¯¯åˆå¹¶çš„æƒ…å†µ
    print("\nğŸ”§ æµ‹è¯•å­—æ®µåˆ†ç¦»ä¿®å¤:")
    problem_text = "æœ€é«˜ä»·:N/Aå•†å“ç‚¹å‡»é‡:253è´­ç‰©è½¦è½¬åŒ–ç‡:4"
    result = matcher.extract_data_parallel(problem_text, "å­—æ®µåˆ†ç¦»æµ‹è¯•")

    print(f"\nğŸ“Š è§£æç»“æœ:")
    for key, value in result.items():
        print(f"   {key}: {value}")

    # éªŒè¯æœŸæœ›ç»“æœ
    expected_results = {
        'æœ€é«˜ä»·': 'N/A',
        'å•†å“ç‚¹å‡»é‡': '253',
        'è´­ç‰©è½¦è½¬åŒ–ç‡': '4%'
    }

    print(f"\nğŸ” éªŒè¯ç»“æœ:")
    all_correct = True

    # æ£€æŸ¥æœ€é«˜ä»·
    if 'æœ€é«˜ä»·' in result and result['æœ€é«˜ä»·'] == 'N/A':
        print(f"   âœ… æœ€é«˜ä»·è§£ææ­£ç¡®: {result['æœ€é«˜ä»·']}")
    else:
        print(f"   âŒ æœ€é«˜ä»·è§£æå¤±è´¥: æœŸæœ› 'N/A', å®é™… '{result.get('æœ€é«˜ä»·', 'æœªæ‰¾åˆ°')}'")
        all_correct = False

    # æ£€æŸ¥å•†å“ç‚¹å‡»é‡
    if 'å•†å“ç‚¹å‡»é‡' in result and result['å•†å“ç‚¹å‡»é‡'] == '253':
        print(f"   âœ… å•†å“ç‚¹å‡»é‡è§£ææ­£ç¡®: {result['å•†å“ç‚¹å‡»é‡']}")
    else:
        print(f"   âŒ å•†å“ç‚¹å‡»é‡è§£æå¤±è´¥: æœŸæœ› '253', å®é™… '{result.get('å•†å“ç‚¹å‡»é‡', 'æœªæ‰¾åˆ°')}'")
        all_correct = False

    # æ£€æŸ¥è´­ç‰©è½¦è½¬åŒ–ç‡ (æ³¨æ„ï¼šå¯èƒ½åŒ¹é…åˆ° '4' è€Œä¸æ˜¯ '4%')
    if 'è´­ç‰©è½¦è½¬åŒ–ç‡' in result:
        if result['è´­ç‰©è½¦è½¬åŒ–ç‡'] in ['4', '4%']:
            print(f"   âœ… è´­ç‰©è½¦è½¬åŒ–ç‡è§£ææ­£ç¡®: {result['è´­ç‰©è½¦è½¬åŒ–ç‡']}")
        else:
            print(f"   âŒ è´­ç‰©è½¦è½¬åŒ–ç‡è§£æå¤±è´¥: æœŸæœ› '4' æˆ– '4%', å®é™… '{result['è´­ç‰©è½¦è½¬åŒ–ç‡']}'")
            all_correct = False
    else:
        print(f"   âŒ è´­ç‰©è½¦è½¬åŒ–ç‡è§£æå¤±è´¥: æœªæ‰¾åˆ°")
        all_correct = False

    # æµ‹è¯•æ›´å¤æ‚çš„æ··åˆå­—ç¬¦ä¸²
    print(f"\nğŸ”„ æµ‹è¯•å¤æ‚æ··åˆå­—ç¬¦ä¸²:")
    complex_text = "æœ€ä½ä»·:1500â‚½æœ€é«˜ä»·:N/Aå•†å“ç‚¹å‡»é‡:253è´­ç‰©è½¦è½¬åŒ–ç‡:4%å±•ç¤ºè½¬åŒ–ç‡:2.5%"
    complex_result = matcher.extract_data_parallel(complex_text, "å¤æ‚æ··åˆæµ‹è¯•")

    print(f"\nğŸ“Š å¤æ‚å­—ç¬¦ä¸²è§£æç»“æœ:")
    for key, value in complex_result.items():
        print(f"   {key}: {value}")

    # éªŒè¯å¤æ‚å­—ç¬¦ä¸²çš„å…³é”®å­—æ®µ
    complex_checks = [
        ('æœ€ä½ä»·', '1500â‚½'),
        ('æœ€é«˜ä»·', 'N/A'),
        ('å•†å“ç‚¹å‡»é‡', '253'),
        ('è´­ç‰©è½¦è½¬åŒ–ç‡', '4%'),
        ('å±•ç¤ºè½¬åŒ–ç‡', '2.5%')
    ]

    complex_correct = True
    for field, expected in complex_checks:
        if field in complex_result and complex_result[field] == expected:
            print(f"   âœ… {field}è§£ææ­£ç¡®: {complex_result[field]}")
        else:
            print(f"   âŒ {field}è§£æå¤±è´¥: æœŸæœ› '{expected}', å®é™… '{complex_result.get(field, 'æœªæ‰¾åˆ°')}'")
            complex_correct = False

    # æ€»ç»“æµ‹è¯•ç»“æœ
    print(f"\nğŸ“Š æµ‹è¯•æ€»ç»“:")
    print(f"   åŸºç¡€å­—æ®µåˆ†ç¦»: {'âœ… é€šè¿‡' if all_correct else 'âŒ å¤±è´¥'}")
    print(f"   å¤æ‚æ··åˆå­—ç¬¦ä¸²: {'âœ… é€šè¿‡' if complex_correct else 'âŒ å¤±è´¥'}")

    overall_success = all_correct and complex_correct
    if overall_success:
        print("\nğŸ‰ å­—æ®µåˆ†ç¦»ä¿®å¤æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
    else:
        print("\nâŒ éƒ¨åˆ†å­—æ®µåˆ†ç¦»æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–æ­£åˆ™è¡¨è¾¾å¼ï¼")

    return overall_success

async def main():
    """ä¸»å‡½æ•°"""
    # é¦–å…ˆæµ‹è¯•ä½£é‡‘è´¹ç‡è§£æåŠŸèƒ½
    test_commission_rate_parsing()

    # æµ‹è¯•æ–°çš„å­—ç¬¦ä¸²æ ¼å¼è§£æåŠŸèƒ½
    test_new_string_formats()

    # æµ‹è¯•å­—æ®µåˆ†ç¦»ä¿®å¤åŠŸèƒ½
    test_field_separation_fix()

    # OZONå•†å“é¡µé¢URL
    url = "https://www.ozon.ru/product/kovrik-v-bagazhnik-iskusstvennaya-kozha-1-sht-2423301080/"

    print("\nğŸš€ å¯åŠ¨ä¼˜åŒ–ç‰ˆOZONé¡µé¢åˆ†æå™¨")
    analyzer = OptimizedOzonPageAnalyzer(debug_mode=True)
    result = await analyzer.analyze_product_page(url)

    if result and result.success:
        print("\n" + "="*80)
        print("ğŸ‰ åˆ†æå®Œæˆ - JSONæ ¼å¼ç»“æœ")
        print("="*80)
        
        # è¾“å‡ºJSONæ ¼å¼ç»“æœ
        json_result = result.to_json()
        print(json_result)
        
        # æ€§èƒ½ç»Ÿè®¡
        print("\nğŸ“Š æ€§èƒ½ç»Ÿè®¡:")
        for key, value in result.performance_stats.items():
            print(f"   {key}: {value:.3f}ç§’")
        
        # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
        with open('tests/resources/ozon_analysis_result.json', 'w', encoding='utf-8') as f:
            f.write(json_result)
        print("\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: tests/resources/ozon_analysis_result.json")
        
    else:
        print("âŒ åˆ†æå¤±è´¥")

if __name__ == "__main__":
    asyncio.run(main())