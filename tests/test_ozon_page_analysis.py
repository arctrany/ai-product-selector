#!/usr/bin/env python3
"""
OZONå•†å“é¡µé¢ç»“æ„åˆ†ææµ‹è¯•ç¨‹åº
ç”¨äºåˆ†æå…·ä½“å•†å“é¡µé¢çš„DOMç»“æ„ï¼Œæ‰¾åˆ°å•†å“è¯¦æƒ…ä¿¡æ¯çš„å‡†ç¡®ä½ç½®
"""

import asyncio
import sys
import os
import json
import re

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src', 'playweight'))

from playweight.logger_config import get_logger
from playweight.engine.browser_service import BrowserService

class OzonPageAnalyzer:
    """OZONé¡µé¢åˆ†æå™¨"""

    def __init__(self, debug_mode=True):
        self.debug_mode = debug_mode
        self.logger = get_logger(debug_mode)
        self.browser_service = None

    async def analyze_product_page(self, url: str):
        """åˆ†æå•†å“é¡µé¢ç»“æ„"""
        try:
            # åˆå§‹åŒ–BrowserService
            print("ğŸš€ åˆå§‹åŒ–æµè§ˆå™¨æœåŠ¡...")
            self.browser_service = BrowserService(debug_port=9222, headless=False)

            # å¯åŠ¨æµè§ˆå™¨æœåŠ¡
            print("ğŸŒ å¯åŠ¨æµè§ˆå™¨...")
            if not await self.browser_service.init_browser():
                print("âŒ æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥")
                return

            # è·å–é¡µé¢å¯¹è±¡
            page = await self.browser_service.get_page()
            if not page:
                print("âŒ æ— æ³•è·å–é¡µé¢å¯¹è±¡")
                return

            print(f"ğŸŒ æ­£åœ¨è®¿é—®é¡µé¢: {url}")
            # ä½¿ç”¨æ›´å®½æ¾çš„åŠ è½½ç­–ç•¥å’Œæ›´é•¿çš„è¶…æ—¶æ—¶é—´
            try:
                await page.goto(url, wait_until='domcontentloaded', timeout=60000)
                print("âœ… é¡µé¢DOMåŠ è½½å®Œæˆ")
            except Exception as e:
                print(f"âš ï¸ é¡µé¢åŠ è½½è¶…æ—¶ï¼Œå°è¯•ç»§ç»­åˆ†æ: {str(e)}")
                # å³ä½¿è¶…æ—¶ä¹Ÿå°è¯•åˆ†æå½“å‰é¡µé¢çŠ¶æ€

            # ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½å’ŒJavaScriptæ‰§è¡Œ
            await asyncio.sleep(5)
            print("âœ… ç­‰å¾…é¡µé¢ç¨³å®šå®Œæˆ")

            # è·å–é¡µé¢æ ‡é¢˜
            title = await page.title()
            print(f"ğŸ“„ é¡µé¢æ ‡é¢˜: {title}")

            # åˆ†æé¡µé¢ç»“æ„
            await self._analyze_page_structure(page)

            # æŸ¥æ‰¾åŒ…å«å•†å“ä¿¡æ¯çš„åŒºåŸŸ
            await self._find_product_info_areas(page)

            # æµ‹è¯•ä¸åŒçš„é€‰æ‹©å™¨
            await self._test_selectors(page)

            # æå–æ‰€æœ‰å¯èƒ½çš„å•†å“ä¿¡æ¯
            result = await self._extract_all_product_data(page)
            return result

        except Exception as e:
            print(f"âŒ é¡µé¢åˆ†æå¤±è´¥: {str(e)}")
        finally:
            if self.browser_service:
                print("ğŸ”„ å…³é—­æµè§ˆå™¨æœåŠ¡...")
                await self.browser_service.close_browser()
    
    async def _analyze_page_structure(self, page):
        """åˆ†æé¡µé¢åŸºæœ¬ç»“æ„"""
        print("\n" + "="*80)
        print("ğŸ“Š é¡µé¢ç»“æ„åˆ†æ")
        print("="*80)
        
        # æŸ¥æ‰¾ä¸»è¦å®¹å™¨
        main_containers = [
            'main', 'div[id*="main"]', 'div[class*="main"]',
            'div[id*="content"]', 'div[class*="content"]',
            'div[id*="product"]', 'div[class*="product"]',
            'div[id*="layout"]', 'div[class*="layout"]'
        ]
        
        for selector in main_containers:
            try:
                elements = await page.query_selector_all(selector)
                if elements:
                    print(f"âœ… æ‰¾åˆ°å®¹å™¨ {selector}: {len(elements)} ä¸ª")
                    for i, elem in enumerate(elements[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ª
                        elem_id = await elem.get_attribute('id')
                        elem_class = await elem.get_attribute('class')
                        print(f"   [{i}] id='{elem_id}' class='{elem_class}'")
            except:
                continue
    
    async def _find_product_info_areas(self, page):
        """æŸ¥æ‰¾åŒ…å«å•†å“ä¿¡æ¯çš„åŒºåŸŸ"""
        print("\n" + "="*80)
        print("ğŸ” æŸ¥æ‰¾å•†å“ä¿¡æ¯åŒºåŸŸ")
        print("="*80)
        
        # æŸ¥æ‰¾åŒ…å«SKUã€é”€é‡ç­‰å…³é”®è¯çš„å…ƒç´ 
        keywords = ['SKU', 'sku', 'é”€é‡', 'æ¯›åˆ©ç‡', 'æ›å…‰é‡', 'åŠ è´­ç‡', 'é”€å”®é¢', 'é€€è´§', 'å“ç‰Œ', 'å–å®¶', 'é‡é‡', 'ä½“ç§¯']
        
        for keyword in keywords:
            try:
                # ä½¿ç”¨XPathæŸ¥æ‰¾åŒ…å«å…³é”®è¯çš„å…ƒç´ 
                xpath = f"//*[contains(text(), '{keyword}')]"
                elements = await page.query_selector_all(f"xpath={xpath}")
                
                if elements:
                    print(f"\nğŸ¯ æ‰¾åˆ°åŒ…å« '{keyword}' çš„å…ƒç´ : {len(elements)} ä¸ª")
                    for i, elem in enumerate(elements[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ª
                        try:
                            text = await elem.text_content()
                            tag = await elem.evaluate('el => el.tagName.toLowerCase()')
                            parent_text = await elem.evaluate('el => el.parentElement ? el.parentElement.textContent.slice(0, 100) : ""')
                            print(f"   [{i}] <{tag}>: {text[:50]}...")
                            print(f"       çˆ¶å…ƒç´ æ–‡æœ¬: {parent_text[:80]}...")
                        except:
                            continue
            except:
                continue
    
    async def _test_selectors(self, page):
        """æµ‹è¯•ä¸åŒçš„é€‰æ‹©å™¨"""
        print("\n" + "="*80)
        print("ğŸ§ª æµ‹è¯•é€‰æ‹©å™¨")
        print("="*80)
        
        # æµ‹è¯•åŸæœ‰çš„XPath
        original_xpaths = [
            '//*[@id="layoutPage"]/div[1]/div[3]/div[3]/div[2]/div/div/div[2]/div[4]/div/div',
            '//*[@id="product-preview-info"]'
        ]
        
        for xpath in original_xpaths:
            try:
                element = await page.query_selector(f"xpath={xpath}")
                if element:
                    text = await element.text_content()
                    print(f"âœ… åŸXPathæœ‰æ•ˆ: {xpath}")
                    print(f"   å†…å®¹: {text[:100]}...")
                else:
                    print(f"âŒ åŸXPathæ— æ•ˆ: {xpath}")
            except Exception as e:
                print(f"âŒ åŸXPathé”™è¯¯: {xpath} - {str(e)}")
        
        # æµ‹è¯•å¸¸è§çš„å•†å“ä¿¡æ¯é€‰æ‹©å™¨
        test_selectors = [
            'div[data-widget="webProductHeading"]',
            'div[data-widget="webPrice"]',
            'div[data-widget="webSale"]',
            'div[data-widget="webProductMainWidget"]',
            'div[data-widget="webCharacteristics"]',
            'div[data-widget="webProductSummary"]',
            '[data-widget*="product"]',
            '[data-widget*="Price"]',
            '[data-widget*="info"]',
            'div[class*="product"]',
            'div[class*="info"]',
            'div[class*="detail"]',
            'div[class*="summary"]'
        ]
        
        for selector in test_selectors:
            try:
                elements = await page.query_selector_all(selector)
                if elements:
                    print(f"âœ… é€‰æ‹©å™¨æœ‰æ•ˆ: {selector} ({len(elements)} ä¸ªå…ƒç´ )")
                    # æ˜¾ç¤ºç¬¬ä¸€ä¸ªå…ƒç´ çš„å†…å®¹
                    if elements:
                        text = await elements[0].text_content()
                        print(f"   å†…å®¹é¢„è§ˆ: {text[:100]}...")
            except:
                continue
    
    async def _extract_all_product_data(self, page):
        """æå–æ‰€æœ‰å¯èƒ½çš„å•†å“æ•°æ®"""
        print("\n" + "="*80)
        print("ğŸ“¦ æå–å•†å“æ•°æ®")
        print("="*80)
        
        # æ–¹æ³•1: é€šè¿‡data-widgetå±æ€§æŸ¥æ‰¾
        await self._extract_by_data_widget(page)
        
        # æ–¹æ³•2: é€šè¿‡æ–‡æœ¬å†…å®¹æŸ¥æ‰¾
        await self._extract_by_text_content(page)
        
        # æ–¹æ³•3: é€šè¿‡å¸¸è§çš„CSSç±»æŸ¥æ‰¾
        await self._extract_by_css_classes(page)

        # æ–°å¢ï¼šå®é™…æå–å•†å“å­—å…¸
        print("\n" + "="*80)
        print("ğŸ¯ å®é™…å•†å“æ•°æ®æå–")
        print("="*80)

        # æå–ç”µé¹åŒºåŸŸæ•°æ®
        dianpeng_data = await self._extract_dianpeng_product_dict(page)

        # æå–seefaråŒºåŸŸæ•°æ®
        seefar_data = await self._extract_seefar_product_dict(page)

        # æ ¼å¼åŒ–è¾“å‡º
        await self._format_and_display_results(dianpeng_data, seefar_data)

        return {
            'dianpeng_area': dianpeng_data,
            'seefar_area': seefar_data
        }

    async def _extract_by_data_widget(self, page):
        """é€šè¿‡data-widgetå±æ€§æå–æ•°æ®"""
        print("\nğŸ¯ æ–¹æ³•1: é€šè¿‡data-widgetå±æ€§æå–")
        
        try:
            # è·å–æ‰€æœ‰å¸¦data-widgetå±æ€§çš„å…ƒç´ 
            elements = await page.query_selector_all('[data-widget]')
            print(f"æ‰¾åˆ° {len(elements)} ä¸ªdata-widgetå…ƒç´ ")
            
            widget_data = {}
            for elem in elements:
                try:
                    widget_name = await elem.get_attribute('data-widget')
                    text_content = await elem.text_content()
                    if text_content and text_content.strip():
                        widget_data[widget_name] = text_content.strip()[:200]  # é™åˆ¶é•¿åº¦
                except:
                    continue
            
            # æ˜¾ç¤ºæ‰¾åˆ°çš„widgetæ•°æ®
            for widget_name, content in widget_data.items():
                if any(keyword in content.lower() for keyword in ['sku', 'é”€é‡', 'æ¯›åˆ©', 'æ›å…‰', 'åŠ è´­', 'å“ç‰Œ', 'å–å®¶', 'é‡é‡']):
                    print(f"   ğŸ“Š {widget_name}: {content}")
                    
        except Exception as e:
            print(f"âŒ data-widgetæå–å¤±è´¥: {str(e)}")
    
    async def _extract_by_text_content(self, page):
        """é€šè¿‡æ–‡æœ¬å†…å®¹æå–æ•°æ®"""
        print("\nğŸ¯ æ–¹æ³•2: é€šè¿‡æ–‡æœ¬å†…å®¹æå–")
        
        # æŸ¥æ‰¾åŒ…å«æ•°å­—å’Œç‰¹å®šæ¨¡å¼çš„æ–‡æœ¬
        patterns = [
            r'SKU[:\s]*(\d+)',
            r'é”€é‡[:\s]*(\d+)',
            r'æ¯›åˆ©ç‡[:\s]*(\d+%)',
            r'æ›å…‰é‡[:\s]*(\d+)',
            r'åŠ è´­ç‡[:\s]*(\d+\.?\d*%)',
            r'é‡é‡[:\s]*(\d+\s*g)',
            r'ä½“ç§¯[:\s]*(\d+Ã—\d+Ã—\d+mm)',
            r'å“ç‰Œ[:\s]*([^\n\r]+)',
            r'å–å®¶[:\s]*([^\n\r]+)'
        ]
        
        try:
            page_text = await page.evaluate('() => document.body.textContent')
            
            import re
            for pattern in patterns:
                matches = re.findall(pattern, page_text, re.IGNORECASE)
                if matches:
                    print(f"   ğŸ“Š æ‰¾åˆ°æ¨¡å¼ {pattern}: {matches[:3]}")  # åªæ˜¾ç¤ºå‰3ä¸ªåŒ¹é…
                    
        except Exception as e:
            print(f"âŒ æ–‡æœ¬å†…å®¹æå–å¤±è´¥: {str(e)}")
    
    async def _extract_by_css_classes(self, page):
        """é€šè¿‡CSSç±»æå–æ•°æ®"""
        print("\nğŸ¯ æ–¹æ³•3: é€šè¿‡CSSç±»æå–")
        
        # å¸¸è§çš„å•†å“ä¿¡æ¯CSSç±»æ¨¡å¼
        css_patterns = [
            'div[class*="sku"]',
            'div[class*="price"]',
            'div[class*="sale"]',
            'div[class*="brand"]',
            'div[class*="seller"]',
            'div[class*="weight"]',
            'div[class*="size"]',
            'div[class*="dimension"]',
            'span[class*="value"]',
            'span[class*="number"]',
            'div[class*="stat"]',
            'div[class*="metric"]'
        ]
        
        for pattern in css_patterns:
            try:
                elements = await page.query_selector_all(pattern)
                if elements:
                    print(f"   ğŸ“Š CSSæ¨¡å¼ {pattern}: {len(elements)} ä¸ªå…ƒç´ ")
                    for i, elem in enumerate(elements[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ª
                        text = await elem.text_content()
                        if text and text.strip():
                            print(f"      [{i}]: {text.strip()[:50]}")
            except:
                continue

    async def _extract_dianpeng_product_dict(self, page):
        """æå–ç”µé¹åŒºåŸŸçš„å•†å“å­—å…¸"""
        print("\nğŸ¯ æå–ç”µé¹åŒºåŸŸæ•°æ®...")

        product_dict = {}

        try:
            # æ–¹æ³•1: ä½¿ç”¨åŸå§‹ç”µé¹åŒºåŸŸXPath
            dianpeng_xpath = '//*[@id="layoutPage"]/div[1]/div[3]/div[3]/div[2]/div/div/div[2]/div[4]/div/div'
            dianpeng_element = await page.query_selector(f"xpath={dianpeng_xpath}")

            if dianpeng_element:
                print("âœ… æ‰¾åˆ°ç”µé¹åŒºåŸŸå…ƒç´ ")
                text_content = await dianpeng_element.text_content()
                if text_content:
                    print(f"ğŸ“ ç”µé¹åŒºåŸŸåŸå§‹æ–‡æœ¬: {text_content[:200]}...")
                    product_dict.update(self._parse_kv_pairs_from_text(text_content, "ç”µé¹åŒºåŸŸ"))

            # æ–¹æ³•2: é€šè¿‡classåŒ…å«"product"çš„å…ƒç´ æŸ¥æ‰¾ç”µé¹ä¿¡æ¯
            product_elements = await page.query_selector_all('div[class*="product"]')
            for i, element in enumerate(product_elements):
                try:
                    text_content = await element.text_content()
                    if text_content and "ç”µé¹ä¿¡æ¯" in text_content:
                        print(f"âœ… æ‰¾åˆ°ç”µé¹ä¿¡æ¯å…ƒç´ [{i}]")
                        print(f"ğŸ“ ç”µé¹ä¿¡æ¯æ–‡æœ¬: {text_content[:300]}...")
                        product_dict.update(self._parse_kv_pairs_from_text(text_content, f"ç”µé¹ä¿¡æ¯å…ƒç´ [{i}]"))
                except:
                    continue

            # æ–¹æ³•3: é€šè¿‡åŒ…å«å…³é”®å•†å“ä¿¡æ¯çš„å…ƒç´ æå–
            info_selectors = [
                'div[class*="info"]',
                'span:has-text("sku")',
                'span:has-text("å“ç‰Œ")',
                'span:has-text("é”€é‡")',
                'span:has-text("é”€å”®é¢")'
            ]

            for selector in info_selectors:
                try:
                    elements = await page.query_selector_all(selector)
                    for element in elements:
                        text_content = await element.text_content()
                        if text_content and any(keyword in text_content for keyword in ['sku', 'å“ç‰Œ', 'é”€é‡', 'é”€å”®é¢', 'ç±»ç›®', 'è´§å·']):
                            print(f"ğŸ“ ä¿¡æ¯å…ƒç´ æ–‡æœ¬: {text_content[:100]}...")
                            product_dict.update(self._parse_kv_pairs_from_text(text_content, f"ä¿¡æ¯å…ƒç´ -{selector}"))
                except:
                    continue

        except Exception as e:
            print(f"âŒ ç”µé¹åŒºåŸŸæå–å¤±è´¥: {str(e)}")

        print(f"ğŸ“Š ç”µé¹åŒºåŸŸæå–åˆ° {len(product_dict)} ä¸ªå­—æ®µ")
        return product_dict

    async def _extract_seefar_product_dict(self, page):
        """æå–seefaråŒºåŸŸçš„å•†å“å­—å…¸"""
        print("\nğŸ¯ æå–seefaråŒºåŸŸæ•°æ®...")

        product_dict = {}

        try:
            # æ–¹æ³•1: å°è¯•åŸå§‹çš„seefar XPath
            seefar_xpath = '//*[@id="product-preview-info"]'
            seefar_element = await page.query_selector(f"xpath={seefar_xpath}")

            if seefar_element:
                print("âœ… æ‰¾åˆ°seefaråŒºåŸŸå…ƒç´ ")
                text_content = await seefar_element.text_content()
                if text_content:
                    print(f"ğŸ“ seefaråŒºåŸŸåŸå§‹æ–‡æœ¬: {text_content[:200]}...")
                    product_dict.update(self._parse_kv_pairs_from_text(text_content, "seefaråŒºåŸŸ"))

            # æ–¹æ³•2: ä½¿ç”¨data-widgeté€‰æ‹©å™¨
            widget_selectors = [
                'div[data-widget="webProductHeading"]',
                'div[data-widget="webPrice"]',
                'div[data-widget="webSale"]',
                'div[data-widget="webProductMainWidget"]',
                'div[data-widget="webCharacteristics"]',
                'div[data-widget="webProductSummary"]',
                'div[data-widget="webProductDetails"]'
            ]

            for selector in widget_selectors:
                try:
                    elements = await page.query_selector_all(selector)
                    if elements:
                        print(f"âœ… æ‰¾åˆ° {selector} å…ƒç´ : {len(elements)} ä¸ª")
                        for element in elements:
                            text_content = await element.text_content()
                            if text_content:
                                print(f"ğŸ“ {selector} æ–‡æœ¬: {text_content[:150]}...")
                                element_dict = self._parse_kv_pairs_from_text(text_content, f"seefar-{selector}")
                                product_dict.update(element_dict)
                except:
                    continue

            # æ–¹æ³•3: é€šè¿‡åŒ…å«seefarå…³é”®ä¿¡æ¯çš„å…ƒç´ æå–
            seefar_selectors = [
                'span:has-text("å–å®¶")',
                'span:has-text("é…é€")',
                'span:has-text("å˜ä½“æ•°")',
                'span:has-text("è·Ÿå–æ•°")',
                'span:has-text("åº“å­˜")',
                'span:has-text("ä¸Šæ¶æ—¶é—´")',
                'div:has-text("ZONFANT")',
                'div:has-text("RFBS")'
            ]

            for selector in seefar_selectors:
                try:
                    elements = await page.query_selector_all(selector)
                    for element in elements:
                        text_content = await element.text_content()
                        if text_content and any(keyword in text_content for keyword in ['å–å®¶', 'é…é€', 'å˜ä½“æ•°', 'è·Ÿå–æ•°', 'åº“å­˜', 'ä¸Šæ¶æ—¶é—´', 'ZONFANT', 'RFBS']):
                            print(f"ğŸ“ seefarå…³é”®ä¿¡æ¯: {text_content[:100]}...")
                            product_dict.update(self._parse_kv_pairs_from_text(text_content, f"seefarå…³é”®ä¿¡æ¯-{selector}"))
                except:
                    continue

            # æ–¹æ³•4: é€šè¿‡å•†å“é¡µé¢çš„ä¸»è¦ä¿¡æ¯åŒºåŸŸæå–
            main_selectors = [
                'main',
                'div[class*="main"]',
                'div[id*="main"]',
                'div[class*="product"]'
            ]

            for selector in main_selectors:
                try:
                    elements = await page.query_selector_all(selector)
                    for element in elements:
                        # åªå¤„ç†åŒ…å«seefarç›¸å…³ä¿¡æ¯çš„å…ƒç´ 
                        text_content = await element.text_content()
                        if text_content and any(keyword in text_content for keyword in ['å–å®¶', 'é…é€', 'ZONFANT', 'RFBS']):
                            print(f"ğŸ“ ä¸»è¦åŒºåŸŸseefarä¿¡æ¯: {text_content[:100]}...")
                            product_dict.update(self._parse_kv_pairs_from_text(text_content, f"ä¸»è¦åŒºåŸŸ-{selector}"))
                            break  # æ‰¾åˆ°ä¸€ä¸ªå°±å¤Ÿäº†ï¼Œé¿å…é‡å¤
                except:
                    continue

        except Exception as e:
            print(f"âŒ seefaråŒºåŸŸæå–å¤±è´¥: {str(e)}")

        print(f"ğŸ“Š seefaråŒºåŸŸæå–åˆ° {len(product_dict)} ä¸ªå­—æ®µ")
        return product_dict

    def _parse_kv_pairs_from_text(self, text, source_name):
        """ä»æ–‡æœ¬ä¸­è§£æK:Vå¯¹"""
        kv_dict = {}

        if not text or not text.strip():
            return kv_dict

        print(f"ğŸ” è§£ææ–‡æœ¬æ¥æº: {source_name}")
        print(f"ğŸ“ åŸå§‹æ–‡æœ¬ç‰‡æ®µ: {text[:200]}...")

        # å®šä¹‰åŒ¹é…æ¨¡å¼ - æ›´ç²¾ç¡®çš„åŒ¹é…å®é™…é¡µé¢æ ¼å¼
        patterns = [
            # SKUæ¨¡å¼ - åŒ¹é…æ•°å­—ID
            (r'sku[:\s]*(\d+)', 'sku'),
            (r'SKU[:\s]*(\d+)', 'sku'),
            (r'(?:^|\s)(\d{10,})(?:\s|$)', 'sku'),  # åŒ¹é…ç‹¬ç«‹çš„é•¿æ•°å­—ä½œä¸ºSKU

            # é”€é‡æ¨¡å¼ - æ›´ç²¾ç¡®åŒ¹é…
            (r'æœˆé”€é‡[:\s]*(\d+)', 'æœˆé”€é‡'),
            (r'é”€é‡[:\s]*(\d+)', 'é”€é‡'),
            (r'è¿‘30å¤©é”€é‡[:\s]*(\d+)', 'è¿‘30å¤©é”€é‡'),
            (r'Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ½Ğ¾[:\s]*(\d+)', 'é”€é‡'),
            (r'æ€»é”€é‡[:\s]*(\d+)', 'é”€é‡'),

            # å“ç‰Œæ¨¡å¼ - åŒ¹é…å®é™…æ ¼å¼ï¼ŒåŒ…æ‹¬"Ğ±ĞµĞ· Ğ±Ñ€ĞµĞ½Ğ´Ğ°"
            (r'å“ç‰Œ[:\s]*([^ä¸€ä¸‰è´§ä¿ƒæœˆè¢«å•†è´­å±•æˆä½£ä½“å¹³å–å‘¨é•¿å®½é«˜é‡\n\r]{1,30})', 'å“ç‰Œ'),
            (r'Ğ±Ñ€ĞµĞ½Ğ´[:\s]*([A-Za-z0-9\u0400-\u04ff\s]{1,30})', 'å“ç‰Œ'),
            (r'Ğ‘Ñ€ĞµĞ½Ğ´[:\s]*([A-Za-z0-9\u0400-\u04ff\s]{1,30})', 'å“ç‰Œ'),
            (r'(Ğ±ĞµĞ·\s+Ğ±Ñ€ĞµĞ½Ğ´Ğ°)', 'å“ç‰Œ'),  # åŒ¹é…"Ğ±ĞµĞ· Ğ±Ñ€ĞµĞ½Ğ´Ğ°"

            # ç±»ç›®æ¨¡å¼ - åŒ¹é…ä¿„æ–‡ç±»ç›®
            (r'ä¸€çº§ç±»ç›®[:\s]*([^ä¸‰è´§ä¿ƒæœˆè¢«å•†è´­å±•æˆä½£ä½“å¹³å–å‘¨é•¿å®½é«˜é‡\n\r]{1,30})', 'ä¸€çº§ç±»ç›®'),
            (r'ä¸‰çº§ç±»ç›®[:\s]*([^è´§ä¿ƒæœˆè¢«å•†è´­å±•æˆä½£ä½“å¹³å–å‘¨é•¿å®½é«˜é‡\n\r]{1,30})', 'ä¸‰çº§ç±»ç›®'),

            # è´§å·æ¨¡å¼
            (r'è´§å·[:\s]*([A-Za-z0-9\-]{1,30})', 'è´§å·'),

            # ä¿ƒé”€æ´»åŠ¨æ¨¡å¼
            (r'ä¿ƒé”€æ´»åŠ¨[:\s]*([^æœˆè¢«å•†è´­å±•æˆä½£ä½“å¹³å–å‘¨é•¿å®½é«˜é‡\n\r]{1,30})', 'ä¿ƒé”€æ´»åŠ¨'),
            (r'(\d+å¤©å‚ä¸\d+å¤©)', 'ä¿ƒé”€æ´»åŠ¨'),  # åŒ¹é…"28å¤©å‚ä¸28å¤©"æ ¼å¼

            # é”€å”®é¢æ¨¡å¼ - åŒ¹é…å¸¦â‚½ç¬¦å·çš„é‡‘é¢
            (r'æœˆé”€å”®é¢[:\s]*([0-9.,]+â‚½)', 'æœˆé”€å”®é¢'),
            (r'è¿‘30å¤©é”€å”®é¢[:\s]*([0-9\s.,]+â‚½)', 'è¿‘30å¤©é”€å”®é¢'),
            (r'([0-9]+\.[0-9]+â‚½)', 'æœˆé”€å”®é¢'),  # åŒ¹é…"24711.102â‚½"æ ¼å¼

            # é‡é‡æ¨¡å¼ - ç²¾ç¡®åŒ¹é…æ•°å­—+å•ä½
            (r'é‡é‡[:\s]*(\d+g)', 'é‡é‡'),
            (r'Ğ²ĞµÑ[:\s]*(\d+\s*[gĞºĞ³])', 'é‡é‡'),
            (r'Ğ’ĞµÑ[:\s]*(\d+\s*[gĞºĞ³])', 'é‡é‡'),
            (r'(\d+g)(?=\s|$|Ğš)', 'é‡é‡'),  # åŒ¹é… "2500g" æ ¼å¼

            # å°ºå¯¸æ¨¡å¼ - åŒ¹é…mmå•ä½
            (r'é•¿åº¦[:\s]*(\d+mm)', 'é•¿åº¦'),
            (r'å®½åº¦[:\s]*(\d+mm)', 'å®½åº¦'),
            (r'é«˜åº¦[:\s]*(\d+mm)', 'é«˜åº¦'),

            # å„ç§è½¬åŒ–ç‡æ¨¡å¼ - åŒ¹é…ç™¾åˆ†æ¯”
            (r'æ¯›åˆ©ç‡[:\s]*(\d+\.?\d*%)', 'æ¯›åˆ©ç‡'),
            (r'è´­ç‰©è½¦è½¬åŒ–ç‡[:\s]*(\d+\.?\d*%)', 'è´­ç‰©è½¦è½¬åŒ–ç‡'),
            (r'å±•ç¤ºè½¬åŒ–ç‡[:\s]*(\d+\.?\d*%)', 'å±•ç¤ºè½¬åŒ–ç‡'),
            (r'æˆäº¤ç‡[:\s]*(\d+\.?\d*%[^)]*(?:\([^)]*\))?)', 'æˆäº¤ç‡'),  # åŒ…å«æ‹¬å·è¯´æ˜
            (r'å‘¨è½¬åŠ¨æ€[:\s]*(\d+\.?\d*%)', 'å‘¨è½¬åŠ¨æ€'),

            # æ›å…‰é‡å’Œç‚¹å‡»é‡æ¨¡å¼
            (r'æ›å…‰é‡[:\s]*(\d+)', 'æ›å…‰é‡'),
            (r'å•†å“ç‚¹å‡»é‡[:\s]*(\d+)', 'å•†å“ç‚¹å‡»é‡'),
            (r'å•†å“å±•ç¤ºæ€»é‡[:\s]*(\d+)', 'å•†å“å±•ç¤ºæ€»é‡'),

            # åŠ è´­ç‡æ¨¡å¼
            (r'åŠ è´­ç‡[:\s]*(\d+\.?\d*%)', 'åŠ è´­ç‡'),

            # å–å®¶ç›¸å…³æ¨¡å¼ - åŒ¹é…å®é™…æ ¼å¼
            (r'å–å®¶[:\s]*([A-Za-z0-9\u4e00-\u9fff\u0400-\u04ff]{1,30})', 'å–å®¶'),
            (r'å–å®¶ç±»å‹[:\s]*([A-Za-z0-9]{1,10})', 'å–å®¶ç±»å‹'),
            (r'é…é€[:\s]*([A-Za-z0-9]{1,10})', 'é…é€'),
            (r'Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ²ĞµÑ†[:\s]*([A-Za-z0-9\u0400-\u04ff]{1,30})', 'å–å®¶'),
            # ç‰¹æ®ŠåŒ¹é…ZONFANTå’ŒRFBS
            (r'(ZONFANT)', 'å–å®¶'),
            (r'(RFBS)', 'é…é€'),

            # å˜ä½“æ•°å’Œè·Ÿå–æ•°æ¨¡å¼
            (r'å˜ä½“æ•°[:\s]*(\d+)', 'å˜ä½“æ•°'),
            (r'è·Ÿå–æ•°[:\s]*(\d+)', 'è·Ÿå–æ•°'),
            (r'è¢«è·Ÿæ•°é‡[:\s]*([^\så•†è´­å±•æˆä½£ä½“å¹³å–å‘¨é•¿å®½é«˜é‡]{1,20})', 'è¢«è·Ÿæ•°é‡'),

            # åº“å­˜æ¨¡å¼
            (r'åº“å­˜[:\s]*([^\så•†è´­å±•æˆä½£ä½“å¹³å–å‘¨é•¿å®½é«˜é‡]{1,10})', 'åº“å­˜'),

            # æ—¶é—´æ¨¡å¼ - åŒ¹é…æ—¥æœŸæ ¼å¼
            (r'ä¸Šæ¶æ—¶é—´[:\s]*([0-9\-]+(?:\([^)]+\))?)', 'ä¸Šæ¶æ—¶é—´'),
            (r'å•†å“åˆ›å»ºæ—¶é—´[:\s]*([0-9.]+\s*\([^)]+\))', 'å•†å“åˆ›å»ºæ—¶é—´'),
            (r'(\d{2}\.\d{2}\.\d{4}\s*\([^)]+\))', 'å•†å“åˆ›å»ºæ—¶é—´'),  # åŒ¹é…"07.07.2025 (å·²åˆ›å»º 106 å¤©)"

            # ä»·æ ¼æ¨¡å¼ - åŒ¹é…â‚½ç¬¦å·
            (r'å¹³å‡ä»·æ ¼[:\s]*([0-9]+â‚½)', 'å¹³å‡ä»·æ ¼'),
            (r'æœ€ä½ä»·[:\s]*([^\s]{1,20})', 'æœ€ä½ä»·'),
            (r'æœ€é«˜ä»·[:\s]*([^\s]{1,20})', 'æœ€é«˜ä»·'),

            # ä½£é‡‘è´¹ç‡æ¨¡å¼ - åŒ¹é…å¤æ‚çš„è´¹ç‡ç»“æ„
            (r'ä»·æ ¼â‰¤1500å¢å¸ƒ[:\s]*(\d+\.?\d*%)', 'ä½£é‡‘è´¹ç‡1500ä»¥ä¸‹'),
            (r'ä»·æ ¼>1501å¢å¸ƒä»·æ ¼â‰¤5000å¢å¸ƒ[:\s]*(\d+\.?\d*%)', 'ä½£é‡‘è´¹ç‡1501-5000'),
            (r'ä»·æ ¼>5001å¢å¸ƒ[:\s]*(\d+\.?\d*%)', 'ä½£é‡‘è´¹ç‡5001ä»¥ä¸Š'),

            # ä½“ç§¯æ¨¡å¼ - åŒ¹é…ä¸åŒæ ¼å¼
            (r'ä½“ç§¯[:\s]*(\d+Ã—\d+Ã—\d+\s*mm)', 'ä½“ç§¯'),
            (r'ä½“ç§¯[:\s]*([0-9.]+\s*å…¬å‡[^å•†è´­å±•æˆä½£å¹³å–å‘¨é•¿å®½é«˜é‡]*)', 'ä½“ç§¯å®¹é‡'),
            (r'([0-9.]+\s*å…¬å‡\([^)]+\))', 'ä½“ç§¯å®¹é‡'),  # åŒ¹é…"27.5 å…¬å‡(é•¿xå®½xé«˜)"
            (r'Ñ€Ğ°Ğ·Ğ¼ĞµÑ€[:\s]*(\d+Ã—\d+Ã—\d+\s*Ğ¼Ğ¼)', 'ä½“ç§¯'),
        ]

        for pattern, key in patterns:
            try:
                matches = re.findall(pattern, text, re.IGNORECASE | re.MULTILINE)
                if matches:
                    # å–ç¬¬ä¸€ä¸ªåŒ¹é…ç»“æœï¼Œå¹¶æ¸…ç†ç©ºç™½å­—ç¬¦
                    raw_value = matches[0].strip()
                    if raw_value:
                        # è¿›ä¸€æ­¥æ¸…ç†å€¼
                        cleaned_value = self._clean_extracted_value(raw_value, key)
                        if cleaned_value and key not in kv_dict:  # é¿å…é‡å¤æ·»åŠ 
                            kv_dict[key] = cleaned_value
                            print(f"   ğŸ“ {source_name} æå–: {key} = {cleaned_value}")
            except Exception as e:
                print(f"   âš ï¸ æ¨¡å¼åŒ¹é…é”™è¯¯ {pattern}: {str(e)}")
                continue

        return kv_dict

    def _clean_extracted_value(self, value, key):
        """æ¸…ç†æå–çš„å€¼"""
        if not value:
            return ""

        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        value = re.sub(r'\s+', ' ', value).strip()

        # æ ¹æ®ä¸åŒçš„é”®è¿›è¡Œç‰¹å®šæ¸…ç†
        if key == 'sku':
            # SKUåªä¿ç•™æ•°å­—
            value = re.sub(r'[^\d]', '', value)
            if len(value) < 5:  # SKUè‡³å°‘5ä½æ•°å­—
                return ""

        elif key == 'å“ç‰Œ':
            # å“ç‰Œåæ¸…ç†ï¼šç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™å­—æ¯æ•°å­—å’Œå¸¸è§ç¬¦å·
            value = re.sub(r'[^\w\s\-\.&]', '', value)
            if len(value) > 50:  # å“ç‰Œåä¸åº”è¯¥å¤ªé•¿
                return ""

        elif key == 'é‡é‡':
            # é‡é‡æ ¼å¼æ ‡å‡†åŒ–
            value = re.sub(r'\s+', ' ', value)
            if not re.match(r'\d+\s*[gĞºĞ³]', value):
                return ""

        elif key == 'ä½“ç§¯':
            # ä½“ç§¯æ ¼å¼æ ‡å‡†åŒ–
            value = re.sub(r'\s+', '', value)
            if not re.match(r'\d+Ã—\d+Ã—\d+mm', value):
                return ""

        elif key == 'å–å®¶':
            # å–å®¶åæ¸…ç†
            value = re.sub(r'[^\w\s\-\.]', '', value)
            if len(value) > 30:  # å–å®¶åä¸åº”è¯¥å¤ªé•¿
                return ""

        elif key in ['æ¯›åˆ©ç‡', 'åŠ è´­ç‡']:
            # ç™¾åˆ†æ¯”æ ¼å¼æ£€æŸ¥
            if not value.endswith('%'):
                return ""

        elif key in ['é”€é‡', 'æ›å…‰é‡']:
            # æ•°å­—æ ¼å¼æ£€æŸ¥
            if not value.isdigit():
                return ""

        return value

    async def _format_and_display_results(self, dianpeng_data, seefar_data):
        """æ ¼å¼åŒ–å¹¶æ˜¾ç¤ºç»“æœ"""
        print("\n" + "="*80)
        print("ğŸ¨ æ ¼å¼åŒ–è¾“å‡ºç»“æœ")
        print("="*80)

        # åˆå¹¶æ‰€æœ‰æ•°æ®
        all_data = {}
        all_data.update(dianpeng_data)
        all_data.update(seefar_data)

        if not all_data:
            print("âŒ æœªæå–åˆ°ä»»ä½•å•†å“ä¿¡æ¯")
            return

        print("\nğŸ“¦ å•†å“ä¿¡æ¯å­—å…¸:")
        print("-" * 50)

        # æŒ‰æŒ‡å®šæ ¼å¼è¾“å‡º
        formatted_items = []
        for key, value in all_data.items():
            formatted_item = f"{key}:{value}"
            formatted_items.append(formatted_item)
            print(f"   {formatted_item}")

        print("\nğŸ¯ æ ¼å¼åŒ–å­—ç¬¦ä¸²:")
        print("-" * 50)
        formatted_string = "ï¼Œ".join(formatted_items)
        print(f"   {formatted_string}")

        print("\nğŸ“Š æå–ç»Ÿè®¡:")
        print("-" * 50)
        print(f"   ç”µé¹åŒºåŸŸå­—æ®µæ•°: {len(dianpeng_data)}")
        print(f"   seefaråŒºåŸŸå­—æ®µæ•°: {len(seefar_data)}")
        print(f"   æ€»å­—æ®µæ•°: {len(all_data)}")

        return {
            'formatted_string': formatted_string,
            'total_fields': len(all_data),
            'dianpeng_fields': len(dianpeng_data),
            'seefar_fields': len(seefar_data)
        }

async def main():
    """ä¸»å‡½æ•°"""
    # OZONå•†å“é¡µé¢URL
    url = "https://www.ozon.ru/product/kovrik-v-bagazhnik-iskusstvennaya-kozha-1-sht-2423301080/"

    analyzer = OzonPageAnalyzer(debug_mode=True)
    result = await analyzer.analyze_product_page(url)

    if result:
        print("\n" + "="*80)
        print("ğŸ æœ€ç»ˆç»“æœ")
        print("="*80)
        print(f"æå–ç»“æœ: {result}")

if __name__ == "__main__":
    asyncio.run(main())