要求：实现一个基于图片相似度的自动评分工具，要求给定两个url（或者本地图片文件）, 可以返回相似度评分0～1。 
图片的内容偏向与商品内容。由于用户不一定拥有 GPU，因此需要考虑用合适的方案实现。当然如果用户有gpu， 可以利用gpu加速。默认情况使用cpu。
将这个需求实现在src,tools/image_similarity.py里
注意：尽量使用业界的existing library，并尽可能使用开源库。


基于你的约束（本地可能无 GPU、Windows/macOS、对象是商品图片、需要0–1的相似度评分），推荐采用“分层方案 + 可选深度语义”的组合，既能在 CPU 上跑得动，又能兼顾近重复与语义相似。

可选方案（按复杂度与鲁棒性递增）

方案A（极快，近重复/轻微改动）

感知哈希（pHash/dHash/whash）→ 得到汉明距离，映射到相似度分数
结构相似度 SSIM → 对同尺寸或可对齐图片更有效
适用：同一商品的不同压缩、轻微缩放/亮度调整、裁边不大
分数映射：
hash 距离 d，位长 b（通常64）：score_hash = 1 - d / b
SSIM 本身 ∈ [0,1]，score_ssim = ssim
推荐综合分：score = 0.6score_hash + 0.4score_ssim
方案B（平衡，语义相似）

先用方案A粗筛，再用 CLIP（CPU 可跑）提取图像向量，计算余弦相似
适用：不同角度/背景/构图但同一商品或同类商品
分数映射：cosine ∈ [-1,1] → score_clip = (cos + 1)/2
推荐综合分：若方案A高于0.9直接返回，否则 score = 0.5A + 0.5score_clip
方案C（鲁棒，几何变化/局部匹配）

在方案B基础上，对有明显裁剪/旋转/纹理相似的图用 ORB/SIFT 做局部特征匹配
适用：强裁剪、不同视角、部分遮挡
分数映射（简单做法）：score_orb = min(1.0, good_matches / 200) 或 good_matches / max(kp_cnt, 1)
推荐综合分：score = 0.4A + 0.4score_clip + 0.2*score_orb
依赖与跨平台

必选（纯 CPU 即可）
Pillow, imagehash, scikit-image, opencv-python, numpy
可选（语义）
torch（CPU 版即可）、transformers 或 open_clip（选小模型）
pip
pip install pillow imagehash scikit-image opencv-python numpy torch transformers
阈值参考（需按你的数据微调）

近重复：score ≥ 0.9
同一商品不同图：score ≥ 0.8
同类商品：score ≥ 0.65–0.75
建议抽样几百对标注，画出分布后再定阈值。
一个可直接跑的实现（CPU 友好）
说明：

提供 hash、ssim、orb、clip 四种打分，auto_score 会按建议组合。
CLIP 默认用 ViT-B/32，CPU 也能跑；可缓存向量避免重复计算。
import numpy as np
from PIL import Image, ImageOps
import imagehash
from skimage.metrics import structural_similarity as ssim
import cv2
import torch
from transformers import CLIPModel, CLIPProcessor

def load_rgb(path):
img = Image.open(path).convert('RGB')
return img

def preprocess_for_ssim(a: Image.Image, b: Image.Image):
# 转灰度 + 对齐尺寸
a = a.convert('L')
b = b.convert('L')
w = 512
a = ImageOps.contain(a, (w, w))
b = ImageOps.contain(b, (w, w))
a = np.array(a)
b = np.array(b)
h = min(a.shape[0], b.shape[0])
w = min(a.shape[1], b.shape[1])
a = a[:h, :w]
b = b[:h, :w]
return a, b

def score_hash(a: Image.Image, b: Image.Image):
h1 = imagehash.phash(a)
h2 = imagehash.phash(b)
b_len = 64 # phash 默认64位
dist = abs(h1 - h2)
return max(0.0, 1.0 - dist / b_len)

def score_ssim(a: Image.Image, b: Image.Image):
g1, g2 = preprocess_for_ssim(a, b)
s = ssim(g1, g2, data_range=255)
# 限制到[0,1]
return float(np.clip(s, 0.0, 1.0))

def score_orb(a: Image.Image, b: Image.Image):
# ORB 特征匹配得分，鲁棒于旋转/缩放/裁剪
a_cv = cv2.cvtColor(np.array(a), cv2.COLOR_RGB2GRAY)
b_cv = cv2.cvtColor(np.array(b), cv2.COLOR_RGB2GRAY)
orb = cv2.ORB_create(1000)
kp1, des1 = orb.detectAndCompute(a_cv, None)
kp2, des2 = orb.detectAndCompute(b_cv, None)
if des1 is None or des2 is None:
return 0.0
bf = cv2.BFMatcher(cv2.NORM_HAMMING)
matches = bf.knnMatch(des1, des2, k=2)
good = [m for m, n in matches if m.distance < 0.75 * n.distance]
# 归一化到[0,1]，可根据数据调上限
score = min(1.0, len(good) / 200.0)
return float(score)

class CLIPScorer:
def init(self, model_name="openai/clip-vit-base-patch32", device=None):
self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
self.model = CLIPModel.from_pretrained(model_name).to(self.device)
self.processor = CLIPProcessor.from_pretrained(model_name)
self.cache = {}

def embed(self, img: Image.Image):
key = id(img)
if key in self.cache:
return self.cache[key]
inputs = self.processor(images=img, return_tensors="pt").to(self.device)
with torch.no_grad():
feats = self.model.get_image_features(**inputs)
feats = feats / feats.norm(dim=-1, keepdim=True)
v = feats.cpu().numpy()[0]
self.cache[key] = v
return v

def score(self, a: Image.Image, b: Image.Image):
va = self.embed(a)
vb = self.embed(b)
cos = float(np.dot(va, vb))
# 余弦 (-1,1)→(0,1)
return (cos + 1.0) / 2.0
class ProductImageSimilarity:
def init(self, use_clip=True, clip_model="openai/clip-vit-base-patch32"):
self.use_clip = use_clip
self.clip = CLIPScorer(clip_model) if use_clip else None

def score_fast(self, a_path, b_path):
a = load_rgb(a_path)
b = load_rgb(b_path)
h = score_hash(a, b)
s = score_ssim(a, b)
return 0.6 * h + 0.4 * s

def score_semantic(self, a_path, b_path):
a = load_rgb(a_path)
b = load_rgb(b_path)
base = self.score_fast(a_path, b_path)
if base >= 0.9 or not self.use_clip:
return base
clip_s = self.clip.score(a, b)
# 可选 ORB：仅在 base 和 clip 分歧较大时再算，节省开销
orb_s = score_orb(a, b) if abs(base - clip_s) > 0.25 else 0.0
# 综合（权重可调）
if orb_s > 0:
return 0.4 * base + 0.4 * clip_s + 0.2 * orb_s
return 0.5 * base + 0.5 * clip_s

def auto_score(self, a_path, b_path):
# 先快，再深；根据需要选择
return self.score_semantic(a_path, b_path) if self.use_clip else self.score_fast(a_path, b_path)
使用建议

预处理
统一读取为 RGB；可把短边缩放到固定尺寸（如 256–512）再比较。
商品图常有白底/留白，可选做“内容裁剪”（trim border）提升 SSIM/哈希稳定性；或采用中心裁剪+填充策略。
性能预估（CPU，单对图片）
hash+ssim：3–15ms
ORB：30–150ms（取决于特征数）
CLIP（ViT-B/32 CPU）：150–500ms/图（首次加载模型较慢，后续可缓存向量）
缓存与批量
对大批图片库做检索：先统一生成 CLIP 向量，存 npy/磁盘，再用 Faiss/HNSW 做相似检索；online 对比只做少量候选。
阈值标定
按你的商品品类抽 200–500 对对比，画出正/负样本分布，选 95%TPR 对应的阈值作为“同一商品”的分界；再选较低阈值表示“同类相似”。
何时选哪种

只做近重复/去水印重复：方案A 即可（快，阈值易定）
要“同一商品不同角度/背景”的识别：方案B（hash+ssim 预筛 + CLIP）
有强裁剪/角度/遮挡：方案B + ORB 作为纠偏