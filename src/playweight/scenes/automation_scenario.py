"""
è‡ªåŠ¨åŒ–åœºæ™¯æ¨¡å— - Seerfaråº—é“ºæ•°æ®çˆ¬å–è‡ªåŠ¨åŒ–åœºæ™¯
åŒ…å«ä¸¥æ ¼çš„æ“ä½œè¯­ä¹‰å’Œé¡ºåºï¼Œæ¯ä¸ªæ­¥éª¤éƒ½æœ‰æ˜ç¡®çš„å®šä¹‰å’Œæ‰§è¡Œé€»è¾‘

è‡ªåŠ¨åŒ–åœºæ™¯æ‰§è¡Œæµç¨‹ï¼š
ç¬¬ä¸€æ­¥ï¼šåˆå§‹åŒ–å’Œç¯å¢ƒå‡†å¤‡
ç¬¬äºŒæ­¥ï¼šé¡µé¢å¯¼èˆªå’ŒåŠ è½½ç­‰å¾…
ç¬¬ä¸‰æ­¥ï¼šæ•°æ®å®šä½å’Œæå–
ç¬¬å››æ­¥ï¼šæ•°æ®éªŒè¯å’Œå¤„ç†
ç¬¬äº”æ­¥ï¼šç»“æœè¾“å‡ºå’Œæ¸…ç†
"""

import asyncio
import re
import pandas as pd
from datetime import datetime
from typing import Optional, Dict, Any, List
from playwright.async_api import Page, ElementHandle

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from ..engine import DOMAnalyzer
from logger_config import get_logger
from engine.browser_common import UniversalPaginator


def _print_store_products_details(products: List[Dict[str, Any]], store_index: int, store_id: str):
    """
    æ‰“å°åº—é“ºå•†å“çš„è¯¦ç»†å±æ€§ä¿¡æ¯

    Args:
        products: å•†å“åˆ—è¡¨
        store_index: åº—é“ºç´¢å¼•
        store_id: åº—é“ºID
    """
    if not products:
        return

    print(f"\nğŸ“‹ åº—é“º {store_index} ({store_id}) å•†å“è¯¦ç»†ä¿¡æ¯:")
    print(f"{'='*80}")

    # å­—æ®µåç§°æ˜ å°„
    field_names = {
        'product_link_url': 'äº§å“é“¾æ¥',
        'product_id': 'äº§å“ID',
        'category': 'ç±»ç›®',
        'price': 'å”®ä»·',
        'sales_volume': 'é”€é‡',
        'sales_amount': 'é”€å”®é¢',
        'profit_margin': 'æ¯›åˆ©ç‡',
        'exposure': 'æ›å…‰é‡',
        'product_views': 'äº§å“å¡æµè§ˆé‡',
        'add_to_cart_rate': 'åŠ è´­ç‡',
        'conversion_rate': 'è®¢å•è½¬åŒ–ç‡',
        'ad_cost_share': 'å¹¿å‘Šè´¹ç”¨ä»½é¢',
        'return_cancel_rate': 'é€€è´§å–æ¶ˆç‡',
        'rating': 'è¯„åˆ†',
        'shop_name': 'åº—é“º',
        'seller_type': 'å–å®¶ç±»å‹',
        'delivery_method': 'é…é€æ–¹å¼',
        'weight': 'é‡é‡',
        'listing_time': 'ä¸Šæ¶æ—¶é—´',
        'product_description': 'å•†å“æè¿°'
    }

    for index, product in enumerate(products, 1):
        print(f"\nğŸ“¦ å•†å“ {index}:")
        print(f"{'-'*60}")

        # æ‰“å°æ‰€æœ‰å±æ€§
        for field_key, field_name in field_names.items():
            value = product.get(field_key, '').strip()
            if value:
                # å¯¹äºé“¾æ¥ï¼Œæˆªæ–­æ˜¾ç¤º
                if field_key == 'product_link_url' and len(value) > 80:
                    display_value = value[:77] + "..."
                elif field_key == 'product_description' and len(value) > 100:
                    display_value = value[:97] + "..."
                else:
                    display_value = value
                print(f"   {field_name:12s}: {display_value}")
            else:
                print(f"   {field_name:12s}: [æœªå¡«å……]")

    # ç»Ÿè®¡å­—æ®µå¡«å……æƒ…å†µ
    print(f"\nğŸ“ˆ å­—æ®µå¡«å……ç»Ÿè®¡:")
    total_count = len(products)
    for field_key, field_name in field_names.items():
        filled_count = sum(1 for product in products if product.get(field_key, '').strip())
        percentage = (filled_count / total_count) * 100 if total_count > 0 else 0
        print(f"   {field_name:12s}: {filled_count:3d}/{total_count} ({percentage:5.1f}%)")

    print(f"{'='*80}")


def _display_product_statistics(products: List[Dict[str, Any]]):
    """
    æ˜¾ç¤ºå•†å“å­—æ®µç»Ÿè®¡ä¿¡æ¯

    Args:
        products: å•†å“åˆ—è¡¨
    """
    if not products:
        return

    print("\nğŸ“ˆ å•†å“å­—æ®µç»Ÿè®¡:")

    # ç»Ÿè®¡å„å­—æ®µçš„å¡«å……æƒ…å†µ
    field_stats = {}
    field_names = {
        'product_link_url': 'äº§å“é“¾æ¥',
        'product_id': 'äº§å“ID',
        'category': 'ç±»ç›®',
        'price': 'å”®ä»·',
        'sales_volume': 'é”€é‡',
        'sales_amount': 'é”€å”®é¢',
        'profit_margin': 'æ¯›åˆ©ç‡',
        'exposure': 'æ›å…‰é‡',
        'product_views': 'äº§å“å¡æµè§ˆé‡',
        'add_to_cart_rate': 'åŠ è´­ç‡',
        'conversion_rate': 'è®¢å•è½¬åŒ–ç‡',
        'ad_cost_share': 'å¹¿å‘Šè´¹ç”¨ä»½é¢',
        'return_cancel_rate': 'é€€è´§å–æ¶ˆç‡',
        'rating': 'è¯„åˆ†',
        'shop_name': 'åº—é“º',
        'seller_type': 'å–å®¶ç±»å‹',
        'delivery_method': 'é…é€æ–¹å¼',
        'weight': 'é‡é‡',
        'listing_time': 'ä¸Šæ¶æ—¶é—´'
    }

    total_count = len(products)

    for field_key, field_name in field_names.items():
        filled_count = sum(1 for product in products if product.get(field_key, '').strip())
        percentage = (filled_count / total_count) * 100 if total_count > 0 else 0
        field_stats[field_name] = f"{filled_count}/{total_count} ({percentage:.1f}%)"

    # æ‰“å°ç»Ÿè®¡ç»“æœ
    for field_name, stats in field_stats.items():
        print(f"   {field_name}: {stats}")


class AutomationScenario:
    """
    è‡ªåŠ¨åŒ–åœºæ™¯æ‰§è¡Œå™¨ - Seerfaråº—é“ºæ•°æ®çˆ¬å–åœºæ™¯
    
    è¯¥ç±»å®šä¹‰äº†ä¸€ä¸ªå®Œæ•´çš„è‡ªåŠ¨åŒ–åœºæ™¯ï¼ŒåŒ…å«ä»¥ä¸‹ä¸¥æ ¼çš„æ‰§è¡Œæ­¥éª¤ï¼š
    1. åœºæ™¯åˆå§‹åŒ– - è®¾ç½®åŸºç¡€å‚æ•°å’Œä¾èµ–ç»„ä»¶
    2. ç¯å¢ƒå‡†å¤‡ - éªŒè¯é¡µé¢å¯¹è±¡å’ŒDOMåˆ†æå™¨
    3. é¡µé¢å¯¼èˆª - è®¿é—®ç›®æ ‡é¡µé¢å¹¶ç­‰å¾…åŠ è½½å®Œæˆ
    4. å…ƒç´ å®šä½ - å®šä½å…³é”®é¡µé¢å…ƒç´ 
    5. æ•°æ®æå– - æŒ‰ç…§é¢„å®šä¹‰è§„åˆ™æå–æ•°æ®
    6. æ•°æ®éªŒè¯ - éªŒè¯æå–æ•°æ®çš„å®Œæ•´æ€§å’Œå‡†ç¡®æ€§
    7. ç»“æœå¤„ç† - æ ¼å¼åŒ–å’Œå­˜å‚¨æå–ç»“æœ
    8. åœºæ™¯æ¸…ç† - æ¸…ç†ä¸´æ—¶èµ„æºå’ŒçŠ¶æ€
    """
    
    def __init__(self, request_delay: float = 2.0, debug_mode: bool = False, max_products_per_store: int = 21):
        """
        ç¬¬ä¸€æ­¥ï¼šåœºæ™¯åˆå§‹åŒ–
        
        åˆå§‹åŒ–è‡ªåŠ¨åŒ–åœºæ™¯æ‰§è¡Œå™¨ï¼Œè®¾ç½®åŸºç¡€å‚æ•°å’Œä¾èµ–ç»„ä»¶
        
        Args:
            request_delay: è¯·æ±‚é—´éš”æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œç”¨äºæ§åˆ¶çˆ¬å–é¢‘ç‡
            debug_mode: æ˜¯å¦å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼Œå½±å“æ—¥å¿—è¾“å‡ºçº§åˆ«
        """
        # åŸºç¡€é…ç½®å‚æ•°
        self.request_delay = request_delay
        self.debug_mode = debug_mode
        self.max_products_per_store = max_products_per_store

        # æ ¸å¿ƒç»„ä»¶åˆå§‹åŒ–
        self.page: Optional[Page] = None
        self.dom_analyzer: Optional[DOMAnalyzer] = None
        
        # æ—¥å¿—ç»„ä»¶åˆå§‹åŒ–
        self.logger = get_logger(debug_mode)
        
        # Seerfarå¹³å°URLæ¨¡æ¿
        self.seerfar_url_template = "https://seerfar.cn/admin/store-detail.html?storeId={store_id}&platform=OZON"
        
        # åœºæ™¯æ‰§è¡ŒçŠ¶æ€
        self.scenario_state = {
            'initialized': False,
            'page_ready': False,
            'current_step': 'initialization'
        }
        
        self.logger.info("ğŸ¯ è‡ªåŠ¨åŒ–åœºæ™¯åˆå§‹åŒ–å®Œæˆ")
        self.logger.info(f"   è¯·æ±‚é—´éš”: {self.request_delay}ç§’")
        self.logger.info(f"   è°ƒè¯•æ¨¡å¼: {'å¯ç”¨' if self.debug_mode else 'ç¦ç”¨'}")
        self.logger.info(f"   æ¯åº—é“ºæœ€å¤§å•†å“æ•°: {self.max_products_per_store}ä¸ª")

    def set_page(self, page: Page):
        """
        ç¬¬äºŒæ­¥ï¼šç¯å¢ƒå‡†å¤‡ - è®¾ç½®é¡µé¢å¯¹è±¡å’ŒDOMåˆ†æå™¨
        
        Args:
            page: Playwrighté¡µé¢å¯¹è±¡
        """
        self.logger.info("ğŸ”§ æ‰§è¡Œç¬¬äºŒæ­¥ï¼šç¯å¢ƒå‡†å¤‡")
        
        if not page:
            raise ValueError("é¡µé¢å¯¹è±¡ä¸èƒ½ä¸ºç©º")
        
        self.page = page
        self.dom_analyzer = DOMAnalyzer(page, debug_mode=self.debug_mode)
        
        # æ›´æ–°åœºæ™¯çŠ¶æ€
        self.scenario_state['page_ready'] = True
        self.scenario_state['current_step'] = 'environment_ready'
        
        self.logger.info("âœ… ç¯å¢ƒå‡†å¤‡å®Œæˆ - é¡µé¢å¯¹è±¡å’ŒDOMåˆ†æå™¨å·²å°±ç»ª")
    
    def build_seerfar_url(self, store_id: str) -> str:
        """
        æ„å»ºSeerfaråº—é“ºè¯¦æƒ…é¡µURL
        
        Args:
            store_id: åº—é“ºID
            
        Returns:
            str: å®Œæ•´çš„åº—é“ºè¯¦æƒ…é¡µURL
        """
        if not store_id:
            raise ValueError("åº—é“ºIDä¸èƒ½ä¸ºç©º")
        
        url = self.seerfar_url_template.format(store_id=store_id)
        self.logger.debug(f"æ„å»ºURL: {url}")
        return url
    
    def extract_store_id_from_data(self, store_info: Dict[str, Any]) -> Optional[str]:
        """
        ä»åº—é“ºä¿¡æ¯ä¸­æå–åº—é“ºID
        
        Args:
            store_info: åº—é“ºä¿¡æ¯å­—å…¸
            
        Returns:
            Optional[str]: æå–åˆ°çš„åº—é“ºIDï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å›None
        """
        self.logger.debug("å¼€å§‹ä»åº—é“ºä¿¡æ¯ä¸­æå–åº—é“ºID")
        
        # å¯èƒ½çš„åº—é“ºIDå­—æ®µå
        possible_id_fields = ['store_id', 'storeId', 'id', 'shop_id', 'shopId']
        
        # ç¬¬ä¸€è½®ï¼šç›´æ¥åŒ¹é…å­—æ®µå
        for field in possible_id_fields:
            if field in store_info and pd.notna(store_info[field]):
                store_id = str(store_info[field]).strip()
                if store_id:
                    self.logger.debug(f"ä»å­—æ®µ '{field}' æå–åˆ°åº—é“ºID: {store_id}")
                    return store_id
        
        # ç¬¬äºŒè½®ï¼šéå†æ‰€æœ‰å­—æ®µï¼Œå¯»æ‰¾æ•°å­—å‹ID
        for key, value in store_info.items():
            if pd.notna(value):
                try:
                    num_value = int(float(str(value)))
                    if 100 <= num_value <= 999999999999:  # åº—é“ºIDé€šå¸¸åœ¨è¿™ä¸ªèŒƒå›´
                        store_id = str(num_value)
                        self.logger.debug(f"ä»å­—æ®µ '{key}' æå–åˆ°åº—é“ºID: {store_id}")
                        return store_id
                except (ValueError, TypeError):
                    continue
        
        self.logger.warning("æœªèƒ½ä»åº—é“ºä¿¡æ¯ä¸­æå–åˆ°æœ‰æ•ˆçš„åº—é“ºID")
        return None
    
    async def execute_store_data_extraction_scenario(self, url: str) -> Dict[str, Any]:
        """
        ç¬¬ä¸‰æ­¥ï¼šæ‰§è¡Œåº—é“ºæ•°æ®æå–åœºæ™¯
        
        è¿™æ˜¯æ ¸å¿ƒçš„è‡ªåŠ¨åŒ–åœºæ™¯ï¼ŒåŒ…å«ä»¥ä¸‹ä¸¥æ ¼çš„æ‰§è¡Œæ­¥éª¤ï¼š
        3.1 é¡µé¢å¯¼èˆªå’ŒåŠ è½½ç­‰å¾…
        3.2 é¡µé¢å…ƒç´ å®šä½å’ŒéªŒè¯
        3.3 åº—é“ºåŸºç¡€æ•°æ®æå–
        3.4 é”€å”®æ•°æ®æå–
        3.5 å•†å“åˆ—è¡¨æ•°æ®æå–
        3.6 æ•°æ®éªŒè¯å’Œæ•´åˆ
        
        Args:
            url: ç›®æ ‡é¡µé¢URL
            
        Returns:
            Dict[str, Any]: æå–çš„åº—é“ºæ•°æ®
        """
        self.logger.info("ğŸš€ æ‰§è¡Œç¬¬ä¸‰æ­¥ï¼šåº—é“ºæ•°æ®æå–åœºæ™¯")
        self.scenario_state['current_step'] = 'data_extraction'
        
        # åˆå§‹åŒ–è¿”å›æ•°æ®ç»“æ„
        store_data = {
            'url': url,
            'extraction_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'success': False,
            'error_message': '',
            'store_id': '',
            'page_title': '',
            'sales_amount': '',
            'sales_volume': '',
            'daily_avg_sales': '',
            'products': []
        }
        
        try:
            # æ­¥éª¤3.1ï¼šé¡µé¢å¯¼èˆªå’ŒåŠ è½½ç­‰å¾…
            await self._step_3_1_navigate_and_wait(url, store_data)

            # æ­¥éª¤3.2ï¼šé¡µé¢å…ƒç´ å®šä½å’ŒéªŒè¯
            await self._step_3_2_locate_and_verify_elements(store_data)
            
            # æ­¥éª¤3.3ï¼šåº—é“ºåŸºç¡€æ•°æ®æå–
            await self._step_3_3_extract_basic_store_data(url, store_data)
            
            # æ­¥éª¤3.4ï¼šé”€å”®æ•°æ®æå–
            await self._step_3_4_extract_sales_data(store_data)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦è·³è¿‡åç»­å¤„ç†
            if store_data.get('skip_processing', False):
                self.logger.warning(f"âš ï¸ è·³è¿‡å•†å“æ•°æ®æå–ï¼š{store_data.get('skip_reason', 'æœªçŸ¥åŸå› ')}")
                # ç›´æ¥è·³åˆ°éªŒè¯æ­¥éª¤ï¼Œè®¾ç½®å¤±è´¥çŠ¶æ€
                store_data['success'] = False
                store_data['products'] = []
                store_data['error_message'] = store_data.get('skip_reason', 'å…³é”®é”€å”®æ•°æ®æ— æ³•è·å–')
                self.logger.info("âœ… åº—é“ºå¤„ç†å®Œæˆï¼ˆå·²è·³è¿‡ï¼‰")
            else:
                # æ­¥éª¤3.5ï¼šå•†å“åˆ—è¡¨æ•°æ®æå–
                await self._step_3_5_extract_products_data(url, store_data)
            
            # æ­¥éª¤3.6ï¼šæ•°æ®éªŒè¯å’Œæ•´åˆ
            await self._step_3_6_validate_and_integrate_data(store_data)
            
        except Exception as e:
            error_msg = str(e)
            store_data['error_message'] = error_msg
            self.logger.error(f"âŒ åº—é“ºæ•°æ®æå–åœºæ™¯æ‰§è¡Œå¤±è´¥: {error_msg}")
        
        return store_data
    
    async def _step_3_1_navigate_and_wait(self, url: str, store_data: Dict[str, Any]):
        """
        æ­¥éª¤3.1ï¼šé¡µé¢å¯¼èˆªå’ŒåŠ è½½ç­‰å¾…
        
        æ‰§è¡Œé¡µé¢å¯¼èˆªï¼Œå¹¶ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½
        """
        self.logger.info("ğŸ“ æ‰§è¡Œæ­¥éª¤3.1ï¼šé¡µé¢å¯¼èˆªå’ŒåŠ è½½ç­‰å¾…")
        
        if not self.page:
            raise Exception("é¡µé¢å¯¹è±¡æœªè®¾ç½®")
        
        self.logger.info(f"ğŸŒ è®¿é—®é¡µé¢: {url}")
        
        # å¯¼èˆªåˆ°ç›®æ ‡é¡µé¢ï¼Œç­‰å¾…ç½‘ç»œç©ºé—²
        await self.page.goto(url, wait_until='networkidle', timeout=30000)
        
        # é¢å¤–ç­‰å¾…ç¡®ä¿é¡µé¢å®Œå…¨åŠ è½½
        await asyncio.sleep(2)
        
        self.logger.info("âœ… æ­¥éª¤3.1å®Œæˆï¼šé¡µé¢å¯¼èˆªå’ŒåŠ è½½ç­‰å¾…")
    
    async def _step_3_2_locate_and_verify_elements(self, store_data: Dict[str, Any]):
        """
        æ­¥éª¤3.2ï¼šé¡µé¢å…ƒç´ å®šä½å’ŒéªŒè¯
        
        å®šä½å…³é”®é¡µé¢å…ƒç´ ï¼ŒéªŒè¯é¡µé¢æ˜¯å¦æ­£ç¡®åŠ è½½
        """
        self.logger.info("ğŸ“ æ‰§è¡Œæ­¥éª¤3.2ï¼šé¡µé¢å…ƒç´ å®šä½å’ŒéªŒè¯")
        
        # è·å–é¡µé¢æ ‡é¢˜è¿›è¡ŒéªŒè¯
        try:
            store_data['page_title'] = await self.page.title()
            self.logger.info(f"ğŸ“„ é¡µé¢æ ‡é¢˜: {store_data['page_title']}")
        except Exception as e:
            self.logger.warning(f"è·å–é¡µé¢æ ‡é¢˜å¤±è´¥: {str(e)}")
        
        # ç­‰å¾…å…³é”®å…ƒç´ å‡ºç°
        try:
            # ç­‰å¾…é¡µé¢ä¸»è¦å†…å®¹åŒºåŸŸåŠ è½½
            await self.page.wait_for_selector('body', timeout=10000)
            self.logger.debug("é¡µé¢ä¸»ä½“å…ƒç´ å·²åŠ è½½")
        except Exception as e:
            self.logger.warning(f"ç­‰å¾…é¡µé¢å…ƒç´ è¶…æ—¶: {str(e)}")
        
        self.logger.info("âœ… æ­¥éª¤3.2å®Œæˆï¼šé¡µé¢å…ƒç´ å®šä½å’ŒéªŒè¯")
    
    async def _step_3_3_extract_basic_store_data(self, url: str, store_data: Dict[str, Any]):
        """
        æ­¥éª¤3.3ï¼šåº—é“ºåŸºç¡€æ•°æ®æå–
        
        æå–åº—é“ºIDç­‰åŸºç¡€ä¿¡æ¯
        """
        self.logger.info("ğŸ“ æ‰§è¡Œæ­¥éª¤3.3ï¼šåº—é“ºåŸºç¡€æ•°æ®æå–")
        
        # ä»URLä¸­æå–åº—é“ºID
        store_id_match = re.search(r'storeId=(\d+)', url)
        if store_id_match:
            store_data['store_id'] = store_id_match.group(1)
            self.logger.info(f"ğŸª åº—é“ºID: {store_data['store_id']}")
        
        self.logger.info("âœ… æ­¥éª¤3.3å®Œæˆï¼šåº—é“ºåŸºç¡€æ•°æ®æå–")
    
    async def _step_3_4_extract_sales_data(self, store_data: Dict[str, Any]):
        """
        æ­¥éª¤3.4ï¼šé”€å”®æ•°æ®æå–
        
        ä½¿ç”¨ç²¾ç¡®çš„XPathå®šä½é”€å”®æ•°æ®å…ƒç´ ï¼Œå¹¶éªŒè¯å…³é”®é”€å”®æ•°æ®æ˜¯å¦æˆåŠŸè·å–
        """
        self.logger.info("ğŸ“ æ‰§è¡Œæ­¥éª¤3.4ï¼šé”€å”®æ•°æ®æå–")

        # æå–é”€å”®é¢
        await self._extract_sales_amount(store_data)

        # æå–é”€é‡
        await self._extract_sales_volume(store_data)

        # æå–æ—¥å‡é”€é‡
        await self._extract_daily_avg_sales(store_data)

        # éªŒè¯å…³é”®é”€å”®æ•°æ®æ˜¯å¦æˆåŠŸè·å–
        sales_amount = store_data.get('sales_amount', '').strip()
        sales_volume = store_data.get('sales_volume', '').strip()
        daily_avg_sales = store_data.get('daily_avg_sales', '').strip()

        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰å…³é”®é”€å”®æ•°æ®éƒ½æ— æ³•è·å–
        if not sales_amount and not sales_volume and not daily_avg_sales:
            self.logger.warning("âš ï¸ å…³é”®é”€å”®æ•°æ®ï¼ˆé”€å”®é¢ã€é”€é‡ã€æ—¥å‡é”€é‡ï¼‰å‡æ— æ³•è·å–ï¼Œå°†è·³è¿‡è¯¥åº—é“ºçš„åç»­å¤„ç†")
            store_data['skip_processing'] = True
            store_data['skip_reason'] = "å…³é”®é”€å”®æ•°æ®ï¼ˆé”€å”®é¢_30å¤©ã€åº—é“ºé”€é‡_30å¤©ã€æ—¥å‡é”€é‡_30å¤©ï¼‰æ— æ³•è·å–"
            self.logger.info("âœ… æ­¥éª¤3.4å®Œæˆï¼šé”€å”®æ•°æ®æå–ï¼ˆæ£€æµ‹åˆ°è·³è¿‡æ¡ä»¶ï¼‰")
        else:
            self.logger.info("âœ… æ­¥éª¤3.4å®Œæˆï¼šé”€å”®æ•°æ®æå–")
    
    async def _step_3_5_extract_products_data(self, url: str, store_data: Dict[str, Any]):
        """
        æ­¥éª¤3.5ï¼šå•†å“åˆ—è¡¨æ•°æ®æå–
        
        æå–åº—é“ºå•†å“åˆ—è¡¨ä¿¡æ¯
        """
        self.logger.info("ğŸ“ æ‰§è¡Œæ­¥éª¤3.5ï¼šå•†å“åˆ—è¡¨æ•°æ®æå–")
        
        products = await self.extract_products_from_store(url, store_data['store_id'])
        store_data['products'] = products
        
        self.logger.info(f"âœ… æ­¥éª¤3.5å®Œæˆï¼šæå–åˆ° {len(products)} ä¸ªå•†å“")
    
    async def _step_3_6_validate_and_integrate_data(self, store_data: Dict[str, Any]):
        """
        æ­¥éª¤3.6ï¼šæ•°æ®éªŒè¯å’Œæ•´åˆ
        
        éªŒè¯æå–æ•°æ®çš„å®Œæ•´æ€§ï¼Œè®¾ç½®æˆåŠŸæ ‡å¿—
        æ³¨æ„ï¼šå¦‚æœåº—é“ºå·²è¢«æ ‡è®°ä¸ºè·³è¿‡å¤„ç†ï¼Œåˆ™ä¿æŒå¤±è´¥çŠ¶æ€ä¸å˜
        """
        self.logger.info("ğŸ“ æ‰§è¡Œæ­¥éª¤3.6ï¼šæ•°æ®éªŒè¯å’Œæ•´åˆ")

        # æ£€æŸ¥æ˜¯å¦å·²ç»è¢«æ ‡è®°ä¸ºè·³è¿‡å¤„ç†
        if store_data.get('skip_processing', False):
            self.logger.info("ğŸ”„ åº—é“ºå·²è¢«æ ‡è®°ä¸ºè·³è¿‡å¤„ç†ï¼Œä¿æŒå¤±è´¥çŠ¶æ€")
            # ç¡®ä¿è·³è¿‡çš„åº—é“ºä¿æŒå¤±è´¥çŠ¶æ€ï¼Œä¸è¿›è¡Œé‡æ–°è¯„ä¼°
            if 'success' not in store_data:
                store_data['success'] = False
            self.logger.info("âœ… æ­¥éª¤3.6å®Œæˆï¼šæ•°æ®éªŒè¯å’Œæ•´åˆï¼ˆè·³è¿‡çŠ¶æ€ä¿æŒï¼‰")
            return

        # å¯¹äºæœªè·³è¿‡çš„åº—é“ºï¼Œè¿›è¡Œæ­£å¸¸çš„æ•°æ®éªŒè¯
        # åˆ¤æ–­æ˜¯å¦æˆåŠŸæå–åˆ°å…³é”®æ•°æ®
        has_store_data = any([store_data.get('sales_amount', ''), store_data.get('sales_volume', ''), store_data.get('daily_avg_sales', '')])
        has_product_data = len(store_data.get('products', [])) > 0

        if has_store_data or has_product_data:
            store_data['success'] = True
            if has_product_data:
                self.logger.info(f"ğŸ¯ æ•°æ®æå–æˆåŠŸï¼ŒåŒ…å« {len(store_data['products'])} ä¸ªå•†å“ä¿¡æ¯")
            else:
                self.logger.info("ğŸ¯ åº—é“ºæ•°æ®æå–æˆåŠŸ")
        else:
            store_data['success'] = False
            store_data['error_message'] = "æœªèƒ½æå–åˆ°ä»»ä½•å…³é”®æ•°æ®"
            self.logger.warning("âš ï¸ æœªèƒ½æå–åˆ°ä»»ä½•å…³é”®æ•°æ®")
        
        self.logger.info("âœ… æ­¥éª¤3.6å®Œæˆï¼šæ•°æ®éªŒè¯å’Œæ•´åˆ")
    
    async def _extract_sales_amount(self, store_data: Dict[str, Any]):
        """æå–é”€å”®é¢ - ä½¿ç”¨ç”¨æˆ·æä¾›çš„ç²¾ç¡®XPath"""
        try:
            self.logger.info("ğŸ“Š æå–é”€å”®é¢...")
            
            # ç”¨æˆ·æä¾›çš„ç²¾ç¡®XPath
            sales_amount_xpath = "/html/body/div[1]/div/div/div/div/div/div/div[1]/div/div[2]/div[3]/div[1]/div[3]"
            
            # ç­‰å¾…å…ƒç´ å‡ºç°
            try:
                await self.page.wait_for_selector(f'xpath={sales_amount_xpath}', timeout=5000)
            except:
                self.logger.debug("é”€å”®é¢å…ƒç´ ç­‰å¾…è¶…æ—¶ï¼Œç»§ç»­å°è¯•æå–")
            
            element = await self.page.query_selector(f'xpath={sales_amount_xpath}')
            if element:
                text = await element.text_content()
                if text and text.strip():
                    store_data['sales_amount'] = text.strip()
                    self.logger.info(f"âœ… é”€å”®é¢: {store_data['sales_amount']}")
                    return
            
            self.logger.warning("âš ï¸ æœªèƒ½æå–åˆ°é”€å”®é¢æ•°æ®")
            
        except Exception as e:
            self.logger.error(f"âŒ é”€å”®é¢æå–å¤±è´¥: {str(e)}")
    
    async def _extract_sales_volume(self, store_data: Dict[str, Any]):
        """æå–é”€é‡ - ä½¿ç”¨ç”¨æˆ·æä¾›çš„ç²¾ç¡®XPath"""
        try:
            self.logger.info("ğŸ“Š æå–é”€é‡...")
            
            # ç”¨æˆ·æä¾›çš„ç²¾ç¡®XPath
            sales_volume_xpath = "/html/body/div[1]/div/div/div/div/div/div/div[1]/div/div[2]/div[3]/div[2]/div[3]"
            
            # ç­‰å¾…å…ƒç´ å‡ºç°
            try:
                await self.page.wait_for_selector(f'xpath={sales_volume_xpath}', timeout=5000)
            except:
                self.logger.debug("é”€é‡å…ƒç´ ç­‰å¾…è¶…æ—¶ï¼Œç»§ç»­å°è¯•æå–")
            
            element = await self.page.query_selector(f'xpath={sales_volume_xpath}')
            if element:
                text = await element.text_content()
                if text and text.strip():
                    store_data['sales_volume'] = text.strip()
                    self.logger.info(f"âœ… é”€é‡: {store_data['sales_volume']}")
                    return
            
            self.logger.warning("âš ï¸ æœªèƒ½æå–åˆ°é”€é‡æ•°æ®")
            
        except Exception as e:
            self.logger.error(f"âŒ é”€é‡æå–å¤±è´¥: {str(e)}")
    
    async def _extract_daily_avg_sales(self, store_data: Dict[str, Any]):
        """æå–æ—¥å‡é”€é‡ - ä½¿ç”¨ç”¨æˆ·æä¾›çš„ç²¾ç¡®XPath"""
        try:
            self.logger.info("ğŸ“Š æå–æ—¥å‡é”€é‡...")
            
            # ç”¨æˆ·æä¾›çš„ç²¾ç¡®XPath
            daily_avg_xpath = "/html/body/div[1]/div/div/div/div/div/div/div[1]/div/div[2]/div[3]/div[3]/div[3]"
            
            # ç­‰å¾…å…ƒç´ å‡ºç°
            try:
                await self.page.wait_for_selector(f'xpath={daily_avg_xpath}', timeout=5000)
            except:
                self.logger.debug("æ—¥å‡é”€é‡å…ƒç´ ç­‰å¾…è¶…æ—¶ï¼Œç»§ç»­å°è¯•æå–")
            
            element = await self.page.query_selector(f'xpath={daily_avg_xpath}')
            if element:
                text = await element.text_content()
                if text and text.strip():
                    store_data['daily_avg_sales'] = text.strip()
                    self.logger.info(f"âœ… æ—¥å‡é”€é‡: {store_data['daily_avg_sales']}")
                    return
            
            self.logger.warning("âš ï¸ æœªèƒ½æå–åˆ°æ—¥å‡é”€é‡æ•°æ®")
            
        except Exception as e:
            self.logger.error(f"âŒ æ—¥å‡é”€é‡æå–å¤±è´¥: {str(e)}")
    
    async def extract_products_from_store(self, url: str, store_id: str) -> List[Dict[str, Any]]:
        """
        ç¬¬å››æ­¥ï¼šå•†å“åˆ—è¡¨æå–åœºæ™¯ - æ”¯æŒåˆ†é¡µå’Œå®Œæ•´å­—æ®µæå–

        ä»åº—é“ºé¡µé¢æå–å•†å“åˆ—è¡¨ä¿¡æ¯ï¼Œä½¿ç”¨åˆ†é¡µå™¨å¤„ç†æ‰€æœ‰é¡µé¢çš„å•†å“ï¼Œæå–å®Œæ•´çš„å•†å“å­—æ®µä¿¡æ¯

        Args:
            url: åº—é“ºé¡µé¢URL
            store_id: åº—é“ºID

        Returns:
            List[Dict[str, Any]]: å•†å“ä¿¡æ¯åˆ—è¡¨
        """
        self.logger.info("ğŸ›ï¸ æ‰§è¡Œç¬¬å››æ­¥ï¼šå•†å“åˆ—è¡¨æå–åœºæ™¯ï¼ˆæ”¯æŒåˆ†é¡µå’Œå®Œæ•´å­—æ®µæå–ï¼‰")

        try:
            # ç­‰å¾…å•†å“è¡¨æ ¼åŠ è½½
            table_selector = "#store-products-table"
            try:
                await self.page.wait_for_selector(table_selector, timeout=10000)
                self.logger.debug("å•†å“è¡¨æ ¼å·²åŠ è½½")
            except:
                self.logger.warning("å•†å“è¡¨æ ¼ç­‰å¾…è¶…æ—¶")
                return []

            # é¦–å…ˆè·å–è¡¨å¤´ä¿¡æ¯ï¼Œç¡®å®šå„åˆ—çš„ä½ç½®
            column_mapping = await self._get_table_column_mapping()
            self.logger.info(f"ğŸ“‹ è¡¨æ ¼åˆ—æ˜ å°„: {list(column_mapping.keys())}")

            # åˆ›å»ºåˆ†é¡µå™¨å®ä¾‹
            paginator = UniversalPaginator(self.page, debug_mode=self.debug_mode)

            # åˆ›å»ºå•†å“æ•°æ®æå–å›è°ƒå‡½æ•°
            async def extract_products_on_page(root_locator) -> List[Dict[str, Any]]:
                """æ¯é¡µå•†å“æ•°æ®æå–å›è°ƒå‡½æ•° - æå–å®Œæ•´å­—æ®µä¿¡æ¯"""
                page_products = []

                try:
                    # æŸ¥æ‰¾å½“å‰é¡µçš„å•†å“è¡Œ
                    product_rows = root_locator.locator("#store-products-table tbody tr")
                    row_count = await product_rows.count()

                    if row_count == 0:
                        self.logger.warning("å½“å‰é¡µæœªæ‰¾åˆ°å•†å“è¡Œ")
                        return page_products

                    self.logger.info(f"ğŸ” å½“å‰é¡µæ‰¾åˆ° {row_count} ä¸ªå•†å“è¡Œï¼Œå¼€å§‹æå–å®Œæ•´å­—æ®µä¿¡æ¯...")

                    # å¤„ç†æ¯ä¸ªå•†å“è¡Œ
                    for row_index in range(row_count):
                        try:
                            row_locator = product_rows.nth(row_index)

                            # åˆå§‹åŒ–å®Œæ•´çš„å•†å“ä¿¡æ¯ç»“æ„
                            product_info = {
                                'row_index': len(page_products) + 1,
                                'store_id': store_id,
                                'product_link_url': '',
                                'product_id': '',
                                'category': '',  # ç±»ç›®
                                'price': '',  # å”®ä»·
                                'sales_volume': '',  # é”€é‡
                                'sales_amount': '',  # é”€å”®é¢
                                'profit_margin': '',  # æ¯›åˆ©ç‡
                                'exposure': '',  # æ›å…‰é‡
                                'product_views': '',  # äº§å“å¡æµè§ˆé‡
                                'add_to_cart_rate': '',  # åŠ è´­ç‡
                                'conversion_rate': '',  # è®¢å•è½¬åŒ–ç‡
                                'ad_cost_share': '',  # å¹¿å‘Šè´¹ç”¨ä»½é¢
                                'return_cancel_rate': '',  # é€€è´§å–æ¶ˆç‡
                                'rating': '',  # è¯„åˆ†
                                'shop_name': '',  # åº—é“º
                                'seller_type': '',  # å–å®¶ç±»å‹
                                'delivery_method': '',  # é…é€æ–¹å¼
                                'weight': '',  # é‡é‡
                                'listing_time': '',  # ä¸Šæ¶æ—¶é—´
                                'product_description': ''
                            }

                            # æå–å„åˆ—æ•°æ®
                            await self._extract_product_row_data(row_locator, product_info, column_mapping)

                            page_products.append(product_info)
                            self.logger.info(f"âœ… å•†å“ {product_info['row_index']} æå–æˆåŠŸ")

                        except Exception as e:
                            self.logger.error(f"âŒ å¤„ç†å•†å“è¡Œ {row_index + 1} å¤±è´¥: {str(e)}")
                            continue

                    # é¡µé¢å¤„ç†å®Œæˆåçš„å»¶è¿Ÿ
                    if self.request_delay > 0:
                        await asyncio.sleep(self.request_delay)

                except Exception as e:
                    self.logger.error(f"âŒ é¡µé¢å•†å“æå–å¤±è´¥: {str(e)}")

                return page_products

            # ä½¿ç”¨åˆ†é¡µå™¨æ‰§è¡Œåˆ†é¡µéå†ï¼Œé™åˆ¶å•†å“æ•°é‡
            all_products = await paginator.paginate_numbers(
                root_selector="#tab-hot-products",
                item_locator="#store-products-table tbody tr",
                on_page=extract_products_on_page,
                max_pages=None,  # ä¸é™åˆ¶é¡µæ•°ï¼Œå¤„ç†æ‰€æœ‰é¡µé¢
                wait_api_substr="hot-products"  # ç­‰å¾…APIå“åº”çš„URLç‰‡æ®µ
            )

            # é™åˆ¶å•†å“æ•°é‡åˆ°é…ç½®çš„é˜ˆå€¼
            if len(all_products) > self.max_products_per_store:
                all_products = all_products[:self.max_products_per_store]
                self.logger.info(f"ğŸ”¢ å•†å“æ•°é‡é™åˆ¶ä¸º {self.max_products_per_store} ä¸ª")

            # æ‰“å°å•†å“æ€»æ•°
            total_count = len(all_products)
            self.logger.info(f"ğŸ¯ å•†å“åˆ—è¡¨åˆ†é¡µæå–å®Œæˆï¼Œå…±æå– {total_count} ä¸ªå•†å“")
            print(f"\nğŸ“Š å•†å“æ€»æ•°ç»Ÿè®¡: {total_count} ä¸ªå•†å“")

            # æ˜¾ç¤ºå•†å“å­—æ®µç»Ÿè®¡
            if all_products:
                _display_product_statistics(all_products)

            return all_products

        except Exception as e:
            self.logger.error(f"âŒ å•†å“åˆ—è¡¨åˆ†é¡µæå–å¤±è´¥: {str(e)}")
            return []
    
    async def _process_analysis_result(self, analysis_result: Dict[str, Any], product_info: Dict[str, Any], row_index: int):
        """
        å¤„ç†DOMåˆ†æç»“æœï¼Œæå–å•†å“å…³é”®ä¿¡æ¯
        
        Args:
            analysis_result: DOMåˆ†æç»“æœ
            product_info: å•†å“ä¿¡æ¯å­—å…¸
            row_index: å•†å“è¡Œç´¢å¼•
        """
        try:
            # å¤„ç†é“¾æ¥ä¿¡æ¯
            links = analysis_result.get('links', [])
            for link in links:
                real_link = link.get('real_link')
                if real_link and not product_info.get('product_link_url'):
                    product_info['product_link_url'] = real_link
                    self.logger.debug(f"è®¾ç½®äº§å“é“¾æ¥: {real_link}")
                
                text = link.get('text', '').strip()
                if text and len(text) > 5 and not product_info.get('backend_product_link'):
                    product_info['backend_product_link'] = text
                    self.logger.debug(f"è®¾ç½®åå°å•†å“é“¾æ¥: {text[:50]}...")
            
            # å¤„ç†æ–‡æœ¬ä¿¡æ¯
            texts = analysis_result.get('texts', [])
            for text_info in texts:
                text = text_info.get('text', '').strip()
                
                # è¯†åˆ«SKU
                if text_info.get('is_potential_sku') and not product_info.get('sku'):
                    product_info['sku'] = text
                    self.logger.debug(f"è¯†åˆ«åˆ°äº§å“: {text}")
                
                # è¯†åˆ«å•†å“æè¿°
                if len(text) > 10 and not product_info.get('product_description'):
                    product_info['product_description'] = text
                    self.logger.debug(f"è®¾ç½®å•†å“æè¿°: {text[:50]}...")
            
            # å¤„ç†åŠ¨æ€å†…å®¹
            dynamic_data = analysis_result.get('dynamic_data', {})
            dynamic_links = dynamic_data.get('links', [])
            
            for link_data in dynamic_links:
                # å¤„ç†window.opené“¾æ¥
                onclick_str = link_data.get('onclick', '')
                if onclick_str and 'window.open' in onclick_str:
                    url_match = re.search(r"window\.open\(['\"]([^'\"]+)['\"]", onclick_str)
                    if url_match:
                        target_url = url_match.group(1)
                        self.logger.debug(f"å‘ç°window.opené“¾æ¥: {target_url}")
                        
                        # å®é™…æ‰“å¼€é“¾æ¥ï¼ˆå¦‚æœéœ€è¦ï¼‰
                        if self.dom_analyzer:
                            await self.dom_analyzer.open_product_link(target_url, f" (å•†å“ {row_index})")
                        
                        if not product_info.get('product_link_url'):
                            product_info['product_link_url'] = target_url
                            self.logger.debug(f"è®¾ç½®äº§å“é“¾æ¥: {target_url}")
            
        except Exception as e:
            self.logger.error(f"å¤„ç†åˆ†æç»“æœå¤±è´¥: {str(e)}")
    
    async def crawl_all_stores(self, stores_data: List[Dict[str, Any]], limit: Optional[int] = None) -> List[Dict[str, Any]]:
        """
        ç¬¬äº”æ­¥ï¼šæ‰¹é‡åº—é“ºçˆ¬å–åœºæ™¯
        
        æ‰§è¡Œæ‰¹é‡åº—é“ºæ•°æ®çˆ¬å–ï¼ŒåŒ…å«ä¸¥æ ¼çš„æ‰§è¡Œæ§åˆ¶å’Œé”™è¯¯å¤„ç†
        
        Args:
            stores_data: åº—é“ºæ•°æ®åˆ—è¡¨
            limit: é™åˆ¶å¤„ç†çš„åº—é“ºæ•°é‡
            
        Returns:
            List[Dict[str, Any]]: çˆ¬å–ç»“æœåˆ—è¡¨
        """
        self.logger.info("ğŸš€ æ‰§è¡Œç¬¬äº”æ­¥ï¼šæ‰¹é‡åº—é“ºçˆ¬å–åœºæ™¯")
        self.scenario_state['current_step'] = 'batch_crawling'
        
        results = []

        # åº”ç”¨æ•°é‡é™åˆ¶
        if limit:
            stores_data = stores_data[:limit]

        # å…¨å±€å˜é‡ï¼šåº—é“ºæ€»æ•°å’Œå•†å“æ€»æ•°
        total_stores = len(stores_data)
        total_products = 0

        print(f"\nğŸª å¼€å§‹éå†åº—é“ºï¼Œå…± {total_stores} ä¸ªåº—é“º")
        self.logger.info(f"ğŸ“Š è®¡åˆ’å¤„ç† {total_stores} ä¸ªåº—é“º")

        for index, store_info in enumerate(stores_data, 1):
            print(f"\n{'='*60}")
            print(f"ğŸª æ­£åœ¨å¤„ç†ç¬¬ {index}/{total_stores} ä¸ªåº—é“º")
            self.logger.info(f"ğŸª å¤„ç†ç¬¬ {index}/{total_stores} ä¸ªåº—é“º")

            try:
                # æå–åº—é“ºID
                store_id = self.extract_store_id_from_data(store_info)
                if not store_id:
                    print(f"âš ï¸ è·³è¿‡ç¬¬ {index} ä¸ªåº—é“ºï¼šæ— æ³•æå–åº—é“ºID")
                    self.logger.warning(f"è·³è¿‡ç¬¬ {index} ä¸ªåº—é“ºï¼šæ— æ³•æå–åº—é“ºID")
                    continue

                # æ„å»ºURL
                url = self.build_seerfar_url(store_id)
                print(f"ğŸ”— åº—é“ºé“¾æ¥: {url}")

                # æ‰§è¡Œæ•°æ®æå–ï¼ˆåŒ…å«å•†å“åˆ†é¡µé‡‡é›†ï¼‰
                result = await self.execute_store_data_extraction_scenario(url)
                results.append(result)

                # å¤„ç†å½“å‰åº—é“ºçš„å•†å“æ•°æ®
                if isinstance(result, dict) and 'products' in result:
                    store_products = result['products']
                    if store_products:
                        store_product_count = len(store_products)
                        total_products += store_product_count

                        print(f"ğŸ“Š åº—é“º {index} å•†å“æ€»æ•°: {store_product_count} ä¸ª")
                        print(f"ğŸ“ˆ ç´¯è®¡å•†å“æ€»æ•°: {total_products} ä¸ª")

                        # ç«‹å³æ‰“å°å½“å‰åº—é“ºçš„æ‰€æœ‰å•†å“å±æ€§
                        _print_store_products_details(store_products, index, store_id)

                        self.logger.info(f"âœ… åº—é“º {index} å¤„ç†å®Œæˆï¼Œæ”¶é›†åˆ° {store_product_count} ä¸ªå•†å“")
                    else:
                        print(f"âš ï¸ åº—é“º {index} æœªæ”¶é›†åˆ°å•†å“æ•°æ®")
                else:
                    print(f"âš ï¸ åº—é“º {index} æ•°æ®æ ¼å¼å¼‚å¸¸")

                # è¯·æ±‚é—´éš”
                if index < total_stores and self.request_delay > 0:
                    print(f"â³ ç­‰å¾… {self.request_delay} ç§’...")
                    self.logger.debug(f"ç­‰å¾… {self.request_delay} ç§’...")
                    await asyncio.sleep(self.request_delay)

            except Exception as e:
                print(f"âŒ å¤„ç†ç¬¬ {index} ä¸ªåº—é“ºå¤±è´¥: {str(e)}")
                self.logger.error(f"å¤„ç†ç¬¬ {index} ä¸ªåº—é“ºå¤±è´¥: {str(e)}")
                continue

        # æœ€ç»ˆç»Ÿè®¡
        print(f"\n{'='*60}")
        print(f"ğŸ‰ æ‰€æœ‰åº—é“ºéå†å®Œæˆï¼")
        print(f"ğŸ“Š åº—é“ºæ€»æ•°: {total_stores} ä¸ª")
        print(f"ğŸ“Š å•†å“æ€»æ•°: {total_products} ä¸ª")
        print(f"âœ… æˆåŠŸå¤„ç†: {len(results)} ä¸ªåº—é“º")

        self.logger.info(f"ğŸ¯ æ‰¹é‡çˆ¬å–å®Œæˆï¼ŒæˆåŠŸå¤„ç† {len(results)} ä¸ªåº—é“ºï¼Œæ€»è®¡ {total_products} ä¸ªå•†å“")
        return results

    async def _get_table_column_mapping(self) -> Dict[str, int]:
        """
        è·å–è¡¨æ ¼åˆ—æ˜ å°„ï¼Œç¡®å®šå„å­—æ®µåœ¨è¡¨æ ¼ä¸­çš„ä½ç½®

        Returns:
            Dict[str, int]: åˆ—ååˆ°åˆ—ç´¢å¼•çš„æ˜ å°„
        """
        column_mapping = {}

        try:
            # ç­‰å¾…è¡¨å¤´åŠ è½½
            header_selector = "#store-products-table thead tr th"
            await self.page.wait_for_selector(header_selector, timeout=5000)

            # è·å–æ‰€æœ‰è¡¨å¤´å…ƒç´ 
            headers = await self.page.query_selector_all(header_selector)

            for index, header in enumerate(headers):
                try:
                    # è·å–è¡¨å¤´æ–‡æœ¬å†…å®¹
                    header_text = await header.text_content()
                    if header_text:
                        header_text = header_text.strip()

                        # æ˜ å°„ä¸­æ–‡åˆ—ååˆ°è‹±æ–‡å­—æ®µå
                        field_mapping = {
                            'ç±»ç›®': 'category',
                            'å”®ä»·': 'price',
                            'é”€é‡': 'sales_volume',
                            'é”€å”®é¢': 'sales_amount',
                            'æ¯›åˆ©ç‡': 'profit_margin',
                            'æ›å…‰é‡': 'exposure',
                            'äº§å“å¡æµè§ˆé‡': 'product_views',
                            'åŠ è´­ç‡': 'add_to_cart_rate',
                            'è®¢å•è½¬åŒ–ç‡': 'conversion_rate',
                            'å¹¿å‘Šè´¹ç”¨ä»½é¢': 'ad_cost_share',
                            'é€€è´§å–æ¶ˆç‡': 'return_cancel_rate',
                            'è¯„åˆ†': 'rating',
                            'åº—é“º': 'shop_name',
                            'å–å®¶ç±»å‹': 'seller_type',
                            'é…é€æ–¹å¼': 'delivery_method',
                            'é‡é‡': 'weight',
                            'ä¸Šæ¶æ—¶é—´': 'listing_time',
                            'äº§å“': 'product_info'  # åŒ…å«äº§å“é“¾æ¥å’ŒIDçš„åˆ—
                        }

                        for chinese_name, english_field in field_mapping.items():
                            if chinese_name in header_text:
                                column_mapping[english_field] = index
                                self.logger.debug(f"æ˜ å°„åˆ— '{chinese_name}' -> ç´¢å¼• {index}")
                                break

                except Exception as e:
                    self.logger.debug(f"å¤„ç†è¡¨å¤´ {index} å¤±è´¥: {str(e)}")
                    continue

        except Exception as e:
            self.logger.warning(f"è·å–è¡¨æ ¼åˆ—æ˜ å°„å¤±è´¥: {str(e)}")

        return column_mapping

    async def _extract_product_row_data(self, row_locator, product_info: Dict[str, Any], column_mapping: Dict[str, int]):
        """
        ä»å•†å“è¡Œä¸­æå–å®Œæ•´çš„å­—æ®µæ•°æ®

        Args:
            row_locator: è¡Œå®šä½å™¨
            product_info: å•†å“ä¿¡æ¯å­—å…¸
            column_mapping: åˆ—æ˜ å°„
        """
        try:
            # è·å–æ‰€æœ‰å•å…ƒæ ¼
            cells = row_locator.locator("td")
            cell_count = await cells.count()

            # æå–å„å­—æ®µæ•°æ®
            for field_name, column_index in column_mapping.items():
                if column_index < cell_count:
                    try:
                        cell = cells.nth(column_index)

                        # ç‰¹æ®Šå¤„ç†äº§å“ä¿¡æ¯åˆ—ï¼ˆåŒ…å«é“¾æ¥ï¼‰
                        if field_name == 'product_info':
                            await self._extract_product_info_from_cell(cell, product_info)
                        else:
                            # æå–æ™®é€šæ–‡æœ¬å†…å®¹
                            text_content = await cell.text_content()
                            if text_content:
                                product_info[field_name] = text_content.strip()

                    except Exception as e:
                        self.logger.debug(f"æå–å­—æ®µ {field_name} å¤±è´¥: {str(e)}")
                        continue

        except Exception as e:
            self.logger.error(f"æå–å•†å“è¡Œæ•°æ®å¤±è´¥: {str(e)}")

    async def _extract_product_info_from_cell(self, cell_locator, product_info: Dict[str, Any]):
        """
        ä»äº§å“ä¿¡æ¯å•å…ƒæ ¼ä¸­æå–äº§å“é“¾æ¥å’ŒID - ä½¿ç”¨DOMAnalyzeræ·±åº¦åˆ†æ

        Args:
            cell_locator: å•å…ƒæ ¼å®šä½å™¨
            product_info: å•†å“ä¿¡æ¯å­—å…¸
        """
        try:
            # ä½¿ç”¨DOMAnalyzerè¿›è¡Œæ·±åº¦åˆ†æ
            if self.dom_analyzer:
                self.logger.debug("ä½¿ç”¨DOMAnalyzerè¿›è¡Œäº§å“ä¿¡æ¯æ·±åº¦åˆ†æ")

                try:
                    # å°†Locatorè½¬æ¢ä¸ºElementHandleå¯¹è±¡
                    element_handle = await cell_locator.element_handle()
                    if element_handle:
                        # ä½¿ç”¨analyze_elementæ–¹æ³•è¿›è¡Œæ·±åº¦åˆ†æ
                        analysis_result = await self.dom_analyzer.analyze_element(element_handle, f" (å•†å“ {product_info.get('row_index', 0)})")

                        if analysis_result:
                            await self._process_analysis_result(analysis_result, product_info, product_info.get('row_index', 0))
                    else:
                        self.logger.debug("æ— æ³•è·å–ElementHandleå¯¹è±¡")
                except Exception as e:
                    self.logger.debug(f"DOMAnalyzeråˆ†æå¤±è´¥: {str(e)}")

            # æ–¹æ³•1: ä½¿ç”¨å…·ä½“çš„XPathè·¯å¾„æå–äº§å“ID
            try:
                # ä½¿ç”¨ç”¨æˆ·æä¾›çš„å…·ä½“XPathè·¯å¾„æ¨¡å¼
                # åŸè·¯å¾„: //*[@id="tab-hot-products"]/div/div[2]/div[1]/div[2]/div[4]/div[2]/table/tbody/tr[1]/td[3]/div/div[2]/div[2]/span[1]
                # è½¬æ¢ä¸ºç›¸å¯¹äºå•å…ƒæ ¼çš„è·¯å¾„
                product_id_xpath = ".//div/div[2]/div[2]/span[1]"
                product_id_element = cell_locator.locator(f"xpath={product_id_xpath}")

                if await product_id_element.count() > 0:
                    product_id_text = await product_id_element.text_content()
                    if product_id_text and product_id_text.strip():
                        product_info['product_id'] = product_id_text.strip()
                        self.logger.debug(f"é€šè¿‡XPathæå–äº§å“ID: {product_info['product_id']}")
                else:
                    # å°è¯•æ›´é€šç”¨çš„spanæŸ¥æ‰¾
                    spans = cell_locator.locator("span")
                    span_count = await spans.count()
                    self.logger.debug(f"åœ¨å•å…ƒæ ¼ä¸­æ‰¾åˆ° {span_count} ä¸ªspanå…ƒç´ ")

                    # å°è¯•æŸ¥æ‰¾ç¬¬ä¸€ä¸ªåŒ…å«æ–‡æœ¬çš„span
                    for i in range(span_count):
                        span = spans.nth(i)
                        span_text = await span.text_content()
                        if span_text and span_text.strip():
                            product_info['product_id'] = span_text.strip()
                            self.logger.debug(f"é€šè¿‡é€šç”¨spanæŸ¥æ‰¾æå–äº§å“ID: {product_info['product_id']}")
                            break

            except Exception as e:
                self.logger.debug(f"XPathæ–¹å¼æå–äº§å“IDå¤±è´¥: {str(e)}")

            # æ–¹æ³•2: æŸ¥æ‰¾é“¾æ¥å…ƒç´ æå–äº§å“é“¾æ¥
            try:
                links = cell_locator.locator("a")
                link_count = await links.count()

                if link_count > 0:
                    # è·å–ç¬¬ä¸€ä¸ªé“¾æ¥
                    first_link = links.first

                    # æå–é“¾æ¥URL
                    href = await first_link.get_attribute("href")
                    if href:
                        product_info['product_link_url'] = href
                        self.logger.debug(f"æå–äº§å“é“¾æ¥: {href}")

                    # å¦‚æœè¿˜æ²¡æœ‰äº§å“IDï¼Œå°è¯•ä»é“¾æ¥æ–‡æœ¬æå–
                    if not product_info.get('product_id'):
                        link_text = await first_link.text_content()
                        if link_text:
                            product_info['product_id'] = link_text.strip()
                            self.logger.debug(f"ä»é“¾æ¥æ–‡æœ¬æå–äº§å“ID: {product_info['product_id']}")

                    # å°è¯•ä»onclickäº‹ä»¶ä¸­æå–æ›´å¤šä¿¡æ¯
                    onclick = await first_link.get_attribute("onclick")
                    if onclick and 'window.open' in onclick:
                        url_match = re.search(r"window\.open\(['\"]([^'\"]+)['\"]", onclick)
                        if url_match:
                            product_info['product_link_url'] = url_match.group(1)
                            self.logger.debug(f"ä»onclickæå–äº§å“é“¾æ¥: {product_info['product_link_url']}")

            except Exception as e:
                self.logger.debug(f"é“¾æ¥æ–¹å¼æå–å¤±è´¥: {str(e)}")

            # æ–¹æ³•3: æ·±åº¦æœç´¢äº§å“ä¿¡æ¯
            try:
                # æŸ¥æ‰¾æ‰€æœ‰spanå…ƒç´ ï¼Œå¯»æ‰¾å¯èƒ½çš„äº§å“ID
                spans = cell_locator.locator("span")
                span_count = await spans.count()

                for i in range(span_count):
                    try:
                        span = spans.nth(i)
                        span_text = await span.text_content()
                        if span_text and span_text.strip() and not product_info.get('product_id'):
                            # ç®€å•çš„äº§å“IDè¯†åˆ«é€»è¾‘
                            text = span_text.strip()
                            if len(text) > 3 and (text.isalnum() or '-' in text or '_' in text):
                                product_info['product_id'] = text
                                self.logger.debug(f"æ·±åº¦æœç´¢æ‰¾åˆ°äº§å“ID: {text}")
                                break
                    except:
                        continue

            except Exception as e:
                self.logger.debug(f"æ·±åº¦æœç´¢å¤±è´¥: {str(e)}")

            # æå–äº§å“æè¿°
            try:
                description = await cell_locator.text_content()
                if description:
                    product_info['product_description'] = description.strip()
            except Exception as e:
                self.logger.debug(f"æå–äº§å“æè¿°å¤±è´¥: {str(e)}")

            # è®°å½•æå–ç»“æœ
            if product_info.get('product_id'):
                self.logger.info(f"âœ… æˆåŠŸæå–äº§å“ID: {product_info['product_id']}")
            else:
                self.logger.warning("âš ï¸ æœªèƒ½æå–åˆ°äº§å“ID")

            if product_info.get('product_link_url'):
                self.logger.info(f"âœ… æˆåŠŸæå–äº§å“é“¾æ¥: {product_info['product_link_url']}")
            else:
                self.logger.warning("âš ï¸ æœªèƒ½æå–åˆ°äº§å“é“¾æ¥")

        except Exception as e:
            self.logger.error(f"âŒ æå–äº§å“ä¿¡æ¯å¤±è´¥: {str(e)}")

    def _print_product_links(self, products: List[Dict[str, Any]], store_name: str = ""):
        """
        æ‰“å°å•†å“åˆ—è¡¨çš„äº§å“é“¾æ¥ï¼ˆä¸å®é™…æ‰“å¼€ï¼‰

        Args:
            products: å•†å“åˆ—è¡¨
            store_name: åº—é“ºåç§°
        """
        if not products:
            return

        store_info = f" ({store_name})" if store_name else ""
        print(f"\nğŸ”— {store_info} å•†å“é“¾æ¥åˆ—è¡¨ ({len(products)} ä¸ªå•†å“):")

        valid_links = 0

        for index, product in enumerate(products, 1):
            try:
                product_url = product.get('product_link_url', '').strip()
                product_id = product.get('product_id', '').strip()
                category = product.get('category', '').strip()
                price = product.get('price', '').strip()

                if product_url:
                    valid_links += 1
                    # æ‰“å°é“¾æ¥ä¿¡æ¯ï¼Œä¸å®é™…æ‰“å¼€
                    print(f"   {index:3d}. {product_id} | {category} | {price} | {product_url}")
                    self.logger.debug(f"è®°å½•å•†å“é“¾æ¥: {product_id} - {product_url}")
                else:
                    print(f"   {index:3d}. {product_id} | {category} | {price} | [æ— é“¾æ¥]")

            except Exception as e:
                self.logger.error(f"âŒ å¤„ç†ç¬¬ {index} ä¸ªå•†å“é“¾æ¥å¤±è´¥: {str(e)}")
                continue

        print(f"ğŸ“Š é“¾æ¥ç»Ÿè®¡: {valid_links}/{len(products)} ä¸ªå•†å“æœ‰æœ‰æ•ˆé“¾æ¥")
        self.logger.info(f"ğŸ¯ å•†å“é“¾æ¥æ‰“å°å®Œæˆ: {valid_links}/{len(products)} æœ‰æ•ˆé“¾æ¥")


