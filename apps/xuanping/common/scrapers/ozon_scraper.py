"""
OZONå¹³å°æŠ“å–å™¨

è´Ÿè´£ä»OZONå¹³å°æŠ“å–å•†å“ä»·æ ¼ä¿¡æ¯å’Œè·Ÿå–åº—é“ºæ•°æ®ã€‚
åŸºäºæ–°çš„browser_serviceæ¶æ„ã€‚
"""

import asyncio
import logging
import time
from typing import Dict, Any, List, Optional, Tuple

from .xuanping_browser_service import XuanpingBrowserServiceSync
from ..models import ProductInfo, CompetitorStore, clean_price_string, ScrapingResult
from ..config import GoodStoreSelectorConfig


class OzonScraper:
    """OZONå¹³å°æŠ“å–å™¨ - åŸºäºbrowser_serviceæ¶æ„"""
    
    def __init__(self, config: Optional[GoodStoreSelectorConfig] = None):
        """åˆå§‹åŒ–OZONæŠ“å–å™¨"""
        self.config = config or GoodStoreSelectorConfig()
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        self.base_url = self.config.scraping.ozon_base_url
        
        # åˆ›å»ºæµè§ˆå™¨æœåŠ¡
        self.browser_service = XuanpingBrowserServiceSync()
    
    def scrape_product_prices(self, product_url: str) -> ScrapingResult:
        """
        æŠ“å–å•†å“ä»·æ ¼ä¿¡æ¯
        
        Args:
            product_url: å•†å“URL
            
        Returns:
            ScrapingResult: æŠ“å–ç»“æœï¼ŒåŒ…å«ä»·æ ¼ä¿¡æ¯
        """
        start_time = time.time()
        
        try:
            # ä½¿ç”¨æµè§ˆå™¨æœåŠ¡æŠ“å–æ•°æ®
            async def extract_price_data(browser_service):
                """å¼‚æ­¥æå–ä»·æ ¼æ•°æ®"""
                try:
                    # ç­‰å¾…é¡µé¢åŠ è½½
                    await asyncio.sleep(2)
                    
                    # è·å–é¡µé¢å†…å®¹
                    page_content = await browser_service.get_page_content()
                    
                    # è§£æä»·æ ¼ä¿¡æ¯
                    price_data = await self._extract_price_data_from_content(page_content)
                    
                    return price_data
                    
                except Exception as e:
                    self.logger.error(f"æå–ä»·æ ¼æ•°æ®å¤±è´¥: {e}")
                    return {}
            
            # ä½¿ç”¨æµè§ˆå™¨æœåŠ¡æŠ“å–é¡µé¢æ•°æ®
            result = self.browser_service.scrape_page_data(product_url, extract_price_data)
            
            if result.success and result.data:
                return ScrapingResult(
                    success=True,
                    data=result.data,
                    execution_time=time.time() - start_time
                )
            else:
                return ScrapingResult(
                    success=False,
                    data={},
                    error_message=result.error_message or "æœªèƒ½æå–åˆ°ä»·æ ¼ä¿¡æ¯",
                    execution_time=time.time() - start_time
                )
            
        except Exception as e:
            self.logger.error(f"æŠ“å–å•†å“ä»·æ ¼å¤±è´¥: {e}")
            return ScrapingResult(
                success=False,
                data={},
                error_message=str(e),
                execution_time=time.time() - start_time
            )
    
    def scrape_competitor_stores(self, product_url: str, max_competitors: int = 10) -> ScrapingResult:
        """
        æŠ“å–è·Ÿå–åº—é“ºä¿¡æ¯
        
        Args:
            product_url: å•†å“URL
            max_competitors: æœ€å¤§è·Ÿå–åº—é“ºæ•°é‡
            
        Returns:
            ScrapingResult: æŠ“å–ç»“æœï¼ŒåŒ…å«è·Ÿå–åº—é“ºä¿¡æ¯
        """
        start_time = time.time()
        
        try:
            async def extract_competitor_data(browser_service):
                """å¼‚æ­¥æå–è·Ÿå–åº—é“ºæ•°æ®"""
                try:
                    # ç­‰å¾…é¡µé¢åŠ è½½
                    await asyncio.sleep(2)
                    
                    # å°è¯•æ‰“å¼€è·Ÿå–æµ®å±‚
                    await self._open_competitor_popup_async(browser_service)
                    
                    # è·å–é¡µé¢å†…å®¹
                    page_content = await browser_service.get_page_content()
                    
                    # è§£æè·Ÿå–åº—é“ºä¿¡æ¯
                    competitors = await self._extract_competitor_stores_from_content(page_content, max_competitors)
                    
                    return {'competitors': competitors, 'total_count': len(competitors)}
                    
                except Exception as e:
                    self.logger.error(f"æå–è·Ÿå–åº—é“ºæ•°æ®å¤±è´¥: {e}")
                    return {'competitors': [], 'total_count': 0}
            
            # ä½¿ç”¨æµè§ˆå™¨æœåŠ¡æŠ“å–é¡µé¢æ•°æ®
            result = self.browser_service.scrape_page_data(product_url, extract_competitor_data)
            
            if result.success:
                return ScrapingResult(
                    success=True,
                    data=result.data,
                    execution_time=time.time() - start_time
                )
            else:
                return ScrapingResult(
                    success=False,
                    data={'competitors': [], 'total_count': 0},
                    error_message=result.error_message or "æ— æ³•æŠ“å–è·Ÿå–åº—é“ºä¿¡æ¯",
                    execution_time=time.time() - start_time
                )
            
        except Exception as e:
            self.logger.error(f"æŠ“å–è·Ÿå–åº—é“ºä¿¡æ¯å¤±è´¥: {e}")
            return ScrapingResult(
                success=False,
                data={'competitors': [], 'total_count': 0},
                error_message=str(e),
                execution_time=time.time() - start_time
            )
    
    def scrape(self, product_url: str, include_competitors: bool = False, **kwargs) -> ScrapingResult:
        """
        ç»¼åˆæŠ“å–å•†å“ä¿¡æ¯
        
        Args:
            product_url: å•†å“URL
            include_competitors: æ˜¯å¦åŒ…å«è·Ÿå–åº—é“ºä¿¡æ¯
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            ScrapingResult: æŠ“å–ç»“æœ
        """
        start_time = time.time()
        
        try:
            # æŠ“å–ä»·æ ¼ä¿¡æ¯
            price_result = self.scrape_product_prices(product_url)
            if not price_result.success:
                return price_result
            
            result_data = {
                'product_url': product_url,
                'price_data': price_result.data
            }
            
            # å¦‚æœéœ€è¦ï¼ŒæŠ“å–è·Ÿå–åº—é“ºä¿¡æ¯
            if include_competitors:
                competitors_result = self.scrape_competitor_stores(product_url)
                if competitors_result.success:
                    result_data['competitors'] = competitors_result.data['competitors']
                else:
                    self.logger.warning(f"æŠ“å–è·Ÿå–åº—é“ºä¿¡æ¯å¤±è´¥: {competitors_result.error_message}")
                    result_data['competitors'] = []
            
            return ScrapingResult(
                success=True,
                data=result_data,
                execution_time=time.time() - start_time
            )
            
        except Exception as e:
            self.logger.error(f"ç»¼åˆæŠ“å–å•†å“ä¿¡æ¯å¤±è´¥: {e}")
            return ScrapingResult(
                success=False,
                data={},
                error_message=str(e),
                execution_time=time.time() - start_time
            )
    
    async def _extract_price_data_from_content(self, page_content: str) -> Dict[str, Any]:
        """
        ä»é¡µé¢å†…å®¹ä¸­æå–ä»·æ ¼æ•°æ®
        
        Args:
            page_content: é¡µé¢HTMLå†…å®¹
            
        Returns:
            Dict[str, Any]: ä»·æ ¼æ•°æ®
        """
        price_data = {}
        
        try:
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(page_content, 'html.parser')
            
            # ğŸ–¼ï¸ æå–å•†å“å›¾ç‰‡åœ°å€
            image_url = await self._extract_product_image_from_content(soup)
            if image_url:
                price_data['image_url'] = image_url

            # ğŸ“Š æå–è·Ÿå–æ•°é‡
            competitor_count = await self._extract_competitor_count_from_content(soup)
            if competitor_count is not None:
                price_data['competitor_count'] = competitor_count

            # æŠ“å–ç»¿æ ‡ä»·æ ¼ï¼ˆä¿ƒé”€ä»·æ ¼ï¼‰
            green_price_selectors = [
                "[data-widget='webPrice'] .price_discount",
                ".green-price",
                "[class*='discount']",
                "[class*='sale']"
            ]
            
            for selector in green_price_selectors:
                element = soup.select_one(selector)
                if element:
                    green_text = element.get_text(strip=True)
                    price = clean_price_string(green_text)
                    if price and price > 0:
                        price_data['green_price'] = price
                        break
            
            # æŠ“å–é»‘æ ‡ä»·æ ¼ï¼ˆåŸä»·ï¼‰
            black_price_selectors = [
                "[data-widget='webPrice'] .price_original",
                ".black-price",
                "[class*='original']",
                "[class*='regular']"
            ]
            
            for selector in black_price_selectors:
                element = soup.select_one(selector)
                if element:
                    black_text = element.get_text(strip=True)
                    price = clean_price_string(black_text)
                    if price and price > 0:
                        price_data['black_price'] = price
                        break
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç»¿æ ‡ä»·æ ¼ï¼Œå°è¯•æŸ¥æ‰¾ä¸»è¦ä»·æ ¼
            if 'green_price' not in price_data:
                main_price_selectors = [
                    "[data-widget='webPrice'] span",
                    ".price span",
                    "[class*='price'] span"
                ]
                
                for selector in main_price_selectors:
                    elements = soup.select(selector)
                    for element in elements:
                        main_text = element.get_text(strip=True)
                        price = clean_price_string(main_text)
                        if price and price > 0:
                            price_data['green_price'] = price
                            break
                    if 'green_price' in price_data:
                        break
            
            # å¦‚æœæ²¡æœ‰é»‘æ ‡ä»·æ ¼ï¼Œä½¿ç”¨ç»¿æ ‡ä»·æ ¼ä½œä¸ºé»‘æ ‡ä»·æ ¼
            if 'black_price' not in price_data and 'green_price' in price_data:
                price_data['black_price'] = price_data['green_price']
            
            # å°è¯•é€šç”¨æ–¹æ³•æå–ä»·æ ¼
            if not price_data:
                price_data = await self._extract_price_data_generic(soup)
            
            self.logger.debug(f"æå–çš„ä»·æ ¼æ•°æ®: {price_data}")
            return price_data
            
        except Exception as e:
            self.logger.error(f"ä»é¡µé¢å†…å®¹æå–ä»·æ ¼æ•°æ®å¤±è´¥: {e}")
            return {}
    
    async def _extract_product_image_from_content(self, soup) -> Optional[str]:
        """
        ä»é¡µé¢å†…å®¹ä¸­æå–å•†å“å›¾ç‰‡åœ°å€

        Args:
            soup: BeautifulSoupå¯¹è±¡

        Returns:
            str: å•†å“å›¾ç‰‡URLï¼Œå¦‚æœæå–å¤±è´¥è¿”å›None
        """
        try:
            # ğŸ–¼ï¸ ä½¿ç”¨ç”¨æˆ·æä¾›çš„ç²¾ç¡®XPathå¯¹åº”çš„CSSé€‰æ‹©å™¨
            # XPath: //*[@id="layoutPage"]/div[1]/div[3]/div[3]/div[1]/div[1]/div[1]/div/div/div/div[1]/div/div/div[1]/div[1]/div/div/div[2]/div/div/div/img
            image_selectors = [
                "#layoutPage > div:nth-child(1) > div:nth-child(3) > div:nth-child(3) > div:nth-child(1) > div:nth-child(1) > div:nth-child(1) > div > div > div > div:nth-child(1) > div > div > div:nth-child(1) > div:nth-child(1) > div > div > div:nth-child(2) > div > div > div > img",
                "[class*='pdp_y3']",  # ä»ç”¨æˆ·æä¾›çš„HTMLä¸­æå–çš„class
                "[class*='b95_3_3-a']",  # å¤‡ç”¨classé€‰æ‹©å™¨
                "img[src*='multimedia']",  # é€šç”¨çš„OZONå›¾ç‰‡é€‰æ‹©å™¨
                "img[src*='ozone.ru']"  # æ›´é€šç”¨çš„é€‰æ‹©å™¨
            ]

            for selector in image_selectors:
                img_element = soup.select_one(selector)
                if img_element:
                    # è·å–srcå±æ€§
                    src = img_element.get('src')
                    if src:
                        # ğŸ”§ å°†wc50æˆ–wc100æ›¿æ¢ä¸ºwc1000è·å–é«˜æ¸…å›¾ç‰‡
                        high_res_url = self._convert_to_high_res_image(src)
                        self.logger.info(f"âœ… æˆåŠŸæå–å•†å“å›¾ç‰‡: {high_res_url}")
                        return high_res_url

            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•é€šç”¨æ–¹æ³•
            return await self._extract_image_generic(soup)

        except Exception as e:
            self.logger.error(f"æå–å•†å“å›¾ç‰‡å¤±è´¥: {e}")
            return None

    def _convert_to_high_res_image(self, image_url: str) -> str:
        """
        å°†å›¾ç‰‡URLè½¬æ¢ä¸ºé«˜æ¸…ç‰ˆæœ¬

        Args:
            image_url: åŸå§‹å›¾ç‰‡URL

        Returns:
            str: é«˜æ¸…å›¾ç‰‡URL
        """
        try:
            import re
            # å°†wc50æˆ–wc100æ›¿æ¢ä¸ºwc1000
            high_res_url = re.sub(r'/wc\d+/', '/wc1000/', image_url)
            return high_res_url
        except Exception as e:
            self.logger.warning(f"è½¬æ¢é«˜æ¸…å›¾ç‰‡URLå¤±è´¥: {e}")
            return image_url

    async def _extract_competitor_count_from_content(self, soup) -> Optional[int]:
        """
        ä»é¡µé¢å†…å®¹ä¸­æå–è·Ÿå–æ•°é‡

        Args:
            soup: BeautifulSoupå¯¹è±¡

        Returns:
            int: è·Ÿå–æ•°é‡ï¼Œå¦‚æœæå–å¤±è´¥è¿”å›None
        """
        try:
            # ğŸ“Š ä½¿ç”¨ç”¨æˆ·æä¾›çš„ç²¾ç¡®XPathå¯¹åº”çš„CSSé€‰æ‹©å™¨
            # XPath: //*[@id="product-preview-info"]/div[7]/div[3]/span
            competitor_count_selectors = [
                "#product-preview-info > div:nth-child(7) > div:nth-child(3) > span",
                "#product-preview-info div:nth-child(7) div:nth-child(3) span",
                "[id='product-preview-info'] div:nth-child(7) div:nth-child(3) span",
                # å¤‡ç”¨é€‰æ‹©å™¨
                "[class*='competitor'] span",
                "[class*='seller'] span",
                "span[class*='count']"
            ]

            for selector in competitor_count_selectors:
                element = soup.select_one(selector)
                if element:
                    text = element.get_text(strip=True)
                    if text:
                        # æå–æ•°å­—
                        import re
                        numbers = re.findall(r'\d+', text)
                        if numbers:
                            count = int(numbers[0])
                            self.logger.info(f"âœ… æˆåŠŸæå–è·Ÿå–æ•°é‡: {count}")
                            return count

            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•é€šç”¨æ–¹æ³•
            return await self._extract_competitor_count_generic(soup)

        except Exception as e:
            self.logger.error(f"æå–è·Ÿå–æ•°é‡å¤±è´¥: {e}")
            return None

    async def _extract_competitor_count_generic(self, soup) -> Optional[int]:
        """
        é€šç”¨æ–¹æ³•æå–è·Ÿå–æ•°é‡

        Args:
            soup: BeautifulSoupå¯¹è±¡

        Returns:
            int: è·Ÿå–æ•°é‡
        """
        try:
            # æŸ¥æ‰¾åŒ…å«è·Ÿå–ç›¸å…³å…³é”®è¯çš„å…ƒç´ 
            keywords = ['è·Ÿå–', 'seller', 'competitor', 'offer', 'å–å®¶']

            for keyword in keywords:
                elements = soup.find_all(text=lambda text: text and keyword in text.lower())
                for element in elements:
                    parent = element.parent if hasattr(element, 'parent') else None
                    if parent:
                        # åœ¨çˆ¶å…ƒç´ åŠå…¶å…„å¼Ÿå…ƒç´ ä¸­æŸ¥æ‰¾æ•°å­—
                        siblings = parent.find_next_siblings() + parent.find_previous_siblings()
                        for sibling in siblings[:3]:  # é™åˆ¶æŸ¥æ‰¾èŒƒå›´
                            text = sibling.get_text(strip=True)
                            if text:
                                import re
                                numbers = re.findall(r'\d+', text)
                                if numbers:
                                    count = int(numbers[0])
                                    if 0 <= count <= 1000:  # åˆç†çš„è·Ÿå–æ•°é‡èŒƒå›´
                                        return count

            return 0  # é»˜è®¤è¿”å›0è¡¨ç¤ºæ²¡æœ‰è·Ÿå–

        except Exception as e:
            self.logger.error(f"é€šç”¨æ–¹æ³•æå–è·Ÿå–æ•°é‡å¤±è´¥: {e}")
            return 0

    async def _extract_image_generic(self, soup) -> Optional[str]:
        """
        é€šç”¨æ–¹æ³•æå–å•†å“å›¾ç‰‡

        Args:
            soup: BeautifulSoupå¯¹è±¡

        Returns:
            str: å›¾ç‰‡URL
        """
        try:
            # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„å•†å“å›¾ç‰‡
            img_elements = soup.find_all('img')

            for img in img_elements:
                src = img.get('src')
                if src and ('multimedia' in src or 'ozone.ru' in src):
                    # è¿‡æ»¤æ‰æ˜æ˜¾ä¸æ˜¯å•†å“å›¾ç‰‡çš„URL
                    if any(keyword in src.lower() for keyword in ['logo', 'icon', 'banner', 'avatar']):
                        continue

                    # è½¬æ¢ä¸ºé«˜æ¸…ç‰ˆæœ¬
                    high_res_url = self._convert_to_high_res_image(src)
                    return high_res_url

            return None

        except Exception as e:
            self.logger.error(f"é€šç”¨æ–¹æ³•æå–å•†å“å›¾ç‰‡å¤±è´¥: {e}")
            return None

    async def _extract_price_data_generic(self, soup) -> Dict[str, Any]:
        """
        é€šç”¨æ–¹æ³•æå–ä»·æ ¼æ•°æ®
        
        Args:
            soup: BeautifulSoupå¯¹è±¡
            
        Returns:
            Dict[str, Any]: ä»·æ ¼æ•°æ®
        """
        price_data = {}
        
        try:
            # æŸ¥æ‰¾æ‰€æœ‰åŒ…å«ä»·æ ¼ç¬¦å·çš„å…ƒç´ 
            price_elements = soup.find_all(text=lambda text: text and ('â‚½' in text or 'Ñ€ÑƒĞ±' in text))
            
            prices = []
            for text in price_elements[:10]:  # é™åˆ¶æ£€æŸ¥å‰10ä¸ªå…ƒç´ 
                price = clean_price_string(str(text))
                if price and price > 0:
                    prices.append(price)
            
            if prices:
                # é€šå¸¸ç¬¬ä¸€ä¸ªä»·æ ¼æ˜¯å½“å‰ä»·æ ¼ï¼ˆç»¿æ ‡ï¼‰ï¼Œæœ€é«˜ä»·æ ¼å¯èƒ½æ˜¯åŸä»·ï¼ˆé»‘æ ‡ï¼‰
                prices.sort()
                price_data['green_price'] = prices[0]
                if len(prices) > 1:
                    price_data['black_price'] = prices[-1]
                else:
                    price_data['black_price'] = prices[0]
            
            return price_data
            
        except Exception as e:
            self.logger.error(f"é€šç”¨æ–¹æ³•æå–ä»·æ ¼æ•°æ®å¤±è´¥: {e}")
            return {}
    
    async def _open_competitor_popup_async(self, browser_service):
        """
        å¼‚æ­¥æ‰“å¼€è·Ÿå–åº—é“ºæµ®å±‚
        
        Args:
            browser_service: æµè§ˆå™¨æœåŠ¡
        """
        try:
            # è¿™é‡Œå¯ä»¥æ·»åŠ ç‚¹å‡»æ“ä½œæ¥æ‰“å¼€è·Ÿå–æµ®å±‚
            # ç”±äºæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯é¡µé¢å†…å®¹è§£æï¼Œæš‚æ—¶è·³è¿‡ç‚¹å‡»æ“ä½œ
            await asyncio.sleep(1)
            self.logger.debug("å°è¯•æ‰“å¼€è·Ÿå–åº—é“ºæµ®å±‚")
            
        except Exception as e:
            self.logger.warning(f"æ‰“å¼€è·Ÿå–åº—é“ºæµ®å±‚å¤±è´¥: {e}")
    
    async def _extract_competitor_stores_from_content(self, page_content: str, max_competitors: int) -> List[Dict[str, Any]]:
        """
        ä»é¡µé¢å†…å®¹ä¸­æå–è·Ÿå–åº—é“ºä¿¡æ¯
        
        Args:
            page_content: é¡µé¢HTMLå†…å®¹
            max_competitors: æœ€å¤§è·Ÿå–åº—é“ºæ•°é‡
            
        Returns:
            List[Dict[str, Any]]: è·Ÿå–åº—é“ºåˆ—è¡¨
        """
        competitors = []
        
        try:
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(page_content, 'html.parser')
            
            # æŸ¥æ‰¾è·Ÿå–åº—é“ºåˆ—è¡¨
            competitor_selectors = [
                "[class*='seller'] [class*='item']",
                "[class*='competitor'] [class*='item']",
                "[class*='offer'] [class*='item']",
                "li[class*='seller']",
                "div[class*='seller']"
            ]
            
            competitor_elements = []
            for selector in competitor_selectors:
                competitor_elements = soup.select(selector)
                if competitor_elements:
                    break
            
            # æå–åº—é“ºä¿¡æ¯
            for i, element in enumerate(competitor_elements[:max_competitors]):
                try:
                    competitor_data = await self._extract_competitor_from_element(element, i + 1)
                    if competitor_data:
                        competitors.append(competitor_data)
                        
                except Exception as e:
                    self.logger.warning(f"æå–ç¬¬{i+1}ä¸ªè·Ÿå–åº—é“ºä¿¡æ¯å¤±è´¥: {e}")
                    continue
            
            self.logger.info(f"æˆåŠŸæå–{len(competitors)}ä¸ªè·Ÿå–åº—é“ºä¿¡æ¯")
            return competitors
            
        except Exception as e:
            self.logger.error(f"ä»é¡µé¢å†…å®¹æå–è·Ÿå–åº—é“ºåˆ—è¡¨å¤±è´¥: {e}")
            return []
    
    async def _extract_competitor_from_element(self, element, ranking: int) -> Optional[Dict[str, Any]]:
        """
        ä»å…ƒç´ ä¸­æå–è·Ÿå–åº—é“ºä¿¡æ¯
        
        Args:
            element: åº—é“ºå…ƒç´ 
            ranking: æ’å
            
        Returns:
            Dict[str, Any]: åº—é“ºä¿¡æ¯
        """
        try:
            competitor_data = {
                'ranking': ranking
            }
            
            # æå–åº—é“ºåç§°
            name_selectors = [
                "[class*='name']",
                "[class*='seller']",
                "[class*='store']"
            ]
            
            for selector in name_selectors:
                name_element = element.select_one(selector)
                if name_element:
                    competitor_data['store_name'] = name_element.get_text(strip=True)
                    break
            
            # æå–ä»·æ ¼
            price_selectors = [
                "[class*='price']",
                "[class*='cost']"
            ]
            
            for selector in price_selectors:
                price_element = element.select_one(selector)
                if price_element:
                    price_text = price_element.get_text(strip=True)
                    price = clean_price_string(price_text)
                    if price and price > 0:
                        competitor_data['price'] = price
                        break
            
            # æå–åº—é“ºIDï¼ˆå¦‚æœæœ‰é“¾æ¥ï¼‰
            link_element = element.select_one("a")
            if link_element and link_element.get('href'):
                href = link_element.get('href')
                # ä»URLä¸­æå–åº—é“ºID
                import re
                store_id_match = re.search(r'seller[/_](\d+)', href)
                if store_id_match:
                    competitor_data['store_id'] = store_id_match.group(1)
                else:
                    competitor_data['store_id'] = f"store_{ranking}"
            else:
                competitor_data['store_id'] = f"store_{ranking}"
            
            # éªŒè¯æ•°æ®å®Œæ•´æ€§
            if competitor_data.get('store_id'):
                return competitor_data
            else:
                return None
                
        except Exception as e:
            self.logger.warning(f"ä»å…ƒç´ æå–è·Ÿå–åº—é“ºä¿¡æ¯å¤±è´¥: {e}")
            return None
    
    def determine_real_price(self, green_price: Optional[float], 
                           black_price: Optional[float], 
                           competitor_price: Optional[float]) -> Tuple[float, float]:
        """
        æ ¹æ®ä»·æ ¼æ¯”è¾ƒé€»è¾‘ç¡®å®šçœŸå®ä»·æ ¼
        
        Args:
            green_price: ç»¿æ ‡ä»·æ ¼
            black_price: é»‘æ ‡ä»·æ ¼
            competitor_price: è·Ÿå–ä»·æ ¼
            
        Returns:
            Tuple[float, float]: (æœ€ç»ˆç»¿æ ‡ä»·æ ¼, æœ€ç»ˆé»‘æ ‡ä»·æ ¼)
        """
        try:
            # å¦‚æœæ²¡æœ‰ç»¿æ ‡ä»·æ ¼ï¼Œä½¿ç”¨é»‘æ ‡ä»·æ ¼
            if not green_price and black_price:
                return black_price, black_price
            
            # å¦‚æœæ²¡æœ‰è·Ÿå–ä»·æ ¼ï¼Œç›´æ¥è¿”å›åŸä»·æ ¼
            if not competitor_price:
                return green_price or 0, black_price or 0
            
            # ä»·æ ¼æ¯”è¾ƒé€»è¾‘
            if green_price and competitor_price:
                if green_price <= competitor_price:
                    # åˆ†æ”¯1ï¼šä½¿ç”¨å½“å‰å•†å“è¯¦æƒ…é¡µçš„ä»·æ ¼
                    return green_price, black_price or green_price
                else:
                    # åˆ†æ”¯2ï¼šä½¿ç”¨è·Ÿå–ä»·æ ¼ä½œä¸ºæ›´ä½çš„ä»·æ ¼
                    return competitor_price, black_price or green_price
            
            return green_price or 0, black_price or 0
            
        except Exception as e:
            self.logger.error(f"ç¡®å®šçœŸå®ä»·æ ¼å¤±è´¥: {e}")
            return green_price or 0, black_price or 0
    
    def close(self):
        """å…³é—­æŠ“å–å™¨"""
        try:
            if hasattr(self, 'browser_service') and self.browser_service:
                self.browser_service.close()
                self.logger.debug("OzonScraper æµè§ˆå™¨æœåŠ¡å·²å…³é—­")
        except Exception as e:
            self.logger.warning(f"å…³é—­ OzonScraper æ—¶å‡ºç°è­¦å‘Š: {e}")

    def __enter__(self):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        self.browser_service.__enter__()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        self.browser_service.__exit__(exc_type, exc_val, exc_tb)