"""
OZONå¹³å°æŠ“å–å™¨

è´Ÿè´£ä»OZONå¹³å°æŠ“å–å•†å“ä»·æ ¼ä¿¡æ¯å’Œè·Ÿå–åº—é“ºæ•°æ®ã€‚
åŸºäºæ–°çš„browser_serviceæ¶æ„ã€‚
"""

import asyncio
import logging
import time
from typing import Dict, Any, List, Optional, Tuple

from .xuanping_browser_service import XuanpingBrowserServiceSync
from .competitor_scraper import CompetitorScraper
from ..models import ProductInfo, CompetitorStore, clean_price_string, ScrapingResult
from ..config import GoodStoreSelectorConfig


class OzonScraper:
    """OZONå¹³å°æŠ“å–å™¨ - åŸºäºbrowser_serviceæ¶æ„"""
    
    def __init__(self, config: Optional[GoodStoreSelectorConfig] = None):
        """åˆå§‹åŒ–OZONæŠ“å–å™¨"""
        self.config = config or GoodStoreSelectorConfig()
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        self.base_url = self.config.scraping.ozon_base_url
        
        # åˆ›å»ºæµè§ˆå™¨æœåŠ¡
        self.browser_service = XuanpingBrowserServiceSync()

        # åˆ›å»ºè·Ÿå–æŠ“å–å™¨
        self.competitor_scraper = CompetitorScraper()

    def scrape_product_prices(self, product_url: str) -> ScrapingResult:
        """
        æŠ“å–å•†å“ä»·æ ¼ä¿¡æ¯
        
        Args:
            product_url: å•†å“URL
            
        Returns:
            ScrapingResult: æŠ“å–ç»“æœï¼ŒåŒ…å«ä»·æ ¼ä¿¡æ¯
        """
        start_time = time.time()
        
        try:
            # ä½¿ç”¨æµè§ˆå™¨æœåŠ¡æŠ“å–æ•°æ®
            async def extract_price_data(browser_service):
                """å¼‚æ­¥æå–ä»·æ ¼æ•°æ®"""
                try:
                    # ç­‰å¾…é¡µé¢åŠ è½½
                    await asyncio.sleep(2)

                    # è·å–é¡µé¢å†…å®¹
                    page_content = await browser_service.get_page_content()
                    if not page_content:
                        self.logger.error("æœªèƒ½è·å–é¡µé¢å†…å®¹")
                        return {}

                    # è§£æä»·æ ¼ä¿¡æ¯ - ä¿®å¤ï¼šæ”¹ä¸ºåŒæ­¥è°ƒç”¨
                    price_data = self._extract_price_data_from_content_sync(page_content)

                    return price_data

                except Exception as e:
                    self.logger.error(f"æå–ä»·æ ¼æ•°æ®å¤±è´¥: {e}")
                    return {}

            # ä½¿ç”¨æµè§ˆå™¨æœåŠ¡æŠ“å–é¡µé¢æ•°æ®
            result = self.browser_service.scrape_page_data(product_url, extract_price_data)

            if result.success and result.data:
                return ScrapingResult(
                    success=True,
                    data=result.data,
                    execution_time=time.time() - start_time
                )
            else:
                return ScrapingResult(
                    success=False,
                    data={},
                    error_message=result.error_message or "æœªèƒ½æå–åˆ°ä»·æ ¼ä¿¡æ¯",
                    execution_time=time.time() - start_time
                )

        except Exception as e:
            self.logger.error(f"æŠ“å–å•†å“ä»·æ ¼å¤±è´¥: {e}")
            return ScrapingResult(
                success=False,
                data={},
                error_message=str(e),
                execution_time=time.time() - start_time
            )
    
    def scrape_competitor_stores(self, product_url: str, max_competitors: int = 15) -> ScrapingResult:
        """
        æŠ“å–è·Ÿå–åº—é“ºä¿¡æ¯

        Args:
            product_url: å•†å“URL
            max_competitors: æœ€å¤§è·Ÿå–åº—é“ºæ•°é‡ï¼Œé»˜è®¤15ä¸ªï¼ˆåŸé»˜è®¤10ä¸ªï¼‰
            
        Returns:
            ScrapingResult: æŠ“å–ç»“æœï¼ŒåŒ…å«è·Ÿå–åº—é“ºä¿¡æ¯
        """
        start_time = time.time()
        
        try:
            async def extract_competitor_data(browser_service):
                """å¼‚æ­¥æå–è·Ÿå–åº—é“ºæ•°æ®"""
                try:
                    # ç­‰å¾…é¡µé¢åŠ è½½
                    await asyncio.sleep(2)
                    
                    # å°è¯•æ‰“å¼€è·Ÿå–æµ®å±‚
                    await self._open_competitor_popup_async(browser_service)
                    
                    # è·å–é¡µé¢å†…å®¹
                    page_content = await browser_service.get_page_content()
                    
                    # è§£æè·Ÿå–åº—é“ºä¿¡æ¯ - ä¿®å¤ï¼šä½¿ç”¨CompetitorScraper
                    competitors = await self.competitor_scraper.extract_competitors_from_content(page_content, max_competitors)
                    
                    return {'competitors': competitors, 'total_count': len(competitors)}
                    
                except Exception as e:
                    self.logger.error(f"æå–è·Ÿå–åº—é“ºæ•°æ®å¤±è´¥: {e}")
                    return {'competitors': [], 'total_count': 0}
            
            # ä½¿ç”¨æµè§ˆå™¨æœåŠ¡æŠ“å–é¡µé¢æ•°æ®
            result = self.browser_service.scrape_page_data(product_url, extract_competitor_data)
            
            if result.success:
                return ScrapingResult(
                    success=True,
                    data=result.data,
                    execution_time=time.time() - start_time
                )
            else:
                return ScrapingResult(
                    success=False,
                    data={'competitors': [], 'total_count': 0},
                    error_message=result.error_message or "æ— æ³•æŠ“å–è·Ÿå–åº—é“ºä¿¡æ¯",
                    execution_time=time.time() - start_time
                )
            
        except Exception as e:
            self.logger.error(f"æŠ“å–è·Ÿå–åº—é“ºä¿¡æ¯å¤±è´¥: {e}")
            return ScrapingResult(
                success=False,
                data={'competitors': [], 'total_count': 0},
                error_message=str(e),
                execution_time=time.time() - start_time
            )
    
    def scrape(self, product_url: str, include_competitors: bool = False, **kwargs) -> ScrapingResult:
        """
        ç»¼åˆæŠ“å–å•†å“ä¿¡æ¯
        
        Args:
            product_url: å•†å“URL
            include_competitors: æ˜¯å¦åŒ…å«è·Ÿå–åº—é“ºä¿¡æ¯
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            ScrapingResult: æŠ“å–ç»“æœ
        """
        start_time = time.time()
        
        try:
            # æŠ“å–ä»·æ ¼ä¿¡æ¯
            price_result = self.scrape_product_prices(product_url)
            if not price_result.success:
                return price_result
            
            result_data = {
                'product_url': product_url,
                'price_data': price_result.data
            }
            
            # å¦‚æœéœ€è¦ï¼ŒæŠ“å–è·Ÿå–åº—é“ºä¿¡æ¯
            if include_competitors:
                competitors_result = self.scrape_competitor_stores(product_url)
                if competitors_result.success:
                    result_data['competitors'] = competitors_result.data['competitors']
                else:
                    self.logger.warning(f"æŠ“å–è·Ÿå–åº—é“ºä¿¡æ¯å¤±è´¥: {competitors_result.error_message}")
                    result_data['competitors'] = []
            
            return ScrapingResult(
                success=True,
                data=result_data,
                execution_time=time.time() - start_time
            )
            
        except Exception as e:
            self.logger.error(f"ç»¼åˆæŠ“å–å•†å“ä¿¡æ¯å¤±è´¥: {e}")
            return ScrapingResult(
                success=False,
                data={},
                error_message=str(e),
                execution_time=time.time() - start_time
            )

    # ä»·æ ¼é€‰æ‹©å™¨é…ç½® - ä¿®æ­£ï¼šåªæå–å•†å“æœ¬èº«ä»·æ ¼ï¼Œæ’é™¤è·Ÿå–ä»·æ ¼
    PRICE_SELECTORS = [
        ("[data-widget='webPrice'] .tsHeadline500Medium", "green"),  # ä¿®æ­£ï¼šä¸­ç­‰å­—ä½“é€šå¸¸æ˜¯ç»¿æ ‡
        ("[data-widget='webPrice'] .tsHeadline600Large", "black"),   # ä¿®æ­£ï¼šå¤§å­—ä½“é€šå¸¸æ˜¯é»‘æ ‡
        ("[data-widget='webPrice'] span", "auto"),                   # ğŸ”§ é™åˆ¶åœ¨webPriceå®¹å™¨å†…ï¼Œé¿å…è·Ÿå–ä»·æ ¼
        # ğŸš« åˆ é™¤è¿‡äºå®½æ³›çš„é€‰æ‹©å™¨ï¼Œé¿å…è¯¯æå–è·Ÿå–ä»·æ ¼
        # (".b5v3 span", "auto"),                                   # å¤ªå®½æ³›ï¼Œä¼šåŒ¹é…è·Ÿå–ä»·æ ¼
        # ("[class*='price'] span", "auto"),                        # å¤ªå®½æ³›ï¼Œä¼šåŒ¹é…è·Ÿå–ä»·æ ¼
        # ("[data-test-id*='price'] span", "auto"),                 # å¤ªå®½æ³›ï¼Œä¼šåŒ¹é…è·Ÿå–ä»·æ ¼
    ]

    # å›¾ç‰‡é€‰æ‹©å™¨é…ç½® - ç»Ÿä¸€é…ç½®é¿å…é‡å¤
    IMAGE_SELECTORS = [
        "#layoutPage > div:nth-child(1) > div:nth-child(3) > div:nth-child(3) > div:nth-child(1) > div:nth-child(1) > div:nth-child(1) > div > div > div > div:nth-child(1) > div > div > div:nth-child(1) > div:nth-child(1) > div > div > div:nth-child(2) > div > div > div > img",
        "[class*='pdp_y3']",
        "[class*='b95_3_3-a']",
        "img[src*='multimedia']",
        "img[src*='ozone.ru']"
    ]

    def _extract_price_data_core(self, soup, is_async=False) -> Dict[str, Any]:
        """
        æ ¸å¿ƒä»·æ ¼æå–é€»è¾‘ - ç®€åŒ–ç‰ˆæœ¬

        Args:
            soup: BeautifulSoupå¯¹è±¡
            is_async: æ˜¯å¦å¼‚æ­¥è°ƒç”¨

        Returns:
            Dict[str, Any]: ä»·æ ¼æ•°æ®
        """
        try:
            price_data = {}

            # æå–å•†å“å›¾ç‰‡
            image_url = self._extract_product_image_core(soup)
            if image_url:
                price_data['image_url'] = image_url

            # æå–åŸºç¡€ä»·æ ¼ï¼ˆç»¿æ ‡ã€é»‘æ ‡ï¼‰
            basic_prices = self._extract_basic_prices(soup)
            price_data.update(basic_prices)

            # ğŸ”§ ä¿®å¤ï¼šç›´æ¥åœ¨ä¸»æµç¨‹ä¸­æ£€æµ‹è·Ÿå–å…³é”®è¯å¹¶æå–ä»·æ ¼
            page_text = soup.get_text()
            competitor_keywords = ['Ñƒ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ²Ñ†Ğ¾Ğ²', 'ĞµÑÑ‚ÑŒ Ğ´ĞµÑˆĞµĞ²Ğ»Ğµ', 'ĞµÑÑ‚ÑŒ Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ']

            # æ£€æµ‹è·Ÿå–å…³é”®è¯
            for keyword in competitor_keywords:
                if keyword.lower() in page_text.lower():
                    self.logger.info(f"ğŸ” æ£€æµ‹åˆ°è·Ÿå–å…³é”®è¯: {keyword}")
                    price_data.update({
                        'has_competitors': True,
                        'competitor_keyword': keyword
                    })

                    # æå–è·Ÿå–ä»·æ ¼
                    competitor_price = self._extract_competitor_price_value(soup)
                    if competitor_price:
                        price_data['competitor_price'] = competitor_price
                        self.logger.info(f"ğŸ’° è·Ÿå–ä»·æ ¼: {competitor_price}â‚½")
                    break

            return price_data

        except Exception as e:
            self.logger.error(f"æå–ä»·æ ¼æ•°æ®å¤±è´¥: {e}")
            return {}

    def _extract_basic_prices(self, soup) -> Dict[str, Any]:
        """æå–åŸºç¡€ä»·æ ¼ï¼ˆç»¿æ ‡ã€é»‘æ ‡ï¼‰"""
        prices = {}
        green_price = None
        black_price = None
        auto_prices = []  # æ”¶é›†autoç±»å‹çš„ä»·æ ¼

        for selector, price_type in self.PRICE_SELECTORS:
            if green_price and black_price:
                break

            try:
                elements = soup.select(selector)
                for element in elements:
                    price = self._extract_price_from_element(element)
                    if not price:
                        continue

                    if price_type == "green" and not green_price:
                        green_price = price
                        self.logger.info(f"âœ… ç»¿æ ‡ä»·æ ¼: {green_price}â‚½")
                    elif price_type == "black" and not black_price:
                        black_price = price
                        self.logger.info(f"âœ… é»‘æ ‡ä»·æ ¼: {black_price}â‚½")
                    elif price_type == "auto":
                        auto_prices.append((price, element))

                    if green_price and black_price:
                        break
            except Exception:
                continue

        # ğŸ”§ ä¿®å¤ï¼šåˆ é™¤æ™ºèƒ½åˆ†é…é€»è¾‘ï¼ŒOzonScraperåªè´Ÿè´£åŸæ ·æå–ä»·æ ¼
        # auto_prices ä¸­çš„ä»·æ ¼ä¸åº”è¯¥è¢«è‡ªåŠ¨åˆ†é…ï¼Œåº”è¯¥ç”±ä¸Šå±‚ä¸šåŠ¡é€»è¾‘å¤„ç†
        if auto_prices:
            self.logger.debug(f"ğŸ” å‘ç°autoç±»å‹ä»·æ ¼: {auto_prices}ï¼Œä½†ä¸è¿›è¡Œè‡ªåŠ¨åˆ†é…")

        # ğŸ”§ ä¿®å¤ï¼šç»¿æ ‡ä»·æ ¼ä¸å­˜åœ¨æ—¶åº”è¯¥ä¸ºç©ºï¼Œä¸è¦æ·»åŠ åˆ°è¿”å›æ•°æ®ä¸­
        if green_price:
            prices['green_price'] = green_price
        if black_price:
            prices['black_price'] = black_price

        return prices

    # ğŸš« åˆ é™¤æ™ºèƒ½ä»·æ ¼åˆ†é…é€»è¾‘ - ç”¨æˆ·æ˜ç¡®è¦æ±‚ä¸è¦è¿›è¡Œä»»ä½•ä»·æ ¼è®¡ç®—ï¼

    # ğŸš« åˆ é™¤å†—ä½™çš„è·Ÿå–ä»·æ ¼æå–æ–¹æ³• - åŠŸèƒ½å·²é›†æˆåˆ°ä¸»è¦ä»·æ ¼æå–æµç¨‹ä¸­

    def _extract_competitor_price_value(self, soup) -> Optional[float]:
        """æå–å…·ä½“çš„è·Ÿå–ä»·æ ¼æ•°å€¼ - ä½¿ç”¨ç”¨æˆ·æä¾›çš„ç²¾ç¡®é€‰æ‹©å™¨"""
        try:
            # ğŸ¯ ä½¿ç”¨ç”¨æˆ·æä¾›çš„ç²¾ç¡®è·Ÿå–ä»·æ ¼é€‰æ‹©å™¨
            # é€‰æ‹©å™¨ï¼šspan.q6b3_0_2-a1
            # å…ƒç´ ï¼š<span class="q6b3_0_2-a1">From 3 800 â‚½</span>

            competitor_price_selector = "span.q6b3_0_2-a1"

            self.logger.debug(f"ğŸ” ä½¿ç”¨ç²¾ç¡®è·Ÿå–ä»·æ ¼é€‰æ‹©å™¨: {competitor_price_selector}")

            # æŸ¥æ‰¾è·Ÿå–ä»·æ ¼å…ƒç´ 
            competitor_elements = soup.select(competitor_price_selector)

            for element in competitor_elements:
                text = element.get_text(strip=True)
                self.logger.debug(f"ğŸ” æ‰¾åˆ°è·Ÿå–ä»·æ ¼å…ƒç´ æ–‡æœ¬: '{text}'")

                # æå–ä»·æ ¼æ•°å€¼ - å¤„ç† "From 3 800 â‚½" æ ¼å¼
                price = self._extract_price_from_element(element)
                if price and price > 0:
                    self.logger.debug(f"ğŸ¯ æˆåŠŸæå–è·Ÿå–ä»·æ ¼: {price}â‚½")
                    return price

            self.logger.debug("âš ï¸ æœªæ‰¾åˆ°è·Ÿå–ä»·æ ¼å…ƒç´ ")
            return None

        except Exception as e:
            self.logger.error(f"æå–è·Ÿå–ä»·æ ¼å¤±è´¥: {e}")
            return None

    # ğŸ”§ ä¿®å¤ï¼šåˆ é™¤é‡å¤çš„è·Ÿå–åº—é“ºæå–é€»è¾‘ï¼Œè¿™äº›åŠŸèƒ½åº”è¯¥ç”± CompetitorScraper è´Ÿè´£
    # åˆ é™¤äº†å¤§é‡é‡å¤çš„è·Ÿå–åº—é“ºç›¸å…³ä»£ç ï¼ŒèŒè´£åˆ†ç¦»ï¼š
    # - OzonScraper: è´Ÿè´£ä»·æ ¼æå–
    # - CompetitorScraper: è´Ÿè´£è·Ÿå–åº—é“ºäº¤äº’å’Œæå–

    def _extract_price_from_element(self, element) -> Optional[float]:
        """ä»å…ƒç´ ä¸­æå–ä»·æ ¼ - ä¿®å¤ä»·æ ¼æˆªæ–­bug"""
        try:
            text = element.get_text(strip=True)
            if 'â‚½' in text or 'Ñ€ÑƒĞ±' in text:
                # ğŸ”§ ä¿®å¤ï¼šç›´æ¥ä½¿ç”¨ clean_price_string å‡½æ•°ï¼Œé¿å…ä»·æ ¼æˆªæ–­
                from apps.xuanping.common.models import clean_price_string
                price = clean_price_string(text)
                if price and price > 0:
                    return price
            return None
        except Exception:
            return None

    def _extract_product_image_core(self, soup) -> Optional[str]:
        """
        æ ¸å¿ƒå›¾ç‰‡æå–é€»è¾‘ - ç»Ÿä¸€å®ç°é¿å…é‡å¤

        Args:
            soup: BeautifulSoupå¯¹è±¡

        Returns:
            str: å•†å“å›¾ç‰‡URLï¼Œå¦‚æœæå–å¤±è´¥è¿”å›None
        """
        try:
            for selector in self.IMAGE_SELECTORS:
                img_element = soup.select_one(selector)
                if img_element:
                    src = img_element.get('src')
                    if src:
                        high_res_url = self._convert_to_high_res_image(src)
                        self.logger.info(f"âœ… æˆåŠŸæå–å•†å“å›¾ç‰‡: {high_res_url}")
                        return high_res_url

            self.logger.warning("âš ï¸ æœªæ‰¾åˆ°å•†å“å›¾ç‰‡")
            return None

        except Exception as e:
            self.logger.error(f"æå–å•†å“å›¾ç‰‡å¤±è´¥: {e}")
            return None

    def _extract_price_data_from_content_sync(self, page_content: str) -> Dict[str, Any]:
        """
        ä»é¡µé¢å†…å®¹ä¸­æå–ä»·æ ¼æ•°æ® - åŒæ­¥ç‰ˆæœ¬ï¼ˆè°ƒç”¨æ ¸å¿ƒé€»è¾‘ï¼‰
        """
        try:
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(page_content, 'html.parser')
            return self._extract_price_data_core(soup, is_async=False)
        except Exception as e:
            self.logger.error(f"ä»é¡µé¢å†…å®¹æå–ä»·æ ¼æ•°æ®å¤±è´¥: {e}")
            return {}

    def _extract_product_image_from_content_sync(self, soup) -> Optional[str]:
        """
        ä»é¡µé¢å†…å®¹ä¸­æå–å•†å“å›¾ç‰‡åœ°å€ - åŒæ­¥ç‰ˆæœ¬ï¼ˆè°ƒç”¨æ ¸å¿ƒé€»è¾‘ï¼‰
        """
        return self._extract_product_image_core(soup)

    async def _extract_product_image_from_content(self, soup) -> Optional[str]:
        """
        ä»é¡µé¢å†…å®¹ä¸­æå–å•†å“å›¾ç‰‡åœ°å€ - å¼‚æ­¥ç‰ˆæœ¬ï¼ˆè°ƒç”¨æ ¸å¿ƒé€»è¾‘ï¼‰
        """
        return self._extract_product_image_core(soup)

    def _convert_to_high_res_image(self, image_url: str) -> str:
        """
        å°†å›¾ç‰‡URLè½¬æ¢ä¸ºé«˜æ¸…ç‰ˆæœ¬

        Args:
            image_url: åŸå§‹å›¾ç‰‡URL

        Returns:
            str: é«˜æ¸…å›¾ç‰‡URL
        """
        try:
            import re
            # å°†wc50æˆ–wc100æ›¿æ¢ä¸ºwc1000
            high_res_url = re.sub(r'/wc\d+/', '/wc1000/', image_url)
            return high_res_url
        except Exception as e:
            self.logger.warning(f"è½¬æ¢é«˜æ¸…å›¾ç‰‡URLå¤±è´¥: {e}")
            return image_url




    
    async def _open_competitor_popup_async(self, browser_service):
        """
        å¼‚æ­¥æ‰“å¼€è·Ÿå–åº—é“ºæµ®å±‚
        
        Args:
            browser_service: æµè§ˆå™¨æœåŠ¡
        """
        try:
            page = browser_service.browser_driver.page
            self.logger.info("ğŸ” å¼€å§‹æŸ¥æ‰¾å¹¶ç‚¹å‡»è·Ÿå–åŒºåŸŸ...")

            # ğŸ”§ ä½¿ç”¨æ›´å‡†ç¡®çš„XPathå’ŒCSSé€‰æ‹©å™¨ï¼ŒæŒ‰æˆåŠŸç‡æ’åº
            # æ ¹æ®æµ‹è¯•æ—¥å¿—ï¼Œæœ€æœ‰æ•ˆçš„é€‰æ‹©å™¨æ˜¯ "button span div.pdp_t1"
            competitor_button_selectors = [
                # é«˜æˆåŠŸç‡é€‰æ‹©å™¨
                "button span div.pdp_t1",
                # åŸºäºæ–‡æœ¬å†…å®¹çš„é€‰æ‹©å™¨ï¼ˆæ·»åŠ æ›´å¤šå˜ä½“ï¼‰
                "button:has-text('Ğ•ÑÑ‚ÑŒ Ğ´ĞµÑˆĞµĞ²Ğ»Ğµ')",
                "div:has-text('Ğ•ÑÑ‚ÑŒ Ğ´ĞµÑˆĞµĞ²Ğ»Ğµ')",
                "button:has-text('Ğ•ÑÑ‚ÑŒ Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ')",
                "div:has-text('Ğ•ÑÑ‚ÑŒ Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ')",
                # ç®€åŒ–ç‰ˆé€‰æ‹©å™¨
                "[class*='pdp_t1'] button",
                ".pdp_t1 button",
                "div.pdp_t1 button"
            ]

            clicked = False

            for selector in competitor_button_selectors:
                try:
                    self.logger.debug(f"ğŸ” å°è¯•ä½¿ç”¨é€‰æ‹©å™¨: {selector}")
                    # ç­‰å¾…å…ƒç´ å‡ºç°
                    try:
                        if selector.startswith("#layoutPage"):
                            # ä½¿ç”¨XPath
                            xpath = "//*[@id='layoutPage']/div[1]/div[3]/div[3]/div[2]/div/div/div[2]/div[3]/div[2]/div/div/div/button/span/div"
                            self.logger.debug(f"ğŸ” ä½¿ç”¨XPath: {xpath}")
                            await page.wait_for_selector(f'xpath={xpath}', timeout=3000)
                            element = await page.query_selector(f'xpath={xpath}')
                        else:
                            # ä½¿ç”¨CSSé€‰æ‹©å™¨
                            self.logger.debug(f"ğŸ” ä½¿ç”¨CSSé€‰æ‹©å™¨: {selector}")
                            await page.wait_for_selector(selector, timeout=3000)
                            element = await page.query_selector(selector)
                    except Exception as wait_e:
                        self.logger.debug(f"ç­‰å¾…å…ƒç´ å‡ºç°å¤±è´¥: {wait_e}")
                        continue

                    if element:
                        # æ£€æŸ¥å…ƒç´ æ˜¯å¦å¯è§å’Œå¯ç‚¹å‡»
                        is_visible = await element.is_visible()
                        self.logger.debug(f"å…ƒç´ å¯è§æ€§: {is_visible}")
                        if is_visible:
                            # è·å–å…ƒç´ æ–‡æœ¬å†…å®¹ç”¨äºæ—¥å¿—
                            try:
                                text_content = await element.text_content()
                                self.logger.debug(f"å…ƒç´ æ–‡æœ¬å†…å®¹: {text_content[:100] if text_content else 'N/A'}")
                            except:
                                pass

                            # å°è¯•ç‚¹å‡»å…ƒç´ 
                            await element.click()
                            self.logger.info(f"âœ… æˆåŠŸç‚¹å‡»è·Ÿå–åŒºåŸŸ: {selector}")
                            clicked = True

                            # ç­‰å¾…é¡µé¢å“åº”
                            self.logger.info("â³ ç­‰å¾…é¡µé¢å“åº”...")
                            await asyncio.sleep(3)
                            break
                        else:
                            self.logger.debug(f"å…ƒç´ ä¸å¯è§: {selector}")

                except Exception as e:
                    self.logger.debug(f"é€‰æ‹©å™¨ {selector} ç‚¹å‡»å¤±è´¥: {e}")
                    continue



            if clicked:
                self.logger.info("ğŸ¯ è·Ÿå–æµ®å±‚å·²æ‰“å¼€ï¼Œç­‰å¾…å†…å®¹åŠ è½½...")
                await asyncio.sleep(5)  # å¢åŠ ç­‰å¾…æ—¶é—´ç¡®ä¿æµ®å±‚å†…å®¹åŠ è½½
                self.logger.info("âœ… è·Ÿå–æµ®å±‚å†…å®¹åŠ è½½å®Œæˆ")
            else:
                self.logger.warning("âš ï¸ æœªèƒ½æ‰¾åˆ°æˆ–ç‚¹å‡»è·Ÿå–åŒºåŸŸï¼Œå¯èƒ½é¡µé¢ç»“æ„å·²å˜åŒ–")

        except Exception as e:
            self.logger.error(f"æ‰“å¼€è·Ÿå–åº—é“ºæµ®å±‚å¤±è´¥: {e}")

    def close(self):
        """
        å…³é—­æŠ“å–å™¨ï¼Œæ¸…ç†èµ„æº
        """
        try:
            if hasattr(self, 'browser_service') and self.browser_service:
                self.browser_service.close()
                self.logger.info("ğŸ”’ OzonScraper å·²å…³é—­")
        except Exception as e:
            self.logger.error(f"å…³é—­ OzonScraper æ—¶å‘ç”Ÿé”™è¯¯: {e}")

    def __del__(self):
        """
        ææ„å‡½æ•°ï¼Œç¡®ä¿èµ„æºè¢«æ­£ç¡®é‡Šæ”¾
        """
        try:
            self.close()
        except:
            pass




    

    




