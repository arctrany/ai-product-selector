"""
OZONå¹³å°æŠ“å–å™¨ - æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬

è§£å†³çš„æ ¸å¿ƒé—®é¢˜ï¼š
1. DOMæœç´¢è¿‡åº¦ï¼šé¿å…å…¨é¡µé¢éå†ï¼Œåªåœ¨ç‰¹å®šå®¹å™¨å†…æœç´¢
2. é‡å¤å¯¼èˆªï¼šåˆå¹¶æŠ“å–æµç¨‹ï¼Œä¸€æ¬¡å¯¼èˆªè·å–æ‰€æœ‰æ•°æ®
3. é‡å¤æ„å»ºBeautifulSoupï¼šç¼“å­˜å’Œå¤ç”¨è§£æç»“æœ
4. å¼‚æ­¥å‡½æ•°è¯¯ç”¨ï¼šåˆ†ç¦»åŒæ­¥HTMLè§£æå’Œå¼‚æ­¥IOæ“ä½œ
5. é”™è¯¯å¤„ç†ä¼˜åŒ–ï¼šæ˜ç¡®é”™è¯¯åˆ†ç±»å’Œè¶…æ—¶æ§åˆ¶
"""

import asyncio
import logging
import time
from typing import Dict, Any, List, Optional, Tuple
from enum import Enum
from dataclasses import dataclass
from bs4 import BeautifulSoup

from .xuanping_browser_service import XuanpingBrowserServiceSync
from ..models import ProductInfo, CompetitorStore, clean_price_string, ScrapingResult
from ..config import GoodStoreSelectorConfig


class ScrapingErrorCode(Enum):
    """æŠ“å–é”™è¯¯ç æšä¸¾"""
    SUCCESS = "SUCCESS"
    NAVIGATION_FAILED = "NAVIGATION_FAILED"
    CONTAINER_NOT_FOUND = "CONTAINER_NOT_FOUND"
    PARSE_PRICE_FAILED = "PARSE_PRICE_FAILED"
    PARSE_SELLER_FAILED = "PARSE_SELLER_FAILED"
    TIMEOUT = "TIMEOUT"
    UNKNOWN_ERROR = "UNKNOWN_ERROR"


@dataclass
class OptimizedScrapingResult:
    """ä¼˜åŒ–çš„æŠ“å–ç»“æœæ•°æ®ç±»"""
    success: bool
    data: Optional[Dict[str, Any]] = None
    error_code: Optional[ScrapingErrorCode] = None
    error_message: Optional[str] = None


class OzonSelectors:
    """Ozoné€‰æ‹©å™¨é…ç½®ç±» - é›†ä¸­ç®¡ç†æ‰€æœ‰é€‰æ‹©å™¨ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº"""
    
    # ä»·æ ¼å®¹å™¨é€‰æ‹©å™¨ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼Œä¼˜å…ˆä½¿ç”¨ç²¾ç¡®å®¹å™¨ï¼‰
    PRICE_CONTAINERS = [
        "[data-widget='webPrice']",
        "[data-widget='price']", 
        ".price-container",
        "[class*='price-container']"
    ]
    
    # ç»¿æ ‡ä»·æ ¼é€‰æ‹©å™¨ï¼ˆåœ¨å®¹å™¨å†…ä½¿ç”¨ï¼Œé™åˆ¶å›é€€æ·±åº¦ï¼‰
    GREEN_PRICE_SELECTORS = [
        ".tsHeadline600Large",
        "[data-test-id='green-price']",
        ".green-price"
    ]
    
    # é»‘æ ‡ä»·æ ¼é€‰æ‹©å™¨ï¼ˆåœ¨å®¹å™¨å†…ä½¿ç”¨ï¼‰
    BLACK_PRICE_SELECTORS = [
        ".tsHeadline500Medium", 
        "[data-test-id='black-price']",
        ".black-price",
        "[class*='old-price']"
    ]
    
    # è·Ÿå–å®¹å™¨é€‰æ‹©å™¨ï¼ˆä¸¥æ ¼è¦æ±‚å®¹å™¨å­˜åœ¨ï¼‰
    SELLER_CONTAINERS = [
        "[data-widget='sellerList']",
        "#seller-list",
        "[class*='seller-list']"
    ]
    
    # è·Ÿå–åº—é“ºé¡¹é€‰æ‹©å™¨ï¼ˆä»…åœ¨å®¹å™¨å†…æœç´¢ï¼‰
    SELLER_ITEM_SELECTORS = [
        ":scope > div",
        ":scope > li"
    ]
    
    # è·Ÿå–ç‚¹å‡»åŒºåŸŸé€‰æ‹©å™¨
    COMPETITOR_CLICK_SELECTORS = [
        "[data-test-id*='competitor']",
        "[class*='competitor-price']"
    ]


class OzonKeywords:
    """Ozonå…³é”®è¯é…ç½®ç±»"""
    
    # è·Ÿå–ç›¸å…³å…³é”®è¯ï¼ˆä¿„æ–‡ï¼Œå·²ä¼˜åŒ–ä¸ºlower caseï¼‰
    COMPETITOR_KEYWORDS = [
        'Ñƒ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ²Ñ†Ğ¾Ğ²',
        'Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ²Ñ†Ğ¾Ğ²',
        'Ğ¾Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ…',
        'Ñƒ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ…',
        'Ğ´Ñ€ÑƒĞ³Ğ¸Ñ…',
        'Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ²Ñ†Ğ¾Ğ²'
    ]
    
    # ä»·æ ¼ç›¸å…³ç¬¦å·
    PRICE_SYMBOLS = ['â‚½', 'Ñ€ÑƒĞ±']


class OzonScraperOptimized:
    """OZONå¹³å°æŠ“å–å™¨ - æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬"""
    
    def __init__(self, config: Optional[GoodStoreSelectorConfig] = None):
        """åˆå§‹åŒ–OZONæŠ“å–å™¨"""
        self.config = config or GoodStoreSelectorConfig()
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        self.base_url = self.config.scraping.ozon_base_url
        
        # åˆ›å»ºæµè§ˆå™¨æœåŠ¡
        self.browser_service = XuanpingBrowserServiceSync()
        
        # ç¼“å­˜è§£æç»“æœ
        self._soup_cache = {}
        
    def scrape(self, product_url: str, include_competitors: bool = False, max_competitors: int = 10) -> OptimizedScrapingResult:
        """
        ä¼˜åŒ–çš„ç»Ÿä¸€æŠ“å–æ–¹æ³•ï¼šä¸€æ¬¡å¯¼èˆªï¼Œè·å–æ‰€æœ‰éœ€è¦çš„æ•°æ®
        
        Args:
            product_url: å•†å“URL
            include_competitors: æ˜¯å¦åŒ…å«è·Ÿå–åº—é“ºä¿¡æ¯
            max_competitors: æœ€å¤§è·Ÿå–åº—é“ºæ•°é‡
            
        Returns:
            OptimizedScrapingResult: æŠ“å–ç»“æœ
        """
        start_time = time.time()
        
        try:
            self.logger.info(f"ğŸš€ å¼€å§‹ä¼˜åŒ–æŠ“å–: {product_url}")
            
            # ä½¿ç”¨æµè§ˆå™¨æœåŠ¡è¿›è¡Œç»Ÿä¸€æŠ“å–
            async def unified_scrape_async(browser_service):
                """ç»Ÿä¸€çš„å¼‚æ­¥æŠ“å–é€»è¾‘"""
                try:
                    # 1. å¯¼èˆªåˆ°é¡µé¢ï¼ˆåªå¯¼èˆªä¸€æ¬¡ï¼‰
                    await asyncio.sleep(2)
                    page_content = await browser_service.browser_driver.page.content()
                    
                    # 2. å¦‚æœéœ€è¦è·Ÿå–ä¿¡æ¯ï¼Œæ‰“å¼€è·Ÿå–æµ®å±‚
                    if include_competitors:
                        await self._open_competitor_popup_optimized(browser_service.browser_driver.page)
                        # è·å–æ›´æ–°åçš„é¡µé¢å†…å®¹
                        page_content = await browser_service.browser_driver.page.content()
                    
                    # 3. æå–ç‰¹å®šå®¹å™¨çš„HTMLç‰‡æ®µï¼ˆé¿å…å…¨é¡µè§£æï¼‰
                    containers = await self._extract_container_fragments(browser_service.browser_driver.page)
                    
                    return {
                        'page_content': page_content,
                        'containers': containers
                    }
                    
                except Exception as e:
                    self.logger.error(f"å¼‚æ­¥æŠ“å–å¤±è´¥: {e}")
                    return None
            
            # æ‰§è¡Œå¼‚æ­¥æŠ“å–
            scrape_data = self.browser_service.scrape_page_data(
                product_url, 
                unified_scrape_async
            )
            
            if not scrape_data:
                return OptimizedScrapingResult(
                    success=False,
                    error_code=ScrapingErrorCode.NAVIGATION_FAILED,
                    error_message="é¡µé¢å¯¼èˆªå¤±è´¥"
                )
            
            # 4. åŒæ­¥è§£ææ•°æ®ï¼ˆCPU-boundæ“ä½œï¼‰
            result_data = self._parse_scraped_data_sync(
                scrape_data['containers'], 
                include_competitors, 
                max_competitors
            )
            
            elapsed_time = time.time() - start_time
            self.logger.info(f"âœ… ä¼˜åŒ–æŠ“å–å®Œæˆï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’")
            
            return OptimizedScrapingResult(
                success=True,
                data=result_data,
                error_code=ScrapingErrorCode.SUCCESS
            )
            
        except Exception as e:
            elapsed_time = time.time() - start_time
            self.logger.error(f"æŠ“å–å¤±è´¥: {e}, è€—æ—¶: {elapsed_time:.2f}ç§’")
            return OptimizedScrapingResult(
                success=False,
                error_code=ScrapingErrorCode.UNKNOWN_ERROR,
                error_message=str(e)
            )
    
    async def _extract_container_fragments(self, page) -> Dict[str, str]:
        """
        æå–ç‰¹å®šå®¹å™¨çš„HTMLç‰‡æ®µï¼Œé¿å…å…¨é¡µè§£æ
        
        Args:
            page: Playwrighté¡µé¢å¯¹è±¡
            
        Returns:
            Dict[str, str]: å®¹å™¨ç‰‡æ®µå­—å…¸
        """
        containers = {}
        
        try:
            # æå–ä»·æ ¼å®¹å™¨
            for selector in OzonSelectors.PRICE_CONTAINERS:
                try:
                    element = await page.query_selector(selector)
                    if element:
                        containers['price'] = await element.inner_html()
                        self.logger.debug(f"âœ… æå–ä»·æ ¼å®¹å™¨: {selector}")
                        break
                except Exception:
                    continue
            
            # æå–è·Ÿå–å®¹å™¨
            for selector in OzonSelectors.SELLER_CONTAINERS:
                try:
                    element = await page.query_selector(selector)
                    if element:
                        containers['sellers'] = await element.inner_html()
                        self.logger.debug(f"âœ… æå–è·Ÿå–å®¹å™¨: {selector}")
                        break
                except Exception:
                    continue
            
            # æå–å›¾ç‰‡
            try:
                img_element = await page.query_selector("img[src*='ozonstatic']")
                if img_element:
                    containers['image'] = await img_element.get_attribute('src')
            except Exception:
                pass
                
        except Exception as e:
            self.logger.warning(f"æå–å®¹å™¨ç‰‡æ®µå¤±è´¥: {e}")
        
        return containers
    
    def _parse_scraped_data_sync(self, containers: Dict[str, str], include_competitors: bool, max_competitors: int) -> Dict[str, Any]:
        """
        åŒæ­¥è§£ææŠ“å–çš„æ•°æ®ï¼ˆCPU-boundæ“ä½œï¼‰
        
        Args:
            containers: å®¹å™¨HTMLç‰‡æ®µ
            include_competitors: æ˜¯å¦åŒ…å«è·Ÿå–ä¿¡æ¯
            max_competitors: æœ€å¤§è·Ÿå–æ•°é‡
            
        Returns:
            Dict[str, Any]: è§£æç»“æœ
        """
        result = {}
        
        try:
            # è§£æä»·æ ¼ä¿¡æ¯ï¼ˆä»…åœ¨ä»·æ ¼å®¹å™¨å†…ï¼‰
            if 'price' in containers:
                price_data = self._parse_price_from_container_sync(containers['price'])
                result.update(price_data)
            
            # è§£æå›¾ç‰‡
            if 'image' in containers:
                result['image_url'] = containers['image']
            
            # è§£æè·Ÿå–ä¿¡æ¯ï¼ˆä»…åœ¨è·Ÿå–å®¹å™¨å†…ï¼‰
            if include_competitors and 'sellers' in containers:
                competitors = self._parse_competitors_from_container_sync(
                    containers['sellers'], 
                    max_competitors
                )
                result['competitors'] = competitors
            
        except Exception as e:
            self.logger.error(f"åŒæ­¥è§£ææ•°æ®å¤±è´¥: {e}")
        
        return result
    
    def _parse_price_from_container_sync(self, price_html: str) -> Dict[str, Any]:
        """
        ä»ä»·æ ¼å®¹å™¨ä¸­åŒæ­¥è§£æä»·æ ¼ä¿¡æ¯ï¼ˆé¿å…å…¨é¡µæœç´¢ï¼‰
        
        Args:
            price_html: ä»·æ ¼å®¹å™¨HTML
            
        Returns:
            Dict[str, Any]: ä»·æ ¼æ•°æ®
        """
        try:
            # åªæ„å»ºä¸€æ¬¡BeautifulSoupï¼Œä»…è§£æå®¹å™¨å†…å®¹
            soup = BeautifulSoup(price_html, 'html.parser')
            
            green_price = None
            black_price = None
            
            # åœ¨å®¹å™¨å†…æœç´¢ç»¿æ ‡ä»·æ ¼ï¼ˆé™åˆ¶æœç´¢èŒƒå›´ï¼‰
            for selector in OzonSelectors.GREEN_PRICE_SELECTORS:
                element = soup.select_one(selector)
                if element:
                    text = element.get_text(strip=True)
                    if any(symbol in text for symbol in OzonKeywords.PRICE_SYMBOLS):
                        # æ£€æŸ¥æ˜¯å¦æ˜¯è·Ÿå–ä»·æ ¼ï¼ˆé™åˆ¶çˆ¶èŠ‚ç‚¹å›æº¯æ·±åº¦<=2ï¼‰
                        if not self._is_competitor_price_sync(element, max_levels=2):
                            price = clean_price_string(text)
                            if price and price > 0:
                                green_price = price
                                self.logger.debug(f"âœ… æå–ç»¿æ ‡ä»·æ ¼: {green_price}â‚½")
                                break
            
            # åœ¨å®¹å™¨å†…æœç´¢é»‘æ ‡ä»·æ ¼
            for selector in OzonSelectors.BLACK_PRICE_SELECTORS:
                element = soup.select_one(selector)
                if element:
                    text = element.get_text(strip=True)
                    if any(symbol in text for symbol in OzonKeywords.PRICE_SYMBOLS):
                        if not self._is_competitor_price_sync(element, max_levels=2):
                            price = clean_price_string(text)
                            if price and price > 0:
                                black_price = price
                                self.logger.debug(f"âœ… æå–é»‘æ ‡ä»·æ ¼: {black_price}â‚½")
                                break
            
            result = {}
            if green_price:
                result['green_price'] = green_price
            if black_price:
                result['black_price'] = black_price
                
            return result
            
        except Exception as e:
            self.logger.error(f"è§£æä»·æ ¼å®¹å™¨å¤±è´¥: {e}")
            return {}
    
    def _parse_competitors_from_container_sync(self, sellers_html: str, max_competitors: int) -> List[Dict[str, Any]]:
        """
        ä»è·Ÿå–å®¹å™¨ä¸­åŒæ­¥è§£æè·Ÿå–åº—é“ºä¿¡æ¯ï¼ˆé¿å…å…¨é¡µæœç´¢ï¼‰
        
        Args:
            sellers_html: è·Ÿå–å®¹å™¨HTML
            max_competitors: æœ€å¤§è·Ÿå–æ•°é‡
            
        Returns:
            List[Dict[str, Any]]: è·Ÿå–åº—é“ºåˆ—è¡¨
        """
        try:
            # åªæ„å»ºä¸€æ¬¡BeautifulSoupï¼Œä»…è§£æå®¹å™¨å†…å®¹
            soup = BeautifulSoup(sellers_html, 'html.parser')
            competitors = []
            
            # ä¸¥æ ¼åœ¨å®¹å™¨å†…æœç´¢åº—é“ºé¡¹ï¼ˆä¸åšå…¨å±€å›é€€ï¼‰
            competitor_elements = []
            for selector in OzonSelectors.SELLER_ITEM_SELECTORS:
                elements = soup.select(selector)
                if elements:
                    competitor_elements = elements[:max_competitors]  # é™åˆ¶æ•°é‡
                    self.logger.debug(f"âœ… æ‰¾åˆ° {len(competitor_elements)} ä¸ªè·Ÿå–åº—é“º")
                    break
            
            if not competitor_elements:
                self.logger.warning("âš ï¸ è·Ÿå–å®¹å™¨å†…æœªæ‰¾åˆ°åº—é“ºé¡¹")
                return []
            
            # è§£ææ¯ä¸ªåº—é“ºä¿¡æ¯
            for i, element in enumerate(competitor_elements):
                try:
                    competitor_data = self._extract_competitor_from_element_sync(element, i + 1)
                    if competitor_data:
                        competitors.append(competitor_data)
                        self.logger.debug(f"âœ… æå–ç¬¬{i+1}ä¸ªè·Ÿå–åº—é“º: {competitor_data.get('store_name', 'N/A')}")
                except Exception as e:
                    self.logger.warning(f"æå–ç¬¬{i+1}ä¸ªè·Ÿå–åº—é“ºå¤±è´¥: {e}")
                    continue
            
            return competitors
            
        except Exception as e:
            self.logger.error(f"è§£æè·Ÿå–å®¹å™¨å¤±è´¥: {e}")
            return []
    
    def _is_competitor_price_sync(self, element, max_levels: int = 2) -> bool:
        """
        åŒæ­¥æ£€æŸ¥æ˜¯å¦æ˜¯è·Ÿå–ä»·æ ¼ï¼ˆé™åˆ¶å›æº¯æ·±åº¦ï¼‰
        
        Args:
            element: BeautifulSoupå…ƒç´ 
            max_levels: æœ€å¤§å›æº¯å±‚æ•°
            
        Returns:
            bool: æ˜¯å¦æ˜¯è·Ÿå–ä»·æ ¼
        """
        try:
            current = element.parent
            level = 0
            
            while current and level < max_levels:
                parent_text = current.get_text(strip=True).lower()
                # é™åˆ¶æ–‡æœ¬é•¿åº¦ï¼Œé¿å…è¿‡é•¿å­—ç¬¦ä¸²å¤„ç†
                if len(parent_text) > 200:
                    parent_text = parent_text[:200]
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«è·Ÿå–å…³é”®è¯
                if any(keyword in parent_text for keyword in OzonKeywords.COMPETITOR_KEYWORDS):
                    return True
                
                current = current.parent
                level += 1
            
            return False
            
        except Exception:
            return False
    
    def _extract_competitor_from_element_sync(self, element, ranking: int) -> Optional[Dict[str, Any]]:
        """
        ä»å…ƒç´ ä¸­åŒæ­¥æå–è·Ÿå–åº—é“ºä¿¡æ¯ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
        
        Args:
            element: åº—é“ºå…ƒç´ 
            ranking: æ’å
            
        Returns:
            Dict[str, Any]: åº—é“ºä¿¡æ¯
        """
        try:
            competitor_data = {'ranking': ranking}
            
            # æå–åº—é“ºåç§°ï¼ˆçº¿æ€§ç­–ç•¥ï¼Œä¸åšå…¨èŠ‚ç‚¹éå†ï¼‰
            name_selectors = [
                "[data-test-id*='seller']",
                "[class*='seller-name']",
                "[class*='name']"
            ]
            
            for selector in name_selectors:
                name_element = element.select_one(selector)
                if name_element:
                    store_name = name_element.get_text(strip=True)
                    if store_name and len(store_name) > 1:
                        competitor_data['store_name'] = store_name
                        break
            
            # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå•æ¬¡æ–‡æœ¬å¤‡é€‰
            if 'store_name' not in competitor_data:
                text_elements = element.find_all(text=True)
                for text in text_elements[:5]:  # é™åˆ¶æœç´¢æ•°é‡
                    stripped_text = text.strip()
                    if (stripped_text and len(stripped_text) > 1 and 
                        not any(symbol in stripped_text for symbol in OzonKeywords.PRICE_SYMBOLS) and
                        not stripped_text.replace('.', '').replace(',', '').isdigit()):
                        competitor_data['store_name'] = stripped_text
                        break
            
            # æå–ä»·æ ¼ï¼ˆçº¿æ€§ç­–ç•¥ï¼‰
            price_selectors = [
                "[data-test-id*='price']",
                "[class*='price']",
                "span"
            ]
            
            for selector in price_selectors:
                price_element = element.select_one(selector)
                if price_element:
                    price_text = price_element.get_text(strip=True)
                    if any(symbol in price_text for symbol in OzonKeywords.PRICE_SYMBOLS):
                        price = clean_price_string(price_text)
                        if price and price > 0:
                            competitor_data['price'] = price
                            break
            
            # æå–åº—é“ºIDï¼ˆåˆå¹¶æ­£åˆ™ï¼Œä¼˜å…ˆåŒ¹é…ï¼‰
            link_element = element.select_one("a[href*='seller']")
            if link_element and link_element.get('href'):
                href = link_element.get('href')
                import re
                # åˆå¹¶ä¸ºä¸€ä¸ªä¼˜å…ˆåŒ¹é…åºåˆ—
                patterns = [
                    r'/seller/[^/]+-(\d+)',
                    r'/seller/(\d+)',
                    r'sellerId=(\d+)',
                    r'seller[/_](\d+)'
                ]
                
                for pattern in patterns:
                    match = re.search(pattern, href)
                    if match:
                        competitor_data['store_id'] = match.group(1)
                        break
            
            # å…œåº•å¤„ç†
            if 'store_name' not in competitor_data:
                competitor_data['store_name'] = f"åº—é“º{ranking}"
            if 'store_id' not in competitor_data:
                competitor_data['store_id'] = f"store_{ranking}"
            
            return competitor_data
            
        except Exception as e:
            self.logger.warning(f"æå–è·Ÿå–åº—é“ºä¿¡æ¯å¤±è´¥: {e}")
            return None
    
    async def _open_competitor_popup_optimized(self, page) -> bool:
        """
        ä¼˜åŒ–çš„æ‰“å¼€è·Ÿå–æµ®å±‚æ–¹æ³•ï¼ˆæ˜ç¡®è¶…æ—¶æ§åˆ¶ï¼‰
        
        Args:
            page: Playwrighté¡µé¢å¯¹è±¡
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸæ‰“å¼€
        """
        try:
            # é™åˆ¶å°è¯•æ¬¡æ•°ï¼Œé¿å…æ— é™å¾ªç¯
            for selector in OzonSelectors.COMPETITOR_CLICK_SELECTORS[:3]:  # æœ€å¤šå°è¯•3ä¸ªé€‰æ‹©å™¨
                try:
                    await page.wait_for_selector(selector, timeout=3000)
                    element = await page.query_selector(selector)
                    if element and await element.is_visible():
                        await element.click()
                        self.logger.info(f"âœ… æˆåŠŸç‚¹å‡»è·Ÿå–åŒºåŸŸ: {selector}")
                        await asyncio.sleep(3)  # å›ºå®šç­‰å¾…æ—¶é—´
                        return True
                except Exception:
                    continue
            
            self.logger.warning("âš ï¸ æœªèƒ½æ‰“å¼€è·Ÿå–æµ®å±‚")
            return False
            
        except Exception as e:
            self.logger.error(f"æ‰“å¼€è·Ÿå–æµ®å±‚å¤±è´¥: {e}")
            return False
    
    def close(self):
        """å…³é—­æŠ“å–å™¨"""
        try:
            if hasattr(self, 'browser_service') and self.browser_service:
                self.browser_service.close()
                self.logger.debug("OzonScraperOptimized æµè§ˆå™¨æœåŠ¡å·²å…³é—­")
        except Exception as e:
            self.logger.warning(f"å…³é—­ OzonScraperOptimized æ—¶å‡ºç°è­¦å‘Š: {e}")

    def __enter__(self):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        self.browser_service.__enter__()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        self.browser_service.__exit__(exc_type, exc_val, exc_tb)