"""
æ·˜å®å•†å“ä¿¡æ¯çˆ¬è™«ç¤ºä¾‹
ä½¿ç”¨ browser_service.py å®ç°æ·˜å®é¦–é¡µå•†å“ä¿¡æ¯çš„æŠ“å–å’Œåˆ†æ

æŠ€æœ¯è°ƒç ”å‘ç°ï¼š
1. æ·˜å®é¦–é¡µä½¿ç”¨åŠ¨æ€åŠ è½½ï¼Œéœ€è¦ç­‰å¾… JavaScript æ‰§è¡Œ
2. å•†å“ä¿¡æ¯é€šè¿‡ AJAX è¯·æ±‚å¼‚æ­¥åŠ è½½
3. éœ€è¦å¤„ç†åçˆ¬è™«æœºåˆ¶å’Œç™»å½•éªŒè¯
4. å•†å“å…ƒç´ é€šå¸¸åŒ…å« data-spm å±æ€§ç”¨äºåŸ‹ç‚¹ç»Ÿè®¡
"""

import asyncio
import json
import re
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional

from rpa.browser.browser_service import BrowserService
from rpa.browser.config import BrowserConfig


class TaobaoProductCrawler:
    """æ·˜å®å•†å“ä¿¡æ¯çˆ¬è™«"""
    
    def __init__(self, headless: bool = False, request_delay: float = 2.0):
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        Args:
            headless: æ˜¯å¦ä½¿ç”¨æ— å¤´æ¨¡å¼
            request_delay: è¯·æ±‚é—´éš”æ—¶é—´(ç§’)
        """
        self.headless = headless
        self.request_delay = request_delay
        self.browser_service = None
        
        # é…ç½®æµè§ˆå™¨
        self.browser_config = BrowserConfig(
            browser_type='chrome',  # ä½¿ç”¨ Chrome æµè§ˆå™¨
            headless=headless,
            user_data_dir=None,  # ä½¿ç”¨ä¸´æ—¶ç›®å½•
            profile_name="TaobaoCrawler",
            viewport_width=1920,
            viewport_height=1080,
            user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        )
        
        # å•†å“é€‰æ‹©å™¨é…ç½®
        self.selectors = {
            # é¦–é¡µæ¨èå•†å“
            'homepage_products': [
                '[data-spm*="product"]',
                '.item',
                '.product-item',
                '.goods-item',
                '.recommend-item',
                '[data-category="auctions"]'
            ],
            
            # å•†å“ä¿¡æ¯å­—æ®µ
            'product_title': [
                '.title',
                '.item-title',
                '.product-title',
                'h3',
                'h4',
                '.name'
            ],
            
            'product_price': [
                '.price',
                '.item-price',
                '.product-price',
                '.current-price',
                '.sale-price',
                '[data-role="price"]'
            ],
            
            'product_image': [
                'img',
                '.item-img img',
                '.product-img img',
                '.pic img'
            ],
            
            'product_link': [
                'a[href*="item.taobao.com"]',
                'a[href*="detail.tmall.com"]',
                'a[href*="chaoshi.detail.tmall.com"]'
            ],
            
            'product_shop': [
                '.shop-name',
                '.seller-name',
                '.store-name',
                '[data-role="shop"]'
            ],
            
            'product_sales': [
                '.sales',
                '.deal-cnt',
                '.sold-count',
                '[data-role="sales"]'
            ]
        }
    
    async def initialize(self) -> bool:
        """
        åˆå§‹åŒ–æµè§ˆå™¨æœåŠ¡
        
        Returns:
            bool: åˆå§‹åŒ–æ˜¯å¦æˆåŠŸ
        """
        try:
            print("ğŸš€ æ­£åœ¨åˆå§‹åŒ–æµè§ˆå™¨æœåŠ¡...")
            
            # åˆ›å»ºæµè§ˆå™¨æœåŠ¡å®ä¾‹
            self.browser_service = BrowserService(self.browser_config)
            
            # å¯åŠ¨æµè§ˆå™¨
            success = await self.browser_service.start()
            if not success:
                print("âŒ æµè§ˆå™¨å¯åŠ¨å¤±è´¥")
                return False
            
            print("âœ… æµè§ˆå™¨æœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
            return True
            
        except Exception as e:
            print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    async def navigate_to_taobao(self) -> bool:
        """
        å¯¼èˆªåˆ°æ·˜å®é¦–é¡µ
        
        Returns:
            bool: å¯¼èˆªæ˜¯å¦æˆåŠŸ
        """
        try:
            print("ğŸŒ æ­£åœ¨å¯¼èˆªåˆ°æ·˜å®é¦–é¡µ...")
            
            # å¯¼èˆªåˆ°æ·˜å®é¦–é¡µ
            success = await self.browser_service.navigate_to_url("https://www.taobao.com")
            if not success:
                print("âŒ å¯¼èˆªåˆ°æ·˜å®é¦–é¡µå¤±è´¥")
                return False
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            await asyncio.sleep(3)
            
            # æ£€æŸ¥é¡µé¢æ˜¯å¦æ­£ç¡®åŠ è½½
            page_title = await self.get_page_title()
            if "æ·˜å®" not in page_title:
                print(f"âš ï¸ é¡µé¢æ ‡é¢˜å¼‚å¸¸: {page_title}")
            
            print(f"âœ… æˆåŠŸå¯¼èˆªåˆ°æ·˜å®é¦–é¡µ: {page_title}")
            return True
            
        except Exception as e:
            print(f"âŒ å¯¼èˆªå¤±è´¥: {e}")
            return False
    
    async def get_page_title(self) -> str:
        """è·å–é¡µé¢æ ‡é¢˜"""
        try:
            page = self.browser_service.get_page()
            if page:
                return await page.title()
            return ""
        except:
            return ""
    
    async def wait_for_products_to_load(self, timeout: int = 30) -> bool:
        """
        ç­‰å¾…å•†å“åŠ è½½å®Œæˆ
        
        Args:
            timeout: è¶…æ—¶æ—¶é—´(ç§’)
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸåŠ è½½
        """
        try:
            print("â³ ç­‰å¾…å•†å“æ•°æ®åŠ è½½...")
            
            page = self.browser_service.get_page()
            if not page:
                return False
            
            # ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½
            await page.wait_for_load_state('networkidle', timeout=timeout * 1000)
            
            # å°è¯•å¤šä¸ªé€‰æ‹©å™¨ï¼Œæ‰¾åˆ°å•†å“å…ƒç´ 
            for selector in self.selectors['homepage_products']:
                try:
                    elements = await page.query_selector_all(selector)
                    if elements and len(elements) > 0:
                        print(f"âœ… å‘ç° {len(elements)} ä¸ªå•†å“å…ƒç´  (é€‰æ‹©å™¨: {selector})")
                        return True
                except:
                    continue
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å•†å“ï¼Œå°è¯•æ»šåŠ¨é¡µé¢è§¦å‘æ‡’åŠ è½½
            print("ğŸ“œ å°è¯•æ»šåŠ¨é¡µé¢è§¦å‘æ‡’åŠ è½½...")
            await page.evaluate("window.scrollTo(0, document.body.scrollHeight / 2)")
            await asyncio.sleep(2)
            
            # å†æ¬¡æ£€æŸ¥
            for selector in self.selectors['homepage_products']:
                try:
                    elements = await page.query_selector_all(selector)
                    if elements and len(elements) > 0:
                        print(f"âœ… æ»šåŠ¨åå‘ç° {len(elements)} ä¸ªå•†å“å…ƒç´ ")
                        return True
                except:
                    continue
            
            print("âš ï¸ æœªæ‰¾åˆ°å•†å“å…ƒç´ ï¼Œå¯èƒ½éœ€è¦ç™»å½•æˆ–é¡µé¢ç»“æ„å·²å˜åŒ–")
            return False
            
        except Exception as e:
            print(f"âŒ ç­‰å¾…å•†å“åŠ è½½å¤±è´¥: {e}")
            return False
    
    async def extract_product_info(self, product_element) -> Dict[str, Any]:
        """
        ä»å•†å“å…ƒç´ ä¸­æå–ä¿¡æ¯
        
        Args:
            product_element: å•†å“å…ƒç´ 
            
        Returns:
            Dict[str, Any]: å•†å“ä¿¡æ¯
        """
        product_info = {
            'title': '',
            'price': '',
            'image_url': '',
            'product_url': '',
            'shop_name': '',
            'sales_count': '',
            'raw_html': '',
            'extraction_time': datetime.now().isoformat()
        }
        
        try:
            # æå–å•†å“æ ‡é¢˜
            for selector in self.selectors['product_title']:
                try:
                    title_element = await product_element.query_selector(selector)
                    if title_element:
                        title = await title_element.text_content()
                        if title and title.strip():
                            product_info['title'] = title.strip()
                            break
                except:
                    continue
            
            # æå–å•†å“ä»·æ ¼
            for selector in self.selectors['product_price']:
                try:
                    price_element = await product_element.query_selector(selector)
                    if price_element:
                        price_text = await price_element.text_content()
                        if price_text:
                            # æå–æ•°å­—ä»·æ ¼
                            price_match = re.search(r'[\d,]+\.?\d*', price_text.replace('ï¿¥', '').replace('Â¥', ''))
                            if price_match:
                                product_info['price'] = price_match.group(0)
                                break
                except:
                    continue
            
            # æå–å•†å“å›¾ç‰‡
            for selector in self.selectors['product_image']:
                try:
                    img_element = await product_element.query_selector(selector)
                    if img_element:
                        img_src = await img_element.get_attribute('src')
                        if not img_src:
                            img_src = await img_element.get_attribute('data-src')
                        if img_src and img_src.startswith('http'):
                            product_info['image_url'] = img_src
                            break
                except:
                    continue
            
            # æå–å•†å“é“¾æ¥
            for selector in self.selectors['product_link']:
                try:
                    link_element = await product_element.query_selector(selector)
                    if link_element:
                        href = await link_element.get_attribute('href')
                        if href:
                            # å¤„ç†ç›¸å¯¹é“¾æ¥
                            if href.startswith('//'):
                                href = 'https:' + href
                            elif href.startswith('/'):
                                href = 'https://www.taobao.com' + href
                            product_info['product_url'] = href
                            break
                except:
                    continue
            
            # æå–åº—é“ºåç§°
            for selector in self.selectors['product_shop']:
                try:
                    shop_element = await product_element.query_selector(selector)
                    if shop_element:
                        shop_name = await shop_element.text_content()
                        if shop_name and shop_name.strip():
                            product_info['shop_name'] = shop_name.strip()
                            break
                except:
                    continue
            
            # æå–é”€é‡ä¿¡æ¯
            for selector in self.selectors['product_sales']:
                try:
                    sales_element = await product_element.query_selector(selector)
                    if sales_element:
                        sales_text = await sales_element.text_content()
                        if sales_text:
                            product_info['sales_count'] = sales_text.strip()
                            break
                except:
                    continue
            
            # æå–åŸå§‹HTMLï¼ˆç”¨äºè°ƒè¯•ï¼‰
            try:
                raw_html = await product_element.inner_html()
                product_info['raw_html'] = raw_html[:500] + '...' if len(raw_html) > 500 else raw_html
            except:
                pass
            
        except Exception as e:
            print(f"âš ï¸ æå–å•†å“ä¿¡æ¯æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        
        return product_info
    
    async def crawl_homepage_products(self, max_products: Optional[int] = None) -> List[Dict[str, Any]]:
        """
        çˆ¬å–é¦–é¡µå•†å“ä¿¡æ¯
        
        Args:
            max_products: æœ€å¤§å•†å“æ•°é‡é™åˆ¶
            
        Returns:
            List[Dict[str, Any]]: å•†å“ä¿¡æ¯åˆ—è¡¨
        """
        products = []
        
        try:
            print("ğŸ“¦ å¼€å§‹æå–é¦–é¡µå•†å“ä¿¡æ¯...")
            
            page = self.browser_service.get_page()
            if not page:
                print("âŒ æ— æ³•è·å–é¡µé¢å¯¹è±¡")
                return products
            
            # å°è¯•ä¸åŒçš„å•†å“é€‰æ‹©å™¨
            product_elements = []
            for selector in self.selectors['homepage_products']:
                try:
                    elements = await page.query_selector_all(selector)
                    if elements:
                        print(f"ğŸ” ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(elements)} ä¸ªå…ƒç´ ")
                        product_elements = elements
                        break
                except Exception as e:
                    print(f"âš ï¸ é€‰æ‹©å™¨ '{selector}' å¤±è´¥: {e}")
                    continue
            
            if not product_elements:
                print("âŒ æœªæ‰¾åˆ°ä»»ä½•å•†å“å…ƒç´ ")
                return products
            
            # é™åˆ¶å¤„ç†çš„å•†å“æ•°é‡
            if max_products:
                product_elements = product_elements[:max_products]
            
            print(f"ğŸ“‹ å¼€å§‹å¤„ç† {len(product_elements)} ä¸ªå•†å“...")
            
            # æå–æ¯ä¸ªå•†å“çš„ä¿¡æ¯
            for i, element in enumerate(product_elements):
                try:
                    print(f"   å¤„ç†å•†å“ {i+1}/{len(product_elements)}...")
                    
                    product_info = await self.extract_product_info(element)
                    
                    # éªŒè¯å•†å“ä¿¡æ¯çš„æœ‰æ•ˆæ€§
                    if product_info['title'] or product_info['price'] or product_info['image_url']:
                        products.append(product_info)
                        print(f"   âœ… å•†å“ {i+1}: {product_info['title'][:30]}...")
                    else:
                        print(f"   âš ï¸ å•†å“ {i+1}: ä¿¡æ¯ä¸å®Œæ•´ï¼Œè·³è¿‡")
                    
                    # æ·»åŠ å»¶è¿Ÿé¿å…è¿‡å¿«è¯·æ±‚
                    if i < len(product_elements) - 1:
                        await asyncio.sleep(self.request_delay)
                        
                except Exception as e:
                    print(f"   âŒ å¤„ç†å•†å“ {i+1} å¤±è´¥: {e}")
                    continue
            
            print(f"âœ… å•†å“ä¿¡æ¯æå–å®Œæˆï¼Œå…±è·å– {len(products)} ä¸ªæœ‰æ•ˆå•†å“")
            return products
            
        except Exception as e:
            print(f"âŒ çˆ¬å–å•†å“ä¿¡æ¯å¤±è´¥: {e}")
            return products
    
    async def analyze_page_structure(self) -> Dict[str, Any]:
        """
        åˆ†æé¡µé¢ç»“æ„ï¼Œç”¨äºè°ƒè¯•å’Œä¼˜åŒ–é€‰æ‹©å™¨
        
        Returns:
            Dict[str, Any]: é¡µé¢ç»“æ„åˆ†æç»“æœ
        """
        analysis = {
            'page_title': '',
            'page_url': '',
            'total_elements': 0,
            'potential_product_elements': {},
            'common_classes': [],
            'data_attributes': [],
            'network_requests': 0
        }
        
        try:
            print("ğŸ” å¼€å§‹åˆ†æé¡µé¢ç»“æ„...")
            
            page = self.browser_service.get_page()
            if not page:
                return analysis
            
            # åŸºæœ¬é¡µé¢ä¿¡æ¯
            analysis['page_title'] = await page.title()
            analysis['page_url'] = page.url
            
            # ç»Ÿè®¡å„ç§æ½œåœ¨çš„å•†å“å…ƒç´ 
            for selector in self.selectors['homepage_products']:
                try:
                    elements = await page.query_selector_all(selector)
                    analysis['potential_product_elements'][selector] = len(elements)
                except:
                    analysis['potential_product_elements'][selector] = 0
            
            # åˆ†æå¸¸è§çš„ç±»åå’Œå±æ€§
            common_info = await page.evaluate("""
                () => {
                    const allElements = document.querySelectorAll('*');
                    const classes = new Set();
                    const dataAttrs = new Set();
                    
                    Array.from(allElements).forEach(el => {
                        // æ”¶é›†ç±»å
                        if (el.className && typeof el.className === 'string') {
                            el.className.split(' ').forEach(cls => {
                                if (cls.includes('item') || cls.includes('product') || 
                                    cls.includes('goods') || cls.includes('card')) {
                                    classes.add(cls);
                                }
                            });
                        }
                        
                        // æ”¶é›† data å±æ€§
                        Array.from(el.attributes).forEach(attr => {
                            if (attr.name.startsWith('data-') && 
                                (attr.name.includes('spm') || attr.name.includes('product') || 
                                 attr.name.includes('item') || attr.name.includes('track'))) {
                                dataAttrs.add(attr.name);
                            }
                        });
                    });
                    
                    return {
                        classes: Array.from(classes).slice(0, 20),
                        dataAttrs: Array.from(dataAttrs).slice(0, 20),
                        totalElements: allElements.length
                    };
                }
            """)
            
            analysis['total_elements'] = common_info['totalElements']
            analysis['common_classes'] = common_info['classes']
            analysis['data_attributes'] = common_info['dataAttrs']
            
            print("âœ… é¡µé¢ç»“æ„åˆ†æå®Œæˆ")
            
        except Exception as e:
            print(f"âŒ é¡µé¢ç»“æ„åˆ†æå¤±è´¥: {e}")
        
        return analysis
    
    def print_products_summary(self, products: List[Dict[str, Any]]):
        """
        æ‰“å°å•†å“æ‘˜è¦ä¿¡æ¯
        
        Args:
            products: å•†å“åˆ—è¡¨
        """
        if not products:
            print("ğŸ“Š æ²¡æœ‰å•†å“æ•°æ®å¯æ˜¾ç¤º")
            return
        
        print(f"\nğŸ“Š æ·˜å®å•†å“æ•°æ®æ‘˜è¦ (å…± {len(products)} ä¸ªå•†å“):")
        print("=" * 100)
        
        for i, product in enumerate(products[:10], 1):  # åªæ˜¾ç¤ºå‰10ä¸ª
            title = product.get('title', 'N/A')[:40] + '...' if len(product.get('title', '')) > 40 else product.get('title', 'N/A')
            price = product.get('price', 'N/A')
            shop = product.get('shop_name', 'N/A')
            sales = product.get('sales_count', 'N/A')
            
            print(f"{i:3d}. æ ‡é¢˜: {title}")
            print(f"     ä»·æ ¼: Â¥{price:<10} | åº—é“º: {shop:<20} | é”€é‡: {sales}")
            print(f"     é“¾æ¥: {product.get('product_url', 'N/A')[:60]}...")
            print("-" * 100)
        
        if len(products) > 10:
            print(f"... è¿˜æœ‰ {len(products) - 10} ä¸ªå•†å“æœªæ˜¾ç¤º")
        
        print("=" * 100)
    
    def save_products_to_json(self, products: List[Dict[str, Any]], filename: Optional[str] = None):
        """
        ä¿å­˜å•†å“æ•°æ®åˆ°JSONæ–‡ä»¶
        
        Args:
            products: å•†å“åˆ—è¡¨
            filename: æ–‡ä»¶å
        """
        try:
            if not filename:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"taobao_products_{timestamp}.json"
            
            filepath = Path(filename)
            
            # å‡†å¤‡ä¿å­˜çš„æ•°æ®
            save_data = {
                'crawl_time': datetime.now().isoformat(),
                'source': 'taobao.com',
                'total_products': len(products),
                'products': products
            }
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(save_data, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ å•†å“æ•°æ®å·²ä¿å­˜åˆ°: {filepath.absolute()}")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            if self.browser_service:
                print("ğŸ§¹ æ­£åœ¨æ¸…ç†æµè§ˆå™¨èµ„æº...")
                await self.browser_service.shutdown()
                print("âœ… èµ„æºæ¸…ç†å®Œæˆ")
        except Exception as e:
            print(f"âš ï¸ æ¸…ç†èµ„æºæ—¶å‘ç”Ÿé”™è¯¯: {e}")


async def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("ğŸ¯ æ·˜å®å•†å“ä¿¡æ¯çˆ¬è™«æ¼”ç¤º")
    print("=" * 80)
    print()
    print("ğŸ“‹ åŠŸèƒ½è¯´æ˜:")
    print("1. ä½¿ç”¨ browser_service.py æœåŠ¡")
    print("2. è‡ªåŠ¨å¯¼èˆªåˆ°æ·˜å®é¦–é¡µ")
    print("3. æ™ºèƒ½è¯†åˆ«å•†å“å…ƒç´ ")
    print("4. æå–å•†å“è¯¦ç»†ä¿¡æ¯")
    print("5. åˆ†æé¡µé¢ç»“æ„")
    print("6. ä¿å­˜æ•°æ®åˆ°JSONæ–‡ä»¶")
    print()
    
    # é…ç½®å‚æ•°
    headless = False  # ä½¿ç”¨æœ‰å¤´æ¨¡å¼ä¾¿äºè§‚å¯Ÿ
    request_delay = 1.0  # è¯·æ±‚é—´éš”
    max_products = 20  # é™åˆ¶æœ€å¤§å•†å“æ•°é‡
    
    print(f"ğŸ–¥ï¸  æµè§ˆå™¨æ¨¡å¼: {'æ— å¤´æ¨¡å¼' if headless else 'æœ‰å¤´æ¨¡å¼'}")
    print(f"â±ï¸  è¯·æ±‚é—´éš”: {request_delay} ç§’")
    print(f"ğŸ“¦ æœ€å¤§å•†å“æ•°: {max_products}")
    print()
    
    # è¯¢é—®ç”¨æˆ·æ˜¯å¦ç»§ç»­
    try:
        response = input("ğŸ¤” æ˜¯å¦å¼€å§‹çˆ¬å–ï¼Ÿ(y/N): ").strip().lower()
        if response not in ['y', 'yes']:
            print("ğŸ‘‹ çˆ¬å–å·²å–æ¶ˆ")
            return
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­")
        return
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = TaobaoProductCrawler(headless=headless, request_delay=request_delay)
    
    try:
        # åˆå§‹åŒ–æµè§ˆå™¨
        success = await crawler.initialize()
        if not success:
            print("âŒ æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥ï¼Œé€€å‡ºç¨‹åº")
            return
        
        # å¯¼èˆªåˆ°æ·˜å®é¦–é¡µ
        success = await crawler.navigate_to_taobao()
        if not success:
            print("âŒ å¯¼èˆªåˆ°æ·˜å®å¤±è´¥ï¼Œé€€å‡ºç¨‹åº")
            return
        
        # ç­‰å¾…å•†å“åŠ è½½
        success = await crawler.wait_for_products_to_load()
        if not success:
            print("âš ï¸ å•†å“åŠ è½½å¯èƒ½ä¸å®Œæ•´ï¼Œç»§ç»­å°è¯•...")
        
        # åˆ†æé¡µé¢ç»“æ„ï¼ˆå¯é€‰ï¼‰
        print("\nğŸ” åˆ†æé¡µé¢ç»“æ„...")
        analysis = await crawler.analyze_page_structure()
        print(f"ğŸ“„ é¡µé¢æ ‡é¢˜: {analysis['page_title']}")
        print(f"ğŸ”¢ é¡µé¢å…ƒç´ æ€»æ•°: {analysis['total_elements']}")
        print(f"ğŸ¯ æ½œåœ¨å•†å“å…ƒç´ : {analysis['potential_product_elements']}")
        
        # å¼€å§‹çˆ¬å–å•†å“æ•°æ®
        products = await crawler.crawl_homepage_products(max_products=max_products)
        
        if products:
            # æ˜¾ç¤ºæ‘˜è¦
            crawler.print_products_summary(products)
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            crawler.save_products_to_json(products)
            
            print(f"\nğŸ‰ çˆ¬å–æˆåŠŸå®Œæˆï¼å…±è·å– {len(products)} ä¸ªå•†å“æ•°æ®")
        else:
            print("âŒ æœªè·å–åˆ°ä»»ä½•å•†å“æ•°æ®")
            print("ğŸ’¡ å¯èƒ½çš„åŸå› :")
            print("   - éœ€è¦ç™»å½•æ·˜å®è´¦å·")
            print("   - é¡µé¢ç»“æ„å‘ç”Ÿå˜åŒ–")
            print("   - è§¦å‘äº†åçˆ¬è™«æœºåˆ¶")
            print("   - ç½‘ç»œè¿æ¥é—®é¢˜")
    
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­çˆ¬å–")
    except Exception as e:
        print(f"\nâŒ çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
    finally:
        # æ¸…ç†èµ„æº
        await crawler.cleanup()


if __name__ == "__main__":
    # è¿è¡Œçˆ¬è™«
    asyncio.run(main())