"""
å¢å¼ºç‰ˆæ·˜å®å•†å“ä¿¡æ¯çˆ¬è™«
ç»“åˆ Chrome DevTools åˆ†æç»“æœå’Œ BrowserService æ¶æ„

æŠ€æœ¯ç‰¹æ€§ï¼š
1. åŸºäºé‡æ„åçš„ BrowserService æ¶æ„
2. å¤šé‡é€‰æ‹©å™¨ç­–ç•¥å’Œæ™ºèƒ½å…ƒç´ æ£€æµ‹
3. åçˆ¬è™«æœºåˆ¶æ£€æµ‹å’Œåº”å¯¹
4. ç½‘ç»œè¯·æ±‚ç›‘æ§å’Œåˆ†æ
5. æ•°æ®éªŒè¯å’Œæ¸…æ´—
6. è¯¦ç»†çš„é”™è¯¯æŠ¥å‘Šå’Œæ¢å¤æœºåˆ¶
7. Chrome DevTools é›†æˆåˆ†æ
"""

import asyncio
import json
import re
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
import logging

from ..rpa.browser.browser_service import BrowserService
from ..rpa.browser.core.models.browser_config import BrowserConfig, BrowserType

@dataclass
class ProductInfo:
    """å•†å“ä¿¡æ¯æ•°æ®ç±»"""
    title: str = ""
    price: str = ""
    image_url: str = ""
    product_url: str = ""
    shop_name: str = ""
    sales_count: str = ""
    rating: str = ""
    location: str = ""
    extracted_at: str = ""
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return asdict(self)

@dataclass
class CrawlResult:
    """çˆ¬å–ç»“æœæ•°æ®ç±»"""
    success: bool
    products: List[ProductInfo]
    errors: List[str]
    warnings: List[str]
    network_requests: List[Dict[str, Any]]
    page_analysis: Dict[str, Any]
    execution_time: float
    total_products: int

class TaobaoProductCrawler:
    """å¢å¼ºç‰ˆæ·˜å®å•†å“ä¿¡æ¯çˆ¬è™«"""
    
    def __init__(self, headless: bool = False, request_delay: float = 2.0):
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        Args:
            headless: æ˜¯å¦ä½¿ç”¨æ— å¤´æ¨¡å¼
            request_delay: è¯·æ±‚é—´éš”æ—¶é—´(ç§’)
        """
        self.headless = headless
        self.request_delay = request_delay
        self.browser_service = None
        self.start_time = None
        
        # é”™è¯¯å’Œè­¦å‘Šæ”¶é›†
        self.errors = []
        self.warnings = []
        self.network_requests = []
        
        # é…ç½®æ—¥å¿—
        self.logger = logging.getLogger(__name__)
        
        # åŸºäº Chrome DevTools åˆ†æçš„é€‰æ‹©å™¨ç­–ç•¥
        self.selectors = {
            'product_items': [
                '[data-spm*="product"]',
                '[data-category="auctions"]', 
                '.recommend-item',
                '.item',
                '.product-item',
                '.goods-item',
                '[class*="item"]',
                '[class*="product"]'
            ],
            'product_title': [
                '.title',
                '.item-title', 
                'h3',
                'h4',
                '[data-role="title"]',
                '.product-title'
            ],
            'product_price': [
                '.price',
                '.current-price',
                '[data-role="price"]',
                '.price-current',
                '.sale-price'
            ],
            'product_image': [
                'img[src*=".jpg"]',
                'img[data-src]',
                'img[src*=".png"]',
                '.product-img img'
            ],
            'product_link': [
                'a[href*="item.taobao.com"]',
                'a[href*="detail.tmall.com"]',
                'a[href*="product"]'
            ],
            'shop_name': [
                '.shop-name',
                '.store-name',
                '[data-role="shop"]'
            ],
            'sales_count': [
                '.sales',
                '.sold-count',
                '[data-role="sales"]'
            ]
        }
        
        # åçˆ¬è™«æ£€æµ‹å…³é”®è¯
        self.anti_crawling_indicators = [
            'éªŒè¯ç ',
            'captcha',
            'è¯·è¾“å…¥éªŒè¯ç ',
            'è®¿é—®è¿‡äºé¢‘ç¹',
            'è¯·ç¨åå†è¯•',
            'blocked',
            '403',
            '429'
        ]
    
    async def initialize(self) -> bool:
        """åˆå§‹åŒ–æµè§ˆå™¨æœåŠ¡"""
        try:
            # åˆ›å»ºæµè§ˆå™¨é…ç½®
            config = BrowserConfig(
                browser_type=BrowserType.CHROME,
                headless=self.headless,
                viewport={'width': 1920, 'height': 1080},
                default_timeout=30000,
                user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            )
            
            # åˆå§‹åŒ–æµè§ˆå™¨æœåŠ¡
            self.browser_service = BrowserService(config=config, debug_mode=True)
            result = await self.browser_service.initialize()
            
            if result:
                self.logger.info("ğŸš€ æ·˜å®çˆ¬è™«åˆå§‹åŒ–æˆåŠŸ")
                return True
            else:
                self.logger.error("âŒ æµè§ˆå™¨æœåŠ¡åˆå§‹åŒ–å¤±è´¥")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ çˆ¬è™«åˆå§‹åŒ–å¼‚å¸¸: {e}")
            return False
    
    async def crawl_products(self, search_keyword: str, max_pages: int = 3) -> CrawlResult:
        """
        çˆ¬å–å•†å“ä¿¡æ¯
        
        Args:
            search_keyword: æœç´¢å…³é”®è¯
            max_pages: æœ€å¤§çˆ¬å–é¡µæ•°
            
        Returns:
            CrawlResult: çˆ¬å–ç»“æœ
        """
        self.start_time = time.time()
        products = []
        
        try:
            if not self.browser_service:
                await self.initialize()
            
            # æ„å»ºæœç´¢URL
            search_url = f"https://s.taobao.com/search?q={search_keyword}"
            
            self.logger.info(f"ğŸ” å¼€å§‹æœç´¢å•†å“: {search_keyword}")
            
            for page in range(1, max_pages + 1):
                self.logger.info(f"ğŸ“„ æ­£åœ¨çˆ¬å–ç¬¬ {page} é¡µ")
                
                # å¯¼èˆªåˆ°æœç´¢é¡µé¢
                page_url = f"{search_url}&s={(page-1)*44}"
                success = await self.browser_service.navigate_to_url(page_url)
                
                if not success:
                    self.errors.append(f"å¯¼èˆªåˆ°ç¬¬ {page} é¡µå¤±è´¥")
                    continue
                
                # ç­‰å¾…é¡µé¢åŠ è½½
                await asyncio.sleep(self.request_delay)
                
                # æ£€æµ‹åçˆ¬è™«æœºåˆ¶
                if await self._detect_anti_crawling():
                    self.warnings.append(f"ç¬¬ {page} é¡µæ£€æµ‹åˆ°åçˆ¬è™«æœºåˆ¶")
                    break
                
                # æå–å•†å“ä¿¡æ¯
                page_products = await self._extract_products_from_page()
                products.extend(page_products)
                
                self.logger.info(f"âœ… ç¬¬ {page} é¡µæå–åˆ° {len(page_products)} ä¸ªå•†å“")
                
                # é¡µé¢é—´å»¶è¿Ÿ
                if page < max_pages:
                    await asyncio.sleep(self.request_delay)
            
            # ç”Ÿæˆç»“æœ
            execution_time = time.time() - self.start_time
            
            result = CrawlResult(
                success=len(products) > 0,
                products=products,
                errors=self.errors,
                warnings=self.warnings,
                network_requests=self.network_requests,
                page_analysis=await self._analyze_page_structure(),
                execution_time=execution_time,
                total_products=len(products)
            )
            
            self.logger.info(f"ğŸ‰ çˆ¬å–å®Œæˆ! æ€»å…±è·å– {len(products)} ä¸ªå•†å“ï¼Œè€—æ—¶ {execution_time:.2f} ç§’")
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ çˆ¬å–è¿‡ç¨‹å¼‚å¸¸: {e}")
            self.errors.append(str(e))
            
            return CrawlResult(
                success=False,
                products=products,
                errors=self.errors,
                warnings=self.warnings,
                network_requests=self.network_requests,
                page_analysis={},
                execution_time=time.time() - self.start_time if self.start_time else 0,
                total_products=len(products)
            )
    
    async def _detect_anti_crawling(self) -> bool:
        """æ£€æµ‹åçˆ¬è™«æœºåˆ¶"""
        try:
            page_content = await self.browser_service.get_page_content()
            
            for indicator in self.anti_crawling_indicators:
                if indicator.lower() in page_content.lower():
                    self.logger.warning(f"âš ï¸ æ£€æµ‹åˆ°åçˆ¬è™«æŒ‡ç¤ºå™¨: {indicator}")
                    return True
            
            return False
            
        except Exception as e:
            self.logger.error(f"âŒ åçˆ¬è™«æ£€æµ‹å¼‚å¸¸: {e}")
            return False
    
    async def _extract_products_from_page(self) -> List[ProductInfo]:
        """ä»å½“å‰é¡µé¢æå–å•†å“ä¿¡æ¯"""
        products = []
        
        try:
            # ä½¿ç”¨ JavaScript æ‰§è¡Œæå–é€»è¾‘
            extraction_script = """
            () => {
                const products = [];
                
                // å°è¯•å¤šç§å•†å“å®¹å™¨é€‰æ‹©å™¨
                const selectors = [
                    '[data-spm*="product"]',
                    '[data-category="auctions"]', 
                    '.recommend-item',
                    '.item',
                    '.product-item'
                ];
                
                let productElements = [];
                for (const selector of selectors) {
                    productElements = document.querySelectorAll(selector);
                    if (productElements.length > 0) break;
                }
                
                productElements.forEach((element, index) => {
                    try {
                        const product = {
                            title: '',
                            price: '',
                            image_url: '',
                            product_url: '',
                            shop_name: '',
                            sales_count: '',
                            rating: '',
                            location: '',
                            extracted_at: new Date().toISOString()
                        };
                        
                        // æå–æ ‡é¢˜
                        const titleSelectors = ['.title', '.item-title', 'h3', 'h4'];
                        for (const sel of titleSelectors) {
                            const titleEl = element.querySelector(sel);
                            if (titleEl && titleEl.textContent.trim()) {
                                product.title = titleEl.textContent.trim();
                                break;
                            }
                        }
                        
                        // æå–ä»·æ ¼
                        const priceSelectors = ['.price', '.current-price', '[data-role="price"]'];
                        for (const sel of priceSelectors) {
                            const priceEl = element.querySelector(sel);
                            if (priceEl && priceEl.textContent.trim()) {
                                const priceText = priceEl.textContent.trim();
                                const priceMatch = priceText.match(/[\\d,]+\\.?\\d*/);
                                if (priceMatch) {
                                    product.price = priceMatch[0];
                                    break;
                                }
                            }
                        }
                        
                        // æå–å›¾ç‰‡
                        const imgSelectors = ['img[src*=".jpg"]', 'img[data-src]', 'img'];
                        for (const sel of imgSelectors) {
                            const imgEl = element.querySelector(sel);
                            if (imgEl) {
                                product.image_url = imgEl.src || imgEl.dataset.src || '';
                                if (product.image_url) break;
                            }
                        }
                        
                        // æå–é“¾æ¥
                        const linkSelectors = ['a[href*="item.taobao.com"]', 'a[href*="detail.tmall.com"]', 'a'];
                        for (const sel of linkSelectors) {
                            const linkEl = element.querySelector(sel);
                            if (linkEl && linkEl.href) {
                                product.product_url = linkEl.href;
                                break;
                            }
                        }
                        
                        // æå–åº—é“ºåç§°
                        const shopSelectors = ['.shop-name', '.store-name'];
                        for (const sel of shopSelectors) {
                            const shopEl = element.querySelector(sel);
                            if (shopEl && shopEl.textContent.trim()) {
                                product.shop_name = shopEl.textContent.trim();
                                break;
                            }
                        }
                        
                        // åªæœ‰æ ‡é¢˜ä¸ä¸ºç©ºæ‰æ·»åŠ å•†å“
                        if (product.title) {
                            products.push(product);
                        }
                        
                    } catch (e) {
                        console.error('æå–å•†å“ä¿¡æ¯å¼‚å¸¸:', e);
                    }
                });
                
                return products;
            }
            """
            
            # æ‰§è¡Œæå–è„šæœ¬
            if hasattr(self.browser_service.browser_driver, 'page') and self.browser_service.browser_driver.page:
                raw_products = await self.browser_service.browser_driver.page.evaluate(extraction_script)
                
                # è½¬æ¢ä¸º ProductInfo å¯¹è±¡
                for raw_product in raw_products:
                    product = ProductInfo(
                        title=raw_product.get('title', ''),
                        price=raw_product.get('price', ''),
                        image_url=raw_product.get('image_url', ''),
                        product_url=raw_product.get('product_url', ''),
                        shop_name=raw_product.get('shop_name', ''),
                        sales_count=raw_product.get('sales_count', ''),
                        rating=raw_product.get('rating', ''),
                        location=raw_product.get('location', ''),
                        extracted_at=raw_product.get('extracted_at', datetime.now().isoformat())
                    )
                    
                    # æ•°æ®éªŒè¯å’Œæ¸…æ´—
                    if self._validate_product(product):
                        products.append(product)
                
            return products
            
        except Exception as e:
            self.logger.error(f"âŒ æå–å•†å“ä¿¡æ¯å¼‚å¸¸: {e}")
            return []
    
    def _validate_product(self, product: ProductInfo) -> bool:
        """éªŒè¯å•†å“ä¿¡æ¯"""
        # åŸºæœ¬éªŒè¯ï¼šæ ‡é¢˜ä¸èƒ½ä¸ºç©º
        if not product.title or len(product.title.strip()) < 3:
            return False
        
        # ä»·æ ¼éªŒè¯
        if product.price:
            try:
                float(product.price.replace(',', ''))
            except ValueError:
                product.price = ""  # æ¸…ç©ºæ— æ•ˆä»·æ ¼
        
        # URLéªŒè¯
        if product.product_url and not (
            'taobao.com' in product.product_url or 
            'tmall.com' in product.product_url
        ):
            product.product_url = ""
        
        return True
    
    async def _analyze_page_structure(self) -> Dict[str, Any]:
        """åˆ†æé¡µé¢ç»“æ„"""
        try:
            analysis_script = """
            () => {
                return {
                    total_elements: document.querySelectorAll('*').length,
                    product_containers: document.querySelectorAll('[data-spm*="product"]').length,
                    images: document.querySelectorAll('img').length,
                    links: document.querySelectorAll('a').length,
                    page_title: document.title,
                    page_url: window.location.href,
                    viewport: {
                        width: window.innerWidth,
                        height: window.innerHeight
                    },
                    load_state: document.readyState
                };
            }
            """
            
            if hasattr(self.browser_service.browser_driver, 'page') and self.browser_service.browser_driver.page:
                return await self.browser_service.browser_driver.page.evaluate(analysis_script)
            
            return {}
            
        except Exception as e:
            self.logger.error(f"âŒ é¡µé¢ç»“æ„åˆ†æå¼‚å¸¸: {e}")
            return {}
    
    async def save_results(self, result: CrawlResult, output_file: str = None) -> str:
        """ä¿å­˜çˆ¬å–ç»“æœ"""
        if not output_file:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"taobao_products_{timestamp}.json"
        
        try:
            # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
            data = {
                'success': result.success,
                'total_products': result.total_products,
                'execution_time': result.execution_time,
                'products': [product.to_dict() for product in result.products],
                'errors': result.errors,
                'warnings': result.warnings,
                'page_analysis': result.page_analysis,
                'crawl_metadata': {
                    'timestamp': datetime.now().isoformat(),
                    'crawler_version': '2.0',
                    'browser_service': 'enhanced'
                }
            }
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            output_path = Path(output_file)
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_path.absolute()}")
            return str(output_path.absolute())
            
        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜ç»“æœå¼‚å¸¸: {e}")
            return ""
    
    async def close(self):
        """å…³é—­çˆ¬è™«"""
        if self.browser_service:
            await self.browser_service.close()
            self.logger.info("ğŸ”’ çˆ¬è™«å·²å…³é—­")

# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•å‡½æ•°
async def main():
    """ä¸»å‡½æ•°ç¤ºä¾‹"""
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = TaobaoProductCrawler(headless=True, request_delay=3.0)
    
    try:
        # åˆå§‹åŒ–
        if not await crawler.initialize():
            print("âŒ çˆ¬è™«åˆå§‹åŒ–å¤±è´¥")
            return
        
        # çˆ¬å–å•†å“
        result = await crawler.crawl_products("iPhone 15", max_pages=2)
        
        # è¾“å‡ºç»“æœ
        print(f"\nğŸ¯ çˆ¬å–ç»“æœ:")
        print(f"   æˆåŠŸ: {result.success}")
        print(f"   å•†å“æ•°é‡: {result.total_products}")
        print(f"   æ‰§è¡Œæ—¶é—´: {result.execution_time:.2f} ç§’")
        print(f"   é”™è¯¯æ•°é‡: {len(result.errors)}")
        print(f"   è­¦å‘Šæ•°é‡: {len(result.warnings)}")
        
        if result.products:
            print(f"\nğŸ“¦ å•†å“ç¤ºä¾‹:")
            for i, product in enumerate(result.products[:3]):
                print(f"   {i+1}. {product.title[:50]}...")
                print(f"      ä»·æ ¼: {product.price}")
                print(f"      åº—é“º: {product.shop_name}")
        
        # ä¿å­˜ç»“æœ
        output_file = await crawler.save_results(result)
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
    except Exception as e:
        print(f"âŒ è¿è¡Œå¼‚å¸¸: {e}")
    
    finally:
        # å…³é—­çˆ¬è™«
        await crawler.close()

if __name__ == "__main__":
    asyncio.run(main())