"""
é«˜çº§æ·˜å®å•†å“ä¿¡æ¯çˆ¬è™«
ç»“åˆ Chrome DevTools MCP çš„ç½‘ç»œç›‘æ§å’Œå…ƒç´ åˆ†æåŠŸèƒ½

æŠ€æœ¯ç‰¹æ€§ï¼š
1. ç½‘ç»œè¯·æ±‚ç›‘æ§å’Œåˆ†æ
2. åŠ¨æ€å…ƒç´ æ£€æµ‹å’Œé€‚é…
3. åçˆ¬è™«æœºåˆ¶æ£€æµ‹å’Œåº”å¯¹
4. å¤šé‡é€‰æ‹©å™¨ç­–ç•¥
5. æ•°æ®éªŒè¯å’Œæ¸…æ´—
6. è¯¦ç»†çš„é”™è¯¯æŠ¥å‘Šå’Œæ¢å¤æœºåˆ¶
"""

import asyncio
import json
import re
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass

from rpa.browser.browser_service import BrowserService
from rpa.browser.config import BrowserConfig


@dataclass
class CrawlResult:
    """çˆ¬å–ç»“æœæ•°æ®ç±»"""
    success: bool
    products: List[Dict[str, Any]]
    errors: List[str]
    warnings: List[str]
    network_requests: List[Dict[str, Any]]
    page_analysis: Dict[str, Any]
    execution_time: float


class AdvancedTaobaoCrawler:
    """é«˜çº§æ·˜å®å•†å“ä¿¡æ¯çˆ¬è™«"""
    
    def __init__(self, headless: bool = False, request_delay: float = 2.0):
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        Args:
            headless: æ˜¯å¦ä½¿ç”¨æ— å¤´æ¨¡å¼
            request_delay: è¯·æ±‚é—´éš”æ—¶é—´(ç§’)
        """
        self.headless = headless
        self.request_delay = request_delay
        self.browser_service = None
        self.start_time = None
        
        # é”™è¯¯å’Œè­¦å‘Šæ”¶é›†
        self.errors = []
        self.warnings = []
        self.network_requests = []
        
        # é…ç½®æµè§ˆå™¨
        self.browser_config = BrowserConfig(
            browser_type='chrome',
            headless=headless,
            user_data_dir=None,
            profile_name="AdvancedTaobaoCrawler",
            viewport_width=1920,
            viewport_height=1080,
            user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            extra_args=[
                '--disable-blink-features=AutomationControlled',
                '--disable-dev-shm-usage',
                '--no-sandbox',
                '--disable-setuid-sandbox',
                '--disable-web-security',
                '--allow-running-insecure-content'
            ]
        )
        
        # å¤šå±‚çº§é€‰æ‹©å™¨ç­–ç•¥
        self.selector_strategies = {
            'primary': {
                'products': [
                    '[data-spm*="product"]',
                    '[data-category="auctions"]',
                    '.recommend-item',
                    '.item'
                ],
                'title': ['.title', '.item-title', 'h3', 'h4'],
                'price': ['.price', '.current-price', '[data-role="price"]'],
                'image': ['img[src*="jpg"]', 'img[src*="png"]', 'img[src*="webp"]'],
                'link': ['a[href*="item.taobao.com"]', 'a[href*="detail.tmall.com"]']
            },
            'fallback': {
                'products': [
                    '.product-item',
                    '.goods-item',
                    '.card-item',
                    '[class*="item"]',
                    '[class*="product"]'
                ],
                'title': ['.name', '.product-name', '.goods-name', 'span[title]'],
                'price': ['.sale-price', '.rmb', '.yuan', '[class*="price"]'],
                'image': ['img[data-src]', 'img[lazy-src]', '.pic img'],
                'link': ['a[href*="tmall.com"]', 'a[href*="taobao.com"]']
            },
            'aggressive': {
                'products': [
                    'div[class*="item"]',
                    'li[class*="item"]',
                    'div[class*="card"]',
                    'div[data-*]'
                ],
                'title': ['*[title]', 'span', 'div', 'p'],
                'price': ['*[class*="price"]', '*[class*="rmb"]', '*[class*="yuan"]'],
                'image': ['img'],
                'link': ['a[href]']
            }
        }
    
    async def initialize(self) -> bool:
        """åˆå§‹åŒ–æµè§ˆå™¨æœåŠ¡"""
        try:
            print("ğŸš€ æ­£åœ¨åˆå§‹åŒ–é«˜çº§æµè§ˆå™¨æœåŠ¡...")
            self.start_time = time.time()
            
            self.browser_service = BrowserService(self.browser_config)
            success = await self.browser_service.start()
            
            if not success:
                self.errors.append("æµè§ˆå™¨å¯åŠ¨å¤±è´¥")
                return False
            
            # è®¾ç½®ç½‘ç»œç›‘æ§
            await self._setup_network_monitoring()
            
            print("âœ… é«˜çº§æµè§ˆå™¨æœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
            return True
            
        except Exception as e:
            error_msg = f"åˆå§‹åŒ–å¤±è´¥: {e}"
            self.errors.append(error_msg)
            print(f"âŒ {error_msg}")
            return False
    
    async def _setup_network_monitoring(self):
        """è®¾ç½®ç½‘ç»œè¯·æ±‚ç›‘æ§"""
        try:
            page = self.browser_service.get_page()
            if not page:
                return
            
            # ç›‘å¬ç½‘ç»œè¯·æ±‚
            async def handle_request(request):
                request_info = {
                    'url': request.url,
                    'method': request.method,
                    'headers': dict(request.headers),
                    'timestamp': datetime.now().isoformat(),
                    'resource_type': request.resource_type
                }
                self.network_requests.append(request_info)
            
            # ç›‘å¬ç½‘ç»œå“åº”
            async def handle_response(response):
                for req in self.network_requests:
                    if req['url'] == response.url:
                        req['status'] = response.status
                        req['response_headers'] = dict(response.headers)
                        break
            
            page.on('request', handle_request)
            page.on('response', handle_response)
            
            print("ğŸ“¡ ç½‘ç»œç›‘æ§å·²å¯ç”¨")
            
        except Exception as e:
            warning_msg = f"ç½‘ç»œç›‘æ§è®¾ç½®å¤±è´¥: {e}"
            self.warnings.append(warning_msg)
            print(f"âš ï¸ {warning_msg}")
    
    async def navigate_to_taobao(self) -> bool:
        """å¯¼èˆªåˆ°æ·˜å®é¦–é¡µå¹¶è¿›è¡Œæ™ºèƒ½æ£€æµ‹"""
        try:
            print("ğŸŒ æ­£åœ¨å¯¼èˆªåˆ°æ·˜å®é¦–é¡µ...")
            
            success = await self.browser_service.navigate_to_url("https://www.taobao.com")
            if not success:
                self.errors.append("å¯¼èˆªåˆ°æ·˜å®é¦–é¡µå¤±è´¥")
                return False
            
            # ç­‰å¾…é¡µé¢åŸºæœ¬åŠ è½½
            await asyncio.sleep(3)
            
            # æ£€æµ‹é¡µé¢çŠ¶æ€
            page_status = await self._detect_page_status()
            
            if page_status['needs_login']:
                self.warnings.append("é¡µé¢å¯èƒ½éœ€è¦ç™»å½•")
                print("âš ï¸ æ£€æµ‹åˆ°ç™»å½•æç¤ºï¼Œéƒ¨åˆ†åŠŸèƒ½å¯èƒ½å—é™")
            
            if page_status['has_captcha']:
                self.warnings.append("æ£€æµ‹åˆ°éªŒè¯ç ")
                print("âš ï¸ æ£€æµ‹åˆ°éªŒè¯ç ï¼Œå¯èƒ½è§¦å‘äº†åçˆ¬è™«æœºåˆ¶")
            
            if page_status['is_blocked']:
                self.errors.append("é¡µé¢è¢«é˜»æ­¢è®¿é—®")
                print("âŒ é¡µé¢è®¿é—®è¢«é˜»æ­¢")
                return False
            
            print(f"âœ… æˆåŠŸå¯¼èˆªåˆ°æ·˜å®é¦–é¡µ: {page_status['title']}")
            return True
            
        except Exception as e:
            error_msg = f"å¯¼èˆªå¤±è´¥: {e}"
            self.errors.append(error_msg)
            print(f"âŒ {error_msg}")
            return False
    
    async def _detect_page_status(self) -> Dict[str, Any]:
        """æ£€æµ‹é¡µé¢çŠ¶æ€"""
        status = {
            'title': '',
            'needs_login': False,
            'has_captcha': False,
            'is_blocked': False,
            'has_products': False
        }
        
        try:
            page = self.browser_service.get_page()
            if not page:
                return status
            
            # è·å–é¡µé¢æ ‡é¢˜
            status['title'] = await page.title()
            
            # æ£€æµ‹ç™»å½•ç›¸å…³å…ƒç´ 
            login_indicators = [
                'text="ç™»å½•"',
                'text="è¯·ç™»å½•"',
                '[class*="login"]',
                '#login',
                '.login-form'
            ]
            
            for indicator in login_indicators:
                try:
                    element = await page.query_selector(indicator)
                    if element:
                        status['needs_login'] = True
                        break
                except:
                    continue
            
            # æ£€æµ‹éªŒè¯ç 
            captcha_indicators = [
                'text="éªŒè¯ç "',
                '[class*="captcha"]',
                '[class*="verify"]',
                'canvas',
                '.nc_wrapper'
            ]
            
            for indicator in captcha_indicators:
                try:
                    element = await page.query_selector(indicator)
                    if element:
                        status['has_captcha'] = True
                        break
                except:
                    continue
            
            # æ£€æµ‹æ˜¯å¦è¢«é˜»æ­¢
            blocked_indicators = [
                'text="è®¿é—®è¢«æ‹’ç»"',
                'text="403"',
                'text="404"',
                'text="æœåŠ¡å™¨é”™è¯¯"'
            ]
            
            for indicator in blocked_indicators:
                try:
                    element = await page.query_selector(indicator)
                    if element:
                        status['is_blocked'] = True
                        break
                except:
                    continue
            
            # æ£€æµ‹æ˜¯å¦æœ‰å•†å“
            for strategy_name, selectors in self.selector_strategies.items():
                for selector in selectors['products']:
                    try:
                        elements = await page.query_selector_all(selector)
                        if elements and len(elements) > 0:
                            status['has_products'] = True
                            break
                    except:
                        continue
                if status['has_products']:
                    break
            
        except Exception as e:
            self.warnings.append(f"é¡µé¢çŠ¶æ€æ£€æµ‹å¤±è´¥: {e}")
        
        return status
    
    async def smart_wait_for_products(self, timeout: int = 30) -> bool:
        """æ™ºèƒ½ç­‰å¾…å•†å“åŠ è½½"""
        try:
            print("â³ æ™ºèƒ½ç­‰å¾…å•†å“æ•°æ®åŠ è½½...")
            
            page = self.browser_service.get_page()
            if not page:
                return False
            
            start_time = time.time()
            
            # å¤šé˜¶æ®µç­‰å¾…ç­–ç•¥
            while time.time() - start_time < timeout:
                # é˜¶æ®µ1ï¼šç­‰å¾…ç½‘ç»œç©ºé—²
                try:
                    await page.wait_for_load_state('networkidle', timeout=5000)
                except:
                    pass
                
                # é˜¶æ®µ2ï¼šæ£€æµ‹å•†å“å…ƒç´ 
                found_products = False
                for strategy_name, selectors in self.selector_strategies.items():
                    for selector in selectors['products']:
                        try:
                            elements = await page.query_selector_all(selector)
                            if elements and len(elements) > 0:
                                print(f"âœ… ä½¿ç”¨ {strategy_name} ç­–ç•¥å‘ç° {len(elements)} ä¸ªå•†å“å…ƒç´ ")
                                return True
                        except:
                            continue
                
                # é˜¶æ®µ3ï¼šå°è¯•æ»šåŠ¨è§¦å‘æ‡’åŠ è½½
                if not found_products:
                    print("ğŸ“œ å°è¯•æ»šåŠ¨è§¦å‘æ‡’åŠ è½½...")
                    await page.evaluate("window.scrollTo(0, document.body.scrollHeight / 3)")
                    await asyncio.sleep(2)
                    
                    await page.evaluate("window.scrollTo(0, document.body.scrollHeight / 2)")
                    await asyncio.sleep(2)
                
                await asyncio.sleep(1)
            
            self.warnings.append("å•†å“åŠ è½½è¶…æ—¶")
            print("âš ï¸ å•†å“åŠ è½½è¶…æ—¶ï¼Œä½†å°†ç»§ç»­å°è¯•æå–")
            return False
            
        except Exception as e:
            warning_msg = f"ç­‰å¾…å•†å“åŠ è½½å¤±è´¥: {e}"
            self.warnings.append(warning_msg)
            print(f"âš ï¸ {warning_msg}")
            return False
    
    async def extract_products_with_strategies(self, max_products: Optional[int] = None) -> List[Dict[str, Any]]:
        """ä½¿ç”¨å¤šé‡ç­–ç•¥æå–å•†å“ä¿¡æ¯"""
        products = []
        
        try:
            print("ğŸ“¦ å¼€å§‹ä½¿ç”¨å¤šé‡ç­–ç•¥æå–å•†å“ä¿¡æ¯...")
            
            page = self.browser_service.get_page()
            if not page:
                self.errors.append("æ— æ³•è·å–é¡µé¢å¯¹è±¡")
                return products
            
            # å°è¯•ä¸åŒçš„ç­–ç•¥
            for strategy_name, selectors in self.selector_strategies.items():
                print(f"ğŸ¯ å°è¯• {strategy_name} ç­–ç•¥...")
                
                product_elements = []
                for selector in selectors['products']:
                    try:
                        elements = await page.query_selector_all(selector)
                        if elements:
                            print(f"   ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(elements)} ä¸ªå…ƒç´ ")
                            product_elements = elements
                            break
                    except Exception as e:
                        continue
                
                if product_elements:
                    print(f"âœ… {strategy_name} ç­–ç•¥æˆåŠŸï¼Œæ‰¾åˆ° {len(product_elements)} ä¸ªå•†å“å…ƒç´ ")
                    
                    # é™åˆ¶å¤„ç†æ•°é‡
                    if max_products:
                        product_elements = product_elements[:max_products]
                    
                    # æå–å•†å“ä¿¡æ¯
                    strategy_products = await self._extract_products_with_selectors(
                        product_elements, selectors, strategy_name
                    )
                    
                    if strategy_products:
                        products.extend(strategy_products)
                        print(f"âœ… {strategy_name} ç­–ç•¥æå–åˆ° {len(strategy_products)} ä¸ªæœ‰æ•ˆå•†å“")
                        break
                    else:
                        print(f"âš ï¸ {strategy_name} ç­–ç•¥æœªæå–åˆ°æœ‰æ•ˆå•†å“")
                else:
                    print(f"âŒ {strategy_name} ç­–ç•¥æœªæ‰¾åˆ°å•†å“å…ƒç´ ")
            
            if not products:
                self.errors.append("æ‰€æœ‰ç­–ç•¥éƒ½æœªèƒ½æå–åˆ°å•†å“")
            
            return products
            
        except Exception as e:
            error_msg = f"å•†å“æå–å¤±è´¥: {e}"
            self.errors.append(error_msg)
            print(f"âŒ {error_msg}")
            return products
    
    async def _extract_products_with_selectors(self, elements: List, selectors: Dict, strategy_name: str) -> List[Dict[str, Any]]:
        """ä½¿ç”¨æŒ‡å®šé€‰æ‹©å™¨æå–å•†å“ä¿¡æ¯"""
        products = []
        
        for i, element in enumerate(elements):
            try:
                print(f"   å¤„ç†å•†å“ {i+1}/{len(elements)}...")
                
                product_info = {
                    'extraction_strategy': strategy_name,
                    'element_index': i,
                    'title': '',
                    'price': '',
                    'image_url': '',
                    'product_url': '',
                    'shop_name': '',
                    'sales_count': '',
                    'extraction_time': datetime.now().isoformat(),
                    'confidence_score': 0.0
                }
                
                # æå–å„å­—æ®µä¿¡æ¯
                confidence_score = 0.0
                
                # æå–æ ‡é¢˜
                title = await self._extract_field_with_selectors(element, selectors['title'])
                if title:
                    product_info['title'] = title
                    confidence_score += 0.3
                
                # æå–ä»·æ ¼
                price = await self._extract_price_with_selectors(element, selectors['price'])
                if price:
                    product_info['price'] = price
                    confidence_score += 0.3
                
                # æå–å›¾ç‰‡
                image_url = await self._extract_image_with_selectors(element, selectors['image'])
                if image_url:
                    product_info['image_url'] = image_url
                    confidence_score += 0.2
                
                # æå–é“¾æ¥
                product_url = await self._extract_link_with_selectors(element, selectors['link'])
                if product_url:
                    product_info['product_url'] = product_url
                    confidence_score += 0.2
                
                product_info['confidence_score'] = confidence_score
                
                # éªŒè¯å•†å“ä¿¡æ¯è´¨é‡
                if confidence_score >= 0.3:  # è‡³å°‘éœ€è¦æ ‡é¢˜æˆ–ä»·æ ¼
                    products.append(product_info)
                    print(f"   âœ… å•†å“ {i+1}: {product_info['title'][:30]}... (ç½®ä¿¡åº¦: {confidence_score:.2f})")
                else:
                    print(f"   âš ï¸ å•†å“ {i+1}: ä¿¡æ¯ä¸è¶³ï¼Œè·³è¿‡ (ç½®ä¿¡åº¦: {confidence_score:.2f})")
                
                # æ·»åŠ å»¶è¿Ÿ
                if i < len(elements) - 1:
                    await asyncio.sleep(self.request_delay)
                    
            except Exception as e:
                print(f"   âŒ å¤„ç†å•†å“ {i+1} å¤±è´¥: {e}")
                continue
        
        return products
    
    async def _extract_field_with_selectors(self, element, selectors: List[str]) -> str:
        """ä½¿ç”¨å¤šä¸ªé€‰æ‹©å™¨æå–å­—æ®µ"""
        for selector in selectors:
            try:
                field_element = await element.query_selector(selector)
                if field_element:
                    text = await field_element.text_content()
                    if text and text.strip():
                        return text.strip()
            except:
                continue
        return ""
    
    async def _extract_price_with_selectors(self, element, selectors: List[str]) -> str:
        """æå–ä»·æ ¼ä¿¡æ¯"""
        for selector in selectors:
            try:
                price_element = await element.query_selector(selector)
                if price_element:
                    price_text = await price_element.text_content()
                    if price_text:
                        # æå–æ•°å­—ä»·æ ¼
                        price_match = re.search(r'[\d,]+\.?\d*', price_text.replace('ï¿¥', '').replace('Â¥', ''))
                        if price_match:
                            return price_match.group(0)
            except:
                continue
        return ""
    
    async def _extract_image_with_selectors(self, element, selectors: List[str]) -> str:
        """æå–å›¾ç‰‡URL"""
        for selector in selectors:
            try:
                img_element = await element.query_selector(selector)
                if img_element:
                    # å°è¯•å¤šä¸ªå±æ€§
                    for attr in ['src', 'data-src', 'lazy-src', 'data-original']:
                        img_src = await img_element.get_attribute(attr)
                        if img_src and img_src.startswith('http'):
                            return img_src
            except:
                continue
        return ""
    
    async def _extract_link_with_selectors(self, element, selectors: List[str]) -> str:
        """æå–å•†å“é“¾æ¥"""
        for selector in selectors:
            try:
                link_element = await element.query_selector(selector)
                if link_element:
                    href = await link_element.get_attribute('href')
                    if href:
                        # å¤„ç†ç›¸å¯¹é“¾æ¥
                        if href.startswith('//'):
                            href = 'https:' + href
                        elif href.startswith('/'):
                            href = 'https://www.taobao.com' + href
                        return href
            except:
                continue
        return ""
    
    async def analyze_page_deep(self) -> Dict[str, Any]:
        """æ·±åº¦åˆ†æé¡µé¢ç»“æ„"""
        analysis = {
            'page_info': {},
            'element_analysis': {},
            'network_analysis': {},
            'performance_metrics': {},
            'recommendations': []
        }
        
        try:
            print("ğŸ” å¼€å§‹æ·±åº¦é¡µé¢åˆ†æ...")
            
            page = self.browser_service.get_page()
            if not page:
                return analysis
            
            # é¡µé¢åŸºæœ¬ä¿¡æ¯
            analysis['page_info'] = {
                'title': await page.title(),
                'url': page.url,
                'viewport': await page.viewport_size(),
                'user_agent': await page.evaluate('navigator.userAgent')
            }
            
            # å…ƒç´ åˆ†æ
            element_stats = await page.evaluate("""
                () => {
                    const stats = {
                        totalElements: document.querySelectorAll('*').length,
                        images: document.querySelectorAll('img').length,
                        links: document.querySelectorAll('a').length,
                        forms: document.querySelectorAll('form').length,
                        scripts: document.querySelectorAll('script').length,
                        iframes: document.querySelectorAll('iframe').length
                    };
                    
                    // åˆ†æå¯èƒ½çš„å•†å“å®¹å™¨
                    const productContainers = [];
                    const possibleSelectors = [
                        '[data-spm*="product"]',
                        '[class*="item"]',
                        '[class*="product"]',
                        '[class*="goods"]',
                        '[class*="card"]'
                    ];
                    
                    possibleSelectors.forEach(selector => {
                        try {
                            const elements = document.querySelectorAll(selector);
                            if (elements.length > 0) {
                                productContainers.push({
                                    selector: selector,
                                    count: elements.length
                                });
                            }
                        } catch (e) {}
                    });
                    
                    stats.productContainers = productContainers;
                    return stats;
                }
            """)
            
            analysis['element_analysis'] = element_stats
            
            # ç½‘ç»œåˆ†æ
            analysis['network_analysis'] = {
                'total_requests': len(self.network_requests),
                'request_types': {},
                'failed_requests': [],
                'slow_requests': []
            }
            
            # ç»Ÿè®¡è¯·æ±‚ç±»å‹
            for req in self.network_requests:
                req_type = req.get('resource_type', 'unknown')
                analysis['network_analysis']['request_types'][req_type] = \
                    analysis['network_analysis']['request_types'].get(req_type, 0) + 1
                
                # æ£€æŸ¥å¤±è´¥çš„è¯·æ±‚
                if req.get('status', 0) >= 400:
                    analysis['network_analysis']['failed_requests'].append({
                        'url': req['url'],
                        'status': req.get('status'),
                        'method': req['method']
                    })
            
            # æ€§èƒ½æŒ‡æ ‡
            performance = await page.evaluate("""
                () => {
                    const perf = performance.getEntriesByType('navigation')[0];
                    return {
                        loadTime: perf ? perf.loadEventEnd - perf.loadEventStart : 0,
                        domContentLoaded: perf ? perf.domContentLoadedEventEnd - perf.domContentLoadedEventStart : 0,
                        firstPaint: performance.getEntriesByName('first-paint')[0]?.startTime || 0,
                        firstContentfulPaint: performance.getEntriesByName('first-contentful-paint')[0]?.startTime || 0
                    };
                }
            """)
            
            analysis['performance_metrics'] = performance
            
            # ç”Ÿæˆå»ºè®®
            recommendations = []
            
            if element_stats['productContainers']:
                recommendations.append("å‘ç°å¤šä¸ªæ½œåœ¨çš„å•†å“å®¹å™¨ï¼Œå»ºè®®ä½¿ç”¨å¤šç­–ç•¥æå–")
            else:
                recommendations.append("æœªå‘ç°æ˜æ˜¾çš„å•†å“å®¹å™¨ï¼Œå¯èƒ½éœ€è¦ç­‰å¾…åŠ¨æ€åŠ è½½æˆ–ä½¿ç”¨æ›´å®½æ³›çš„é€‰æ‹©å™¨")
            
            if len(self.network_requests) > 50:
                recommendations.append("ç½‘ç»œè¯·æ±‚è¾ƒå¤šï¼Œå»ºè®®å¢åŠ ç­‰å¾…æ—¶é—´")
            
            if analysis['network_analysis']['failed_requests']:
                recommendations.append("å­˜åœ¨å¤±è´¥çš„ç½‘ç»œè¯·æ±‚ï¼Œå¯èƒ½å½±å“é¡µé¢å®Œæ•´æ€§")
            
            analysis['recommendations'] = recommendations
            
            print("âœ… æ·±åº¦é¡µé¢åˆ†æå®Œæˆ")
            
        except Exception as e:
            error_msg = f"é¡µé¢åˆ†æå¤±è´¥: {e}"
            self.errors.append(error_msg)
            print(f"âŒ {error_msg}")
        
        return analysis
    
    async def crawl_with_full_analysis(self, max_products: Optional[int] = None) -> CrawlResult:
        """æ‰§è¡Œå®Œæ•´çš„çˆ¬å–å’Œåˆ†æ"""
        start_time = time.time()
        
        try:
            print("ğŸš€ å¼€å§‹å®Œæ•´çš„çˆ¬å–å’Œåˆ†ææµç¨‹...")
            
            # 1. å¯¼èˆªåˆ°é¡µé¢
            nav_success = await self.navigate_to_taobao()
            if not nav_success:
                return CrawlResult(
                    success=False,
                    products=[],
                    errors=self.errors,
                    warnings=self.warnings,
                    network_requests=self.network_requests,
                    page_analysis={},
                    execution_time=time.time() - start_time
                )
            
            # 2. ç­‰å¾…å•†å“åŠ è½½
            await self.smart_wait_for_products()
            
            # 3. æ·±åº¦åˆ†æé¡µé¢
            page_analysis = await self.analyze_page_deep()
            
            # 4. æå–å•†å“ä¿¡æ¯
            products = await self.extract_products_with_strategies(max_products)
            
            # 5. æ•°æ®éªŒè¯å’Œæ¸…æ´—
            validated_products = self._validate_and_clean_products(products)
            
            execution_time = time.time() - start_time
            
            result = CrawlResult(
                success=len(validated_products) > 0,
                products=validated_products,
                errors=self.errors,
                warnings=self.warnings,
                network_requests=self.network_requests,
                page_analysis=page_analysis,
                execution_time=execution_time
            )
            
            print(f"âœ… å®Œæ•´çˆ¬å–æµç¨‹å®Œæˆï¼Œè€—æ—¶ {execution_time:.2f} ç§’")
            return result
            
        except Exception as e:
            error_msg = f"çˆ¬å–æµç¨‹å¤±è´¥: {e}"
            self.errors.append(error_msg)
            print(f"âŒ {error_msg}")
            
            return CrawlResult(
                success=False,
                products=[],
                errors=self.errors,
                warnings=self.warnings,
                network_requests=self.network_requests,
                page_analysis={},
                execution_time=time.time() - start_time
            )
    
    def _validate_and_clean_products(self, products: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """éªŒè¯å’Œæ¸…æ´—å•†å“æ•°æ®"""
        validated_products = []
        
        for product in products:
            # åŸºæœ¬éªŒè¯
            if not product.get('title') and not product.get('price'):
                continue
            
            # æ•°æ®æ¸…æ´—
            if product.get('title'):
                product['title'] = product['title'].strip()[:200]  # é™åˆ¶é•¿åº¦
            
            if product.get('price'):
                # æ¸…ç†ä»·æ ¼æ ¼å¼
                price = re.sub(r'[^\d.,]', '', product['price'])
                product['price'] = price
            
            if product.get('image_url'):
                # éªŒè¯å›¾ç‰‡URL
                if not product['image_url'].startswith('http'):
                    product['image_url'] = ''
            
            validated_products.append(product)
        
        return validated_products
    
    def print_detailed_report(self, result: CrawlResult):
        """æ‰“å°è¯¦ç»†çš„çˆ¬å–æŠ¥å‘Š"""
        print("\n" + "=" * 100)
        print("ğŸ“Š è¯¦ç»†çˆ¬å–æŠ¥å‘Š")
        print("=" * 100)
        
        # åŸºæœ¬ä¿¡æ¯
        print(f"ğŸ¯ çˆ¬å–çŠ¶æ€: {'æˆåŠŸ' if result.success else 'å¤±è´¥'}")
        print(f"â±ï¸  æ‰§è¡Œæ—¶é—´: {result.execution_time:.2f} ç§’")
        print(f"ğŸ“¦ å•†å“æ•°é‡: {len(result.products)}")
        print(f"âŒ é”™è¯¯æ•°é‡: {len(result.errors)}")
        print(f"âš ï¸  è­¦å‘Šæ•°é‡: {len(result.warnings)}")
        print(f"ğŸŒ ç½‘ç»œè¯·æ±‚: {len(result.network_requests)}")
        
        # é”™è¯¯å’Œè­¦å‘Š
        if result.errors:
            print(f"\nâŒ é”™è¯¯åˆ—è¡¨:")
            for i, error in enumerate(result.errors, 1):
                print(f"   {i}. {error}")
        
        if result.warnings:
            print(f"\nâš ï¸  è­¦å‘Šåˆ—è¡¨:")
            for i, warning in enumerate(result.warnings, 1):
                print(f"   {i}. {warning}")
        
        # é¡µé¢åˆ†æ
        if result.page_analysis:
            print(f"\nğŸ” é¡µé¢åˆ†æ:")
            page_info = result.page_analysis.get('page_info', {})
            print(f"   é¡µé¢æ ‡é¢˜: {page_info.get('title', 'N/A')}")
            
            element_analysis = result.page_analysis.get('element_analysis', {})
            print(f"   é¡µé¢å…ƒç´ : {element_analysis.get('totalElements', 0)}")
            print(f"   å›¾ç‰‡æ•°é‡: {element_analysis.get('images', 0)}")
            print(f"   é“¾æ¥æ•°é‡: {element_analysis.get('links', 0)}")
            
            # å•†å“å®¹å™¨åˆ†æ
            containers = element_analysis.get('productContainers', [])
            if containers:
                print(f"   æ½œåœ¨å•†å“å®¹å™¨:")
                for container in containers[:5]:
                    print(f"     - {container['selector']}: {container['count']} ä¸ª")
        
        # å•†å“æ‘˜è¦
        if result.products:
            print(f"\nğŸ“¦ å•†å“æ‘˜è¦ (æ˜¾ç¤ºå‰5ä¸ª):")
            for i, product in enumerate(result.products[:5], 1):
                title = product.get('title', 'N/A')[:40] + '...' if len(product.get('title', '')) > 40 else product.get('title', 'N/A')
                price = product.get('price', 'N/A')
                confidence = product.get('confidence_score', 0.0)
                strategy = product.get('extraction_strategy', 'N/A')
                
                print(f"   {i}. {title}")
                print(f"      ä»·æ ¼: Â¥{price} | ç½®ä¿¡åº¦: {confidence:.2f} | ç­–ç•¥: {strategy}")
        
        # å»ºè®®
        recommendations = result.page_analysis.get('recommendations', [])
        if recommendations:
            print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
            for i, rec in enumerate(recommendations, 1):
                print(f"   {i}. {rec}")
        
        print("=" * 100)
    
    def save_detailed_report(self, result: CrawlResult, filename: Optional[str] = None):
        """ä¿å­˜è¯¦ç»†æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        try:
            if not filename:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"taobao_crawl_report_{timestamp}.json"
            
            filepath = Path(filename)
            
            # å‡†å¤‡ä¿å­˜çš„æ•°æ®
            report_data = {
                'crawl_summary': {
                    'success': result.success,
                    'execution_time': result.execution_time,
                    'product_count': len(result.products),
                    'error_count': len(result.errors),
                    'warning_count': len(result.warnings),
                    'network_request_count': len(result.network_requests),
                    'timestamp': datetime.now().isoformat()
                },
                'products': result.products,
                'errors': result.errors,
                'warnings': result.warnings,
                'network_requests': result.network_requests,
                'page_analysis': result.page_analysis
            }
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {filepath.absolute()}")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            if self.browser_service:
                print("ğŸ§¹ æ­£åœ¨æ¸…ç†æµè§ˆå™¨èµ„æº...")
                await self.browser_service.shutdown()
                print("âœ… èµ„æºæ¸…ç†å®Œæˆ")
        except Exception as e:
            print(f"âš ï¸ æ¸…ç†èµ„æºæ—¶å‘ç”Ÿé”™è¯¯: {e}")


async def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("ğŸ¯ é«˜çº§æ·˜å®å•†å“ä¿¡æ¯çˆ¬è™«æ¼”ç¤º")
    print("=" * 80)
    print()
    print("ğŸ“‹ é«˜çº§åŠŸèƒ½:")
    print("1. å¤šé‡é€‰æ‹©å™¨ç­–ç•¥")
    print("2. ç½‘ç»œè¯·æ±‚ç›‘æ§")
    print("3. æ™ºèƒ½é¡µé¢çŠ¶æ€æ£€æµ‹")
    print("4. æ·±åº¦é¡µé¢ç»“æ„åˆ†æ")
    print("5. æ•°æ®éªŒè¯å’Œæ¸…æ´—")
    print("6. è¯¦ç»†çš„é”™è¯¯æŠ¥å‘Š")
    print("7. æ€§èƒ½æŒ‡æ ‡ç›‘æ§")
    print()
    
    # é…ç½®å‚æ•°
    headless = False
    request_delay = 1.5
    max_products = 15
    
    print(f"ğŸ–¥ï¸  æµè§ˆå™¨æ¨¡å¼: {'æ— å¤´æ¨¡å¼' if headless else 'æœ‰å¤´æ¨¡å¼'}")
    print(f"â±ï¸  è¯·æ±‚é—´éš”: {request_delay} ç§’")
    print(f"ğŸ“¦ æœ€å¤§å•†å“æ•°: {max_products}")
    print()
    
    # è¯¢é—®ç”¨æˆ·æ˜¯å¦ç»§ç»­
    try:
        response = input("ğŸ¤” æ˜¯å¦å¼€å§‹é«˜çº§çˆ¬å–ï¼Ÿ(y/N): ").strip().lower()
        if response not in ['y', 'yes']:
            print("ğŸ‘‹ çˆ¬å–å·²å–æ¶ˆ")
            return
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­")
        return
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = AdvancedTaobaoCrawler(headless=headless, request_delay=request_delay)
    
    try:
        # åˆå§‹åŒ–
        success = await crawler.initialize()
        if not success:
            print("âŒ åˆå§‹åŒ–å¤±è´¥ï¼Œé€€å‡ºç¨‹åº")
            return
        
        # æ‰§è¡Œå®Œæ•´çˆ¬å–
        result = await crawler.crawl_with_full_analysis(max_products=max_products)
        
        # æ˜¾ç¤ºè¯¦ç»†æŠ¥å‘Š
        crawler.print_detailed_report(result)
        
        # ä¿å­˜æŠ¥å‘Š
        crawler.save_detailed_report(result)
        
        if result.success:
            print(f"\nğŸ‰ é«˜çº§çˆ¬å–æˆåŠŸå®Œæˆï¼")
        else:
            print(f"\nâŒ çˆ¬å–è¿‡ç¨‹ä¸­é‡åˆ°é—®é¢˜ï¼Œè¯·æŸ¥çœ‹è¯¦ç»†æŠ¥å‘Š")
    
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­çˆ¬å–")
    except Exception as e:
        print(f"\nâŒ çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
    finally:
        # æ¸…ç†èµ„æº
        await crawler.cleanup()


if __name__ == "__main__":
    asyncio.run(main())